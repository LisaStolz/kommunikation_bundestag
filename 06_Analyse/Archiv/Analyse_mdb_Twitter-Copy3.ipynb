{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse der Tweets von Bundestagsabgeordneten\n",
    "## 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "\n",
    "db = client['Twitter']\n",
    "All_Tweets_collection = db['Twitter_mdp_extend_datetime']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "127.0.0.1:27017: [Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-befd41275257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             'created_at_datetime': {'$gte': datetime.datetime(2020,2,1,0,0,0),\n\u001b[1;32m     16\u001b[0m                                     '$lt': datetime.datetime(2020,5,1,0,0,0)},\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;34m'retweeted_status'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             })\n\u001b[1;32m     19\u001b[0m         ))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__manipulate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                 \u001b[0m_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36m_refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__session\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_ensure_session\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m   1808\u001b[0m             \u001b[0;31m# Don't make implicit sessions causally consistent. Applications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m             \u001b[0;31m# should always opt-in.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__start_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausal_consistency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConfigurationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInvalidOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[0;31m# Sessions not supported, or multiple users authenticated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m__start_session\u001b[0;34m(self, implicit, **kwargs)\u001b[0m\n\u001b[1;32m   1761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m         \u001b[0;31m# Raises ConfigurationError if sessions are not supported.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m         \u001b[0mserver_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_server_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m         \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSessionOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m         return client_session.ClientSession(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_get_server_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_server_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m         \u001b[0;34m\"\"\"Internal: start or resume a _ServerSession.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1796\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_topology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_server_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_return_server_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymongo/topology.py\u001b[0m in \u001b[0;36mget_server_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m                             \u001b[0many_server_selector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_selection_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m                             None)\n\u001b[0m\u001b[1;32m    486\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_description\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadable_servers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                     self._select_servers_loop(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymongo/topology.py\u001b[0m in \u001b[0;36m_select_servers_loop\u001b[0;34m(self, selector, timeout, address)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 raise ServerSelectionTimeoutError(\n\u001b[0;32m--> 209\u001b[0;31m                     self._error_message(selector))\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_opened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m: 127.0.0.1:27017: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "\n",
    "\n",
    "# sample_df = pd.DataFrame(list(All_Tweets_collection.aggregate([ {\"$sample\": {\"size\": 50 }}], \n",
    "#                                      allowDiskUse=True\n",
    "#                                    )))\n",
    "\n",
    "\n",
    "time_sample_df = pd.DataFrame(list(All_Tweets_collection.find( {\n",
    "            'created_at_datetime': {'$gte': datetime.datetime(2020,2,1,0,0,0),\n",
    "                                    '$lt': datetime.datetime(2020,5,1,0,0,0)},\n",
    "            'retweeted_status': None\n",
    "            })\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new Column date_time - erledigt in Datenbank\n",
    "\n",
    "created_at is saved in Mongodb as string and needs to be converted to a datetime format\n",
    "\n",
    "https://www.programiz.com/python-programming/datetime/strptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# created_at_datetime = []\n",
    "# for date in sample_df.created_at:\n",
    "#     date_string = date\n",
    "#     date_object = datetime.strptime(date_string, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "#     created_at_datetime.append(date_object)\n",
    "    \n",
    "# sample_df[\"date_time\"] = created_at_datetime\n",
    "# #sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Retweets - erledig bei laden in Notebook\n",
    "Zu erkennen sind Retweets am \"RT\" vor dem Text bzw. an der Spalte \"retweeted_status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_text = sample_df[[\"created_at_datetime\", \"full_text\", 'retweeted_status']]\n",
    "# text_df = pd.DataFrame(sample_text)\n",
    "# text_nRT = sample_text[pd.isnull(sample_text['retweeted_status'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regular expression library\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "import re\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # https://www.compart.com/de/unicode/block/U+1F900\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "sample_df=time_sample_df\n",
    "# Remove punctuation\n",
    "sample_df['full_text_processed'] = sample_df['full_text'].map(lambda x: re.sub('[,\\.!?#@\\\\n\"“„\\:;&\\(\\)]', '', x))\n",
    "# Remove Links\n",
    "sample_df['full_text_processed'] = sample_df['full_text_processed'].map(lambda x: re.sub('http.*', '', x))\n",
    "\n",
    "sample_df['full_text_processed'] = sample_df['full_text_processed'].map(lambda x: re.sub('amp', '', x))\n",
    "# Convert the titles to lowercase\n",
    "sample_df['full_text_processed'] = sample_df['full_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "sample_df['full_text_processed'] = sample_df['full_text_processed'].map(lambda x: remove_emoji(x))\n",
    "# Print out the first rows of papers\n",
    "sample_df['full_text_processed'].head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Durchsuche nach Tweets und sortiere aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print words that match certain words:\n",
    "#sample_df['full_text_processed'][sample_df['full_text_processed'].str.match(r'.*fröhlicher gruss.*')==True]\n",
    "\n",
    "# Remove Tweets that match certain words:\n",
    "sample_df = sample_df[~sample_df.full_text_processed.str.contains(\"fröhlicher gruss\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stop_words import get_stop_words\n",
    "#stop_words = get_stop_words('de')\n",
    "\n",
    "import stopwordsiso as stopwords\n",
    "stop_words = list(stopwords.stopwords([\"de\"])) \n",
    "\n",
    "mehr_sw = ['der', 'die', 'das', 'ist' 'es', 'gibt', 'und', 'für', 'auf', 'aus', 'mit', 'dem', 'tb']\n",
    "for word in mehr_sw:\n",
    "    stop_words.append(word)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning \n",
    "Diskretisierung von date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sortieren nach Datum/Zeit\n",
    "sample_df = sample_df.sort_values(by = ['created_at_datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verschiedene Binning Ansätze - aus verschiedenen Gründen nicht verwendet\n",
    "* Bins mit vorgegebenen Breiten (Y, M, ...) -> Labels können nicht mitgenommen werden\n",
    "* Binning mit value_count -> Direkte Ausgabe der Anzahl pro Bin\n",
    "* Binning mit groupby -> Ausgabe der Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut_bins = pd.interval_range(start=pd.Timestamp('2020-02-01'), periods=9, freq='M')\n",
    "# cut_labels = [i for i in range(2009,2021)]\n",
    "# pd.cut(text_nRT['created_at_datetime'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "\n",
    "# text_nRT['created_at_datetime'].value_counts(bins=9, sort=False)\n",
    "\n",
    "# bins = text_nRT.groupby(pd.cut(text_nRT['created_at_datetime'], bins=9, labels=list(range(1,10))))\n",
    "# for b in bins.indices:\n",
    "#     print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unterteilung in Bins und erstellen eines Dictionarys für Zeiträume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['bins'] = pd.cut(sample_df['created_at_datetime'], bins=9, labels=list(range(1,10)))\n",
    "\n",
    "labels=list(range(1,10))\n",
    "weeks = {}\n",
    "    \n",
    "for i in labels:\n",
    "    weeks['week_'+str(i)] = sample_df[sample_df['bins'] == i]    \n",
    "# Unsaubere Variante - jeweils Variable erstellen:\n",
    "# globals()['week_'+str(i)] = text_nRT[text_nRT['bins'] == i]\n",
    "\n",
    "bin_sum_per_week = {}\n",
    "\n",
    "for label in range(1,10): \n",
    "    df = sample_df[sample_df['bins'] == label]\n",
    "    head = list(df.created_at_datetime.head(1))[0]\n",
    "    tail = list(df.created_at_datetime.tail(1))[0]\n",
    "    #print(head.to_datetime)\n",
    "    #print(type(head))\n",
    "    bin_sum_per_week[label] = {}\n",
    "    bin_sum_per_week[label]['Von'] = head\n",
    "    bin_sum_per_week[label]['Bis'] = tail\n",
    "bin_sum_per_week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weitere Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in labels:\n",
    "    weeks['week_'+str(i)] = sample_df[sample_df['bins'] == i] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-3e9e244eebfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Join the different processed titles together.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlong_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text_processed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create a WordCloud object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontour_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontour_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steelblue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(sample_df['full_text_processed'])\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', stopwords=stop_words)\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()\n",
    "\n",
    "#fig, axs = plt.subplots(1,2)\n",
    "\n",
    "#df['korisnika'].plot(ax=axs[0])\n",
    "#df['osiguranika'].plot(ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud Gegenüberstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "for i in labels:\n",
    "    weeks['week_'+str(i)] = sample_df[sample_df['bins'] == i]  \n",
    "\n",
    "def FktWordCloud(long_string):\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', stopwords=stop_words)\n",
    "    wordcloud.generate(long_string)\n",
    "    wordcloud.to_image()\n",
    "    return wordcloud\n",
    "\n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "for i, n in zip(weeks, range(1,10)):\n",
    "    title = \"%s - %s\" % (bin_sum_per_week[n]['Von'].strftime('%d.%m.'),\n",
    "                         bin_sum_per_week[n]['Bis'].strftime('%d.%m.%Y'))\n",
    "    ax = fig.add_subplot(3,3,n, title = title)\n",
    "    sample_df=weeks[i]\n",
    "    long_string = ','.join(sample_df['full_text_processed'])\n",
    "    wordcloud = FktWordCloud(long_string)\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the library with the CountVectorizer method\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# sns.set_style('whitegrid')\n",
    "# %matplotlib inline\n",
    "\n",
    "# # Helper function\n",
    "# def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     words = count_vectorizer.get_feature_names()\n",
    "#     total_counts = np.zeros(len(words))\n",
    "#     for t in count_data:\n",
    "#         total_counts+=t.toarray()[0]\n",
    "    \n",
    "#     count_dict = (zip(words, total_counts))\n",
    "#     count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "#     words = [w[0] for w in count_dict]\n",
    "#     counts = [w[1] for w in count_dict]\n",
    "#     x_pos = np.arange(len(words)) \n",
    "    \n",
    "#     plt.figure(2, figsize=(15, 15/1.6180))\n",
    "#     plt.subplot(title='10 most common words')\n",
    "#     sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "#     sns.barplot(x_pos, counts, palette='husl')\n",
    "#     plt.xticks(x_pos, words, rotation=90) \n",
    "#     plt.xlabel('words')\n",
    "#     plt.ylabel('counts')\n",
    "#     plt.show()\n",
    "    \n",
    "# # Initialise the count vectorizer with the german stop words\n",
    "# count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# # Fit and transform the processed titles\n",
    "# count_data = count_vectorizer.fit_transform(weeks['week_9']['full_text_processed'])\n",
    "# # Visualise the 10 most common words\n",
    "# plot_10_most_common_words(count_data, count_vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot Gegenüberstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the library with the CountVectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0] # total_counts = total_counts + t.toarray\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words))\n",
    "    \n",
    "    with plt.xkcd():\n",
    "        ax = fig.add_subplot(3,3,n, title =  title)\n",
    "        fig.tight_layout()\n",
    "        ax = sns.barplot(x_pos, counts, palette=\"GnBu_d\")\n",
    "        ax.set_xticklabels(words, rotation = 45, fontsize=10)\n",
    "        return ax\n",
    "\n",
    "    \n",
    "# Initialise the count vectorizer with the german stop words\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "for i, n in zip(weeks, range(1,10)):\n",
    "    title = \"%s - %s\" % (bin_sum_per_week[n]['Von'].strftime('%d.%m.'),\n",
    "                         bin_sum_per_week[n]['Bis'].strftime('%d.%m.%Y'))\n",
    "    \n",
    "    # Fit and transform the processed titles\n",
    "    count_data = count_vectorizer.fit_transform(weeks[i]['full_text_processed'])\n",
    "    plot_10_most_common_words(count_data, count_vectorizer, title)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 5\n",
    "number_words = 10\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import os\n",
    "import pyLDAvis\n",
    "LDAvis_data_filepath = os.path.join('/home/lisa/Darmstadt/Master Arbeit/06_Analyse/ldavis_prepared_'+str(number_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import collections\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize \n",
    "from nltk.util import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "bigram=[]\n",
    "   \n",
    "\n",
    "def make_bigrams_network (df):\n",
    "    for tweet in df:\n",
    "        word_data = tweet\n",
    "        tokens = nltk.word_tokenize(word_data)                            # -> Einzelne Wörter (Tokens)\n",
    "        tweets_nsw = [word for word in tokens if not word in stop_words]  # -> Ohne StopWords\n",
    "        terms_bigram = list(nltk.bigrams(tweets_nsw))                     # -> Zweierpärchen (Bigrams) \n",
    "        bigram.append(terms_bigram)\n",
    "    \n",
    "    terms_bigram = bigram\n",
    "    #return terms_bigram\n",
    "    # Flatten list of bigrams in clean tweets\n",
    "    bigrams = list(itertools.chain(*terms_bigram))\n",
    "    # Create counter of words in clean bigrams\n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    bigram_counts.most_common(20)\n",
    "    bigram_df = pd.DataFrame(bigram_counts.most_common(20), columns=['bigram', 'count'])\n",
    "\n",
    "    # Create dictionary of bigrams and their counts\n",
    "    d = bigram_df.set_index('bigram').T.to_dict('records')\n",
    "    \n",
    "    # Create network plot \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Create connections between nodes\n",
    "    for k, v in d[0].items():\n",
    "        G.add_edge(k[0], k[1], weight=(v * 10))\n",
    "        \n",
    "    ax = fig.add_subplot(5,2,n, title =  title)\n",
    "    fig.tight_layout()\n",
    "    pos = nx.spring_layout(G, k=1)\n",
    "    nx.draw_networkx(G, pos,\n",
    "                 font_size=16, width=3, edge_color='grey', node_color='darkblue', with_labels = False, ax=ax)\n",
    "    \n",
    "    for key, value in pos.items():\n",
    "        x, y = value[0]+.135, value[1]+.045\n",
    "        ax.text(x, y, s=key, bbox=dict(alpha=0.25), horizontalalignment='center', fontsize=10)\n",
    "    \n",
    "        \n",
    "fig = plt.figure(figsize = (15, 25))\n",
    "\n",
    "for i, n in zip(weeks, range(1,10)):\n",
    "    title = \"%s - %s\" % (bin_sum_per_week[n]['Von'].strftime('%d.%m.'),\n",
    "                         bin_sum_per_week[n]['Bis'].strftime('%d.%m.%Y'))\n",
    "    \n",
    "    make_bigrams_network(weeks[i]['full_text_processed'])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyse - Wörterbuch\n",
    "## Simple TextBlob Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob_de import TextBlobDE as TextBlob\n",
    "\n",
    "blob = TextBlob(sample_df['full_text_processed'][8]) \n",
    "\n",
    "# print(blob.sentences)\n",
    "# print(blob.tokens)\n",
    "# print(blob.tags)\n",
    "# print(blob.noun_phrases)\n",
    "# print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@himanshu_23732/sentiment-analysis-with-textblob-6bc2eb9ec4ab\n",
    "def sentiment(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "sample_df['Polarity']     = sample_df['full_text_processed'].apply(sentiment).apply(lambda x: x[0])\n",
    "sample_df['Subjectivity'] = sample_df['full_text_processed'].apply(sentiment).apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sentiment scores (polarity) and labels\n",
    "sentiment_scores = sample_df['Polarity']\n",
    "sentiment_category = ['positive' if score > 0 \n",
    "                             else 'negative' if score < 0 \n",
    "                                 else 'neutral' \n",
    "                                     for score in sentiment_scores]\n",
    "\n",
    "\n",
    "# sentiment statistics per news category\n",
    "df = pd.DataFrame([list(sample_df['bins']), list(sentiment_scores), list(sentiment_category)]).T\n",
    "df.columns = ['category', 'sentiment_score', 'sentiment_category']\n",
    "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
    "df.groupby(by=['category']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i, n in zip(weeks, range(1,10)):\n",
    "    label = \"%s - %s\" % (bin_sum_per_week[n]['Von'].strftime('%d.%m.'),\n",
    "                         bin_sum_per_week[n]['Bis'].strftime('%d.%m.%Y'))\n",
    "    labels.append(label)\n",
    "\n",
    "ax = sns.catplot(x=\"category\", hue=\"sentiment_category\", height=13,\n",
    "                    data=df, kind=\"count\", \n",
    "                    palette={\"negative\": \"#FE2020\", \n",
    "                             \"positive\": \"#BADD07\", \n",
    "                             \"neutral\": \"#68BFF5\"})\n",
    "ax.set_xticklabels(labels, rotation = 45, fontsize=12)\n",
    "\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Ekel Freude Furcht Trauer Ueberraschung Verachtung Wut\n",
      "Aasgeier             0    0      0      0      0             1          0 \n",
      "Abbrechen            0    0      0      1      0             0          0 \n",
      "Aberration           1    0      0      0      0             0          0 \n",
      "Aberwitz             0    0      0      0      0             1          0 \n",
      "Abfall               1    0      0      0      0             0          0 \n",
      "Abgeneigtheit        1    0      0      0      0             0          0 \n",
      "Abgeschlagenheit     0    0      0      1      0             0          0 \n",
      "Abgeschmacktheit     1    0      0      0      0             1          0 \n",
      "Abgrund              0    0      1      1      0             0          0 \n",
      "Abhauen              0    0      1      0      0             0          0 \n",
      "Abhängigkeit         0    0      1      0      0             0          0 \n",
      "Abklatsch            0    0      0      0      0             1          0 \n",
      "Abkömmling           0    0      0      0      0             1          0 \n",
      "Ablehnung            1    0      0      0      0             0          0 \n",
      "Ablenkungsmanöver    0    0      0      0      0             1          0 \n",
      "Abneigung            1    0      0      0      0             0          0 \n",
      "Abnicker             0    0      0      0      0             1          0 \n",
      "Abreibung            0    0      1      0      0             0          0 \n",
      "Absahner             0    0      0      0      0             1          0 \n",
      "Abschaum             0    0      0      0      0             1          0 \n",
      "Abscheu              1    0      0      0      0             0          0 \n",
      "Abscheulichkeit      1    0      0      0      0             0          0 \n",
      "Abschied             0    0      0      1      0             0          0 \n",
      "Abschluss            0    1      0      0      0             0          0 \n",
      "Abschlussprüfung     0    0      1      0      0             0          0 \n",
      "Absonderung          1    0      0      0      0             0          0 \n",
      "Abstauber            0    0      0      0      0             1          0 \n",
      "Abstieg              0    0      1      0      0             0          0 \n",
      "Abszess              1    0      0      0      0             0          0 \n",
      "Abtreibung           1    0      1      1      0             0          0 \n",
      "...                 ..   ..     ..     ..     ..            ..         .. \n",
      "ätzend               1    0      0      0      0             1          1 \n",
      "öde                  0    0      0      1      0             0          0 \n",
      "öha                  0    0      0      0      1             0          0 \n",
      "übel                 1    0      0      0      0             0          0 \n",
      "übellaunig           0    0      0      1      0             0          1 \n",
      "übelnehmen           0    0      0      0      0             0          1 \n",
      "übelriechend         1    0      0      0      0             0          0 \n",
      "überdreht            0    0      0      0      0             1          0 \n",
      "überdrüssig          0    0      0      1      0             0          0 \n",
      "überempfindlich      0    0      0      1      0             0          0 \n",
      "überfahren           0    0      0      0      1             0          0 \n",
      "überfallartig        0    0      0      0      1             0          0 \n",
      "überfallen           0    0      0      0      1             0          0 \n",
      "übergeben            1    0      0      0      0             0          0 \n",
      "überglücklich        0    1      0      0      0             0          0 \n",
      "überragend           0    1      0      0      0             0          0 \n",
      "überraschen          0    0      0      0      1             0          0 \n",
      "überraschend         0    0      0      0      1             0          0 \n",
      "überraschenderweise  0    0      0      0      1             0          0 \n",
      "überrascht           0    0      0      0      1             0          0 \n",
      "überrumpeln          0    0      0      0      1             0          0 \n",
      "überschaubar         0    0      0      0      0             1          0 \n",
      "überschnappen        0    0      0      0      0             0          1 \n",
      "überschwänglich      0    1      0      0      0             0          0 \n",
      "übersensibel         0    0      0      1      0             0          0 \n",
      "übertölpeln          0    0      0      0      1             0          0 \n",
      "überwältig           0    1      0      0      0             0          0 \n",
      "überwältigend        0    0      0      0      1             0          0 \n",
      "überwältigt          0    0      1      0      1             0          0 \n",
      "überängstlich        0    0      1      0      0             0          0 \n",
      "\n",
      "[4293 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "filepath = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Lexicon_based/german-emotion-dictionary/fundamental/'\n",
    "emo_df = []\n",
    "words = []\n",
    "emotion = ['Ekel', 'Freude', 'Furcht', 'Trauer', 'Ueberraschung', 'Verachtung', 'Wut']\n",
    "emotion_list= {}\n",
    "\n",
    "for emo in emotion:\n",
    "    with open(filepath + emo + '.txt', newline='\\n') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = [item for sublist in reader for item in sublist] \n",
    "        emotion_list[str(emo)] = data\n",
    "        words.extend(data)\n",
    "        \n",
    "words = list(set(words))\n",
    "emo_df  = pd.DataFrame(index=words, columns=emotion)\n",
    "#print(len(words))\n",
    "#print(emotion_list['Freude'])\n",
    "\n",
    "#emo_df = pd.DataFrame(words)\n",
    "#emo_df.index = emo_df[0]\n",
    "\n",
    "\n",
    "# for line in emo_df:\n",
    "#     emo_df[0].isnull== True\n",
    "#     #print(line)\n",
    "    \n",
    "emo_df=emo_df.sort_index(ascending=True)\n",
    "emo_df = pd.DataFrame(emo_df)\n",
    "\n",
    "for word in emo_df.index:\n",
    "    for emo in emo_df.columns:\n",
    "        if word in emotion_list[emo]:\n",
    "            emo_df.at[word, emo] = 1\n",
    "        \n",
    "        else:\n",
    "            emo_df.at[word, emo] = 0\n",
    "\n",
    "print(emo_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ekel</th>\n",
       "      <th>Freude</th>\n",
       "      <th>Furcht</th>\n",
       "      <th>Trauer</th>\n",
       "      <th>Ueberraschung</th>\n",
       "      <th>Verachtung</th>\n",
       "      <th>Wut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abbrechen</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aberration</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aberwitz</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abfall</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abgeneigtheit</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abgeschlagenheit</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abgeschmacktheit</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abgrund</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abhauen</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abhängigkeit</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abklatsch</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Ekel Freude Furcht Trauer Ueberraschung Verachtung  Wut\n",
       "Abbrechen         NaN  NaN    NaN    1      NaN           NaN        NaN\n",
       "Aberration        1    NaN    NaN    NaN    NaN           NaN        NaN\n",
       "Aberwitz          NaN  NaN    NaN    NaN    NaN           1          NaN\n",
       "Abfall            1    NaN    NaN    NaN    NaN           NaN        NaN\n",
       "Abgeneigtheit     1    NaN    NaN    NaN    NaN           NaN        NaN\n",
       "Abgeschlagenheit  NaN  NaN    NaN    1      NaN           NaN        NaN\n",
       "Abgeschmacktheit  1    NaN    NaN    NaN    NaN           1          NaN\n",
       "Abgrund           NaN  NaN    1      1      NaN           NaN        NaN\n",
       "Abhauen           NaN  NaN    1      NaN    NaN           NaN        NaN\n",
       "Abhängigkeit      NaN  NaN    1      NaN    NaN           NaN        NaN\n",
       "Abklatsch         NaN  NaN    NaN    NaN    NaN           1          NaN"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print_debug = False\n",
    "# def debug(string):\n",
    "#     if print_debug == True:\n",
    "#         print(string)\n",
    "\n",
    "# for word in emo_df.index:\n",
    "#      debug(\"'%s'\" % (word))\n",
    "#      debug(type(word))\n",
    "#      #for emo in emotion_list:\n",
    "#      for emo in emo_df.columns:\n",
    "#          #print(emo)\n",
    "#          #print(type(emo))\n",
    "#          if word in emotion_list[emo]:\n",
    "#              debug(\"Word '%s' is of emotion: '%s'\" % (word, emo))\n",
    "#              #print(emo_df[emo][word])\n",
    "#              #print(emo_df.columns)\n",
    "#              #print(emo_df.ix[1])\n",
    "#              emo_df.at[word, emo] = 1\n",
    "#              debug(emo_df.loc[word])\n",
    "#              debug(\"\\n\")\n",
    "\n",
    "#          #else:\n",
    "#          #    emo_df.at[str(emo)][word] = 0\n",
    "\n",
    "# emo_df[1:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "def text_emotion(document):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column of text and adds 10 columns to the\n",
    "    DataFrame for each of the 10 emotions in the NRC Emotion Lexicon, with each\n",
    "    column containing the value of the text in that emotions\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with ten new columns\n",
    "    '''\n",
    "\n",
    "    import csv\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "    filepath = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Lexicon_based/german-emotion-dictionary/fundamental/'\n",
    "    emo_df = []\n",
    "    words = []\n",
    "    emotion = ['Ekel', 'Freude', 'Furcht', 'Trauer', 'Ueberraschung', 'Verachtung', 'Wut']\n",
    "    emotion_list= {}\n",
    "\n",
    "    for emo in emotion:\n",
    "        with open(filepath + emo + '.txt', newline='\\n') as f:\n",
    "            reader = csv.reader(f)\n",
    "            data = [item for sublist in reader for item in sublist] \n",
    "            emotion_list[str(emo)] = data\n",
    "            words.extend(data)\n",
    "        \n",
    "    words = list(set(words))\n",
    "    emo_df  = pd.DataFrame(index=words, columns=emotion)\n",
    "\n",
    "    \n",
    "    emo_df=emo_df.sort_index(ascending=True)\n",
    "    emo_df = pd.DataFrame(emo_df)\n",
    "\n",
    "    for word in emo_df.index:\n",
    "        for emo in emo_df.columns:\n",
    "            if word in emotion_list[emo]:\n",
    "                emo_df.at[word, emo] = 1\n",
    "        \n",
    "            else:\n",
    "                emo_df.at[word, emo] = 0\n",
    "                \n",
    "    df_emo = pd.DataFrame(0, index=df.index, columns=emotions)\n",
    "    stemmer = SnowballStemmer(\"german\")\n",
    "    for word in document:\n",
    "        word = stemmer.stem(word.lower())\n",
    "        emo_score = emo_df[emo_df.word == word]\n",
    "        if not emo_score.empty:\n",
    "            for emotion in list(emotions):\n",
    "                df_emo.at[i, emotion] += emo_score[emotion]\n",
    "\n",
    "    new_df = pd.concat([new_df, emo_df], axis=1)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "def text_emotion(df, column):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column of text and adds 10 columns to the\n",
    "    DataFrame for each of the 10 emotions in the NRC Emotion Lexicon, with each\n",
    "    column containing the value of the text in that emotions\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: the original DataFrame with ten new columns\n",
    "    '''\n",
    "\n",
    "    new_df = df.copy()\n",
    "\n",
    "    filepath = ('data/'\n",
    "                'NRC-Sentiment-Emotion-Lexicons/'\n",
    "                'NRC-Emotion-Lexicon-v0.92/'\n",
    "                'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')\n",
    "    emolex_df = pd.read_csv(filepath,\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep='\\t')\n",
    "    emolex_words = emolex_df.pivot(index='word',\n",
    "                                   columns='emotion',\n",
    "                                   values='association').reset_index()\n",
    "    emotions = emolex_words.columns.drop('word')\n",
    "    emo_df = pd.DataFrame(0, index=df.index, columns=emotions)\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    \n",
    "    book = ''\n",
    "    chapter = ''\n",
    "    \n",
    "    with tqdm(total=len(list(new_df.iterrows()))) as pbar:\n",
    "        for i, row in new_df.iterrows():\n",
    "            pbar.update(1)\n",
    "            if row['book'] != book:\n",
    "                print(row['book'])\n",
    "                book = row['book']\n",
    "            if row['chapter_title'] != chapter:\n",
    "                print('   ', row['chapter_title'])\n",
    "                chapter = row['chapter_title']\n",
    "                chap = row['chapter_title']\n",
    "            document = word_tokenize(new_df.loc[i][column])\n",
    "            for word in document:\n",
    "                word = stemmer.stem(word.lower())\n",
    "                emo_score = emolex_words[emolex_words.word == word]\n",
    "                if not emo_score.empty:\n",
    "                    for emotion in list(emotions):\n",
    "                        emo_df.at[i, emotion] += emo_score[emotion]\n",
    "\n",
    "    new_df = pd.concat([new_df, emo_df], axis=1)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-a5d6ed8ce93c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_df' is not defined"
     ]
    }
   ],
   "source": [
    "type(sample_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
