{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse der Tweets von Bundestagsabgeordneten\n",
    "## 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "db = client['Twitter']\n",
    "All_Tweets_collection = db['twitter_mdp_ex_date_proj']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "# sample_df = pd.DataFrame(list(All_Tweets_collection.aggregate([ {\"$sample\": {\"size\": 50 }}], \n",
    "#                                      allowDiskUse=True\n",
    "#                                    )))\n",
    "\n",
    "time_sample_df = pd.DataFrame(list(All_Tweets_collection.find( {\n",
    "            'created_at_datetime': {'$gte': datetime.datetime(2020,2,1,0,0,0),\n",
    "                                    '$lt': datetime.datetime(2020,2,20,0,0,0)},\n",
    "            'retweeted_id': None\n",
    "            })\n",
    "        ))# Load the regular expression library\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # https://www.compart.com/de/unicode/block/U+1F900\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "sample_df = time_sample_df\n",
    "# Remove punctuation\n",
    "sample_df.loc[:,('full_text_processed')] = sample_df.loc[:,('full_text')].map(lambda x: re.sub('[,\\.!?#@\\\\n\"“„\\:;&\\(\\)]', '', x))\n",
    "# Remove Links\n",
    "sample_df.loc[:,('full_text_processed')] = sample_df.loc[:,('full_text_processed')].map(lambda x: re.sub('http.*', '', x))\n",
    "\n",
    "sample_df.loc[:,('full_text_processed')] = sample_df.loc[:,('full_text_processed')].map(lambda x: re.sub('amp', '', x))\n",
    "# Convert the titles to lowercase\n",
    "#sample_df['full_text_processed'] = sample_df['full_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "#sample_df['full_text_processed'] = sample_df['full_text_processed'].map(lambda x: remove_emoji(x))\n",
    "# Print out the first rows of papers\n",
    "sample_df['full_text_processed'].head(200)\n",
    "\n",
    "sample_df = sample_df[sample_df['full_text_processed'] != '']\n",
    "sample_df = sample_df[sample_df['full_text_processed'] != ' ']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12171"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_df['full_text_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "for timestamp in sample_df.created_at_datetime:\n",
    "    date.append(timestamp.date())\n",
    "      \n",
    "sample_df.loc[:, ('date')] = date\n",
    "sample_df.loc[:,('month')] = sample_df.created_at_datetime.dt.strftime('%y-%m')\n",
    "# mdp_tweets.loc[:,('week')] = mdp_tweets.created_at_datetime.dt.weekofyear\n",
    "# mdp_tweets.loc[:,('week')] = mdp_tweets.created_at_datetime.dt.strftime('%y-w%V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mdp = np.array(sample_df['full_text_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import pickle, gensim, numpy as np\n",
    "\n",
    "from utilities import get_train_data, get_test_data, Tokenizer, find_subtoken\n",
    "\n",
    "PICKLE_FOLDER_PATH = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/'\n",
    "\n",
    "TRAIN_FILENAME = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/germeval2018.training.txt'\n",
    "#mdp = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/mdp_tweets.txt'\n",
    "\n",
    "#------------------------------\n",
    "#source:\n",
    "#http://www.cl.uni-heidelberg.de/english/research/downloads/resource_pages/GermanTwitterEmbeddings/GermanTwitterEmbeddings_data.shtml\n",
    "MODEL_FILENAME  = \"/home/lisa/Darmstadt/Master Arbeit/06_Analyse/twitter-de_d100_w5_min10.bin\" # 821,8 MB\n",
    "MODEL_DIMENSION = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word2vec_model  = gensim.models.KeyedVectors.load_word2vec_format(MODEL_FILENAME, binary=True)\n",
    "\n",
    "X_train, y_train_t1, y_train_t2 = get_train_data(TRAIN_FILENAME)\n",
    "X_test_mdp                      = sample_mdp\n",
    "\n",
    "X_test_mdp = X_test_mdp[X_test_mdp != '']\n",
    "X_test_mdp = X_test_mdp[X_test_mdp != ' ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGRAM FEATURES\n",
    " * Erstelle n-Gramme mit 3-7 Buchstaben (Funktion: char_vect)\n",
    " * Erstelle n-Gramme mit 1-3 Wörtern /Funkion: token_vect\n",
    " * Anwendung auf Training/ Test und mdp Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vect  = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 7), max_df=0.01, min_df=0.0002,\n",
    "                             preprocessor=Tokenizer(preserve_case=False, join=True).tokenize)\n",
    "\n",
    "token_vect = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 3), max_df=0.01, min_df=0.0002,\n",
    "                             tokenizer=Tokenizer(preserve_case=False, use_stemmer=True).tokenize)\n",
    "\n",
    "X_CNGR_train = char_vect.fit_transform(X_train)\n",
    "X_CNGR_mdp = char_vect.transform(X_test_mdp)\n",
    "\n",
    "X_TNGR_train = token_vect.fit_transform(X_train)\n",
    "X_TNGR_mdp  = token_vect.transform(X_test_mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(X_CNGR_train, open(PICKLE_FOLDER_PATH + \"X_CNGR_train.p\", \"wb\" ))\n",
    "pickle.dump(X_CNGR_mdp,  open(PICKLE_FOLDER_PATH + \"X_CNGR_mdp.p\", \"wb\" ))\n",
    "\n",
    "# pickle.dump(X_TNGR_train, open(PICKLE_FOLDER_PATH + \"X_TNGR_train.p\", \"wb\" ))\n",
    "pickle.dump(X_TNGR_mdp, open(PICKLE_FOLDER_PATH + \"X_TNGR_mdp.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_CNGR_mdp.shape) # (9958, 207575)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMB FEATURES\n",
    "* Tweets werden in Token unterteilt\n",
    "* Prüfe ob die Token in einem Token im vortrainierten word2vec Model entsprechen\n",
    "* Wenn nicht, teile Token in Präfix und Suffix und prüfe für diese das word2vec Model (ggf. beide in emb)\n",
    "* emb enthält pro Tweet Vektoren für Token und wird normalisiert mit der Länge des Tweets + ggf extra Tokens\n",
    "* X_EMB enthält die normalisierten Vektoren pro Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EMB_feats(tweets):   \n",
    "    tknzr = Tokenizer(preserve_case=True)\n",
    "    tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "    \n",
    "    X_EMB = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        emb = np.zeros(MODEL_DIMENSION)\n",
    "        extra_tokens = 0\n",
    "        \n",
    "        for token in tweet:\n",
    "            try:\n",
    "                emb += word2vec_model[token]\n",
    "            except:\n",
    "                prefix = find_subtoken(token, word2vec_model, mode='initial')\n",
    "                suffix = find_subtoken(token, word2vec_model, mode='final')\n",
    "                    \n",
    "                if prefix != None and suffix != None:\n",
    "                    emb += word2vec_model[prefix] + word2vec_model[suffix]\n",
    "                    extra_tokens += 1\n",
    "                elif prefix != None and suffix == None:\n",
    "                    emb += word2vec_model[prefix]\n",
    "                elif prefix == None and suffix != None:\n",
    "                    emb += word2vec_model[suffix]           \n",
    "        emb /= (len(tweet) + extra_tokens)\n",
    "        X_EMB.append(emb)\n",
    "        \n",
    "    return normalize(X_EMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_EMB_train = get_EMB_feats(X_train)\n",
    "X_EMB_mdp  = get_EMB_feats(X_test_mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_EMB_mdp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_EMB_train, open(PICKLE_FOLDER_PATH + \"X_EMB_train.p\", \"wb\" ))\n",
    "pickle.dump(X_EMB_mdp,  open(PICKLE_FOLDER_PATH + \"X_EMB_mdp.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIMP FEATURES\n",
    "* Finden der wichtigen Tokens - also derer die in Tweets der angegebenen Kategorie verwendet werden\n",
    "* Für diese wichtigsten Tokens werden die Features analog der EMB Features aus dem word2vec model abgeleitet\n",
    "* Außerdem werden für alle Tweets analog der EMB feats abgeleitet\n",
    "* Vergleiche mit der Cosine Similarity und gebe die höchsten und niedrigsten Werte pro Tweet zurück\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_imp_tokenlvl(k, category, max_df=0.01, min_df=0.0002):      \n",
    "    token_vect = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False,\n",
    "                                 max_df=max_df, min_df=min_df,\n",
    "                                 tokenizer=Tokenizer(preserve_case=True).tokenize)\n",
    "    \n",
    "    tfidf = token_vect.fit_transform(X_train)\n",
    "    \n",
    "    vocab = token_vect.vocabulary_\n",
    "    inv_vocab = {index: word for word, index in vocab.items()}\n",
    "    \n",
    "    if category in ['OTHER', 'OFFENSE']:\n",
    "        cat_ids = np.where(y_train_t1 == category)\n",
    "    elif category in ['PROFANITY', 'ABUSE', 'INSULT']:\n",
    "        cat_ids = np.where(y_train_t2 == category)\n",
    "        \n",
    "    most_imp_ids = np.argsort(np.asarray(np.mean(tfidf[cat_ids], axis=0)).flatten())[::-1]\n",
    "        \n",
    "    most_imp = []\n",
    "    for index in most_imp_ids:\n",
    "        most_imp.append(inv_vocab[index])\n",
    "\n",
    "    return most_imp[:k]\n",
    "\n",
    "def get_TIMP_feats(tweets, k, category, max_df=0.01, min_df=0.0002):\n",
    "    feats_max = []\n",
    "    feats_min = []\n",
    "           \n",
    "    imp_tokens_vectors = []\n",
    "    for imp_token in k_most_imp_tokenlvl(k, category, max_df=max_df, min_df=min_df):\n",
    "        try:\n",
    "            imp_tokens_vectors.append(word2vec_model[imp_token])\n",
    "        except:\n",
    "            imp_tokens_vectors.append(np.zeros(MODEL_DIMENSION))\n",
    "    \n",
    "    tknzr = Tokenizer(preserve_case=True)\n",
    "    tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        tweet_vectors = []\n",
    "        for token in tweet:\n",
    "            try:\n",
    "                tweet_vectors.append(word2vec_model[token])\n",
    "            except:\n",
    "                prefix = find_subtoken(token, word2vec_model, mode='initial')\n",
    "                suffix = find_subtoken(token, word2vec_model, mode='final')\n",
    "                 \n",
    "                if prefix != None and suffix != None:\n",
    "                    tweet_vectors.append(word2vec_model[prefix])\n",
    "                    tweet_vectors.append(word2vec_model[suffix])\n",
    "                elif prefix != None and suffix == None:\n",
    "                    tweet_vectors.append(word2vec_model[prefix])\n",
    "                elif prefix == None and suffix != None:\n",
    "                    tweet_vectors.append(word2vec_model[suffix])\n",
    "                else:\n",
    "                    tweet_vectors.append(np.zeros(MODEL_DIMENSION))\n",
    "                    \n",
    "        similarity = cosine_similarity(np.asarray(tweet_vectors), np.asarray(imp_tokens_vectors))\n",
    "        \n",
    "        feats_max.append(np.amax(similarity, axis=0))\n",
    "        feats_min.append(np.amin(similarity, axis=0))\n",
    "        \n",
    "    return np.concatenate((feats_max, feats_min), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "N_TIMP_TASK1 = 1250\n",
    "N_TIMP_TASK2 = 170\n",
    "\n",
    "X_TIMP_task1_train = \\\n",
    "np.concatenate((get_TIMP_feats(X_train, N_TIMP_TASK1, 'OTHER'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "\n",
    "X_TIMP_task2_train = \\\n",
    "np.concatenate((get_TIMP_feats(X_train, N_TIMP_TASK2, 'OTHER'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK2, 'ABUSE'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK2, 'INSULT'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK2, 'PROFANITY')), axis=1)\n",
    "\n",
    "X_TIMP_task1_mdp = \\\n",
    "np.concatenate((get_TIMP_feats(X_test_mdp,  N_TIMP_TASK1, 'OTHER'),\n",
    "                get_TIMP_feats(X_test_mdp,  N_TIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "X_TIMP_task2_mdp = \\\n",
    "np.concatenate((get_TIMP_feats(X_test_mdp,  N_TIMP_TASK2, 'OTHER'),\n",
    "                get_TIMP_feats(X_test_mdp,  N_TIMP_TASK2, 'ABUSE'),\n",
    "                get_TIMP_feats(X_test_mdp,  N_TIMP_TASK2, 'INSULT'),\n",
    "                get_TIMP_feats(X_test_mdp,  N_TIMP_TASK2, 'PROFANITY')), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_TIMP_task2_mdp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_TIMP_task1_train, open(PICKLE_FOLDER_PATH + \"X_TIMP_task1_train.p\", \"wb\" ))\n",
    "pickle.dump(X_TIMP_task2_train, open(PICKLE_FOLDER_PATH + \"X_TIMP_task2_train.p\", \"wb\" ))\n",
    "pickle.dump(X_TIMP_task1_mdp,  open(PICKLE_FOLDER_PATH + \"X_TIMP_task1_mdp.p\", \"wb\" ))\n",
    "pickle.dump(X_TIMP_task2_mdp,  open(PICKLE_FOLDER_PATH + \"X_TIMP_task2_mdp.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIMP FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_imp_charlvl(k, category, max_df=0.01, min_df=0.0002):    \n",
    "    char_vect  = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 7), lowercase=False,\n",
    "                                 max_df=max_df, min_df=min_df,\n",
    "                                 preprocessor=Tokenizer(preserve_case=True, join=True).tokenize)\n",
    "\n",
    "    tfidf = char_vect.fit_transform(X_train)\n",
    "    \n",
    "    vocab = char_vect.vocabulary_\n",
    "    inv_vocab = {index: word for word, index in vocab.items()}\n",
    "    \n",
    "    if category in ['OTHER', 'OFFENSE']:\n",
    "        cat_ids = np.where(y_train_t1 == category)\n",
    "    elif category in ['PROFANITY', 'ABUSE', 'INSULT']:\n",
    "        cat_ids = np.where(y_train_t2 == category)       \n",
    "        \n",
    "    most_imp_ids = np.argsort(np.asarray(np.mean(tfidf[cat_ids], axis=0)).flatten())[::-1]\n",
    "        \n",
    "    most_imp = []\n",
    "    for index in most_imp_ids:\n",
    "        most_imp.append(inv_vocab[index])\n",
    "\n",
    "    return most_imp[:k]\n",
    "\n",
    "def get_CIMP_feats(tweets, k, category, max_df=0.01, min_df=0.0002):\n",
    "    feats = np.zeros((len(tweets), k))\n",
    "    for imp_ngram_index, imp_ngram in enumerate(k_most_imp_charlvl(k, category, max_df=max_df, min_df=min_df)):\n",
    "        for tweet_index, tweet in enumerate(tweets):\n",
    "            if tweet.find(imp_ngram) != -1:\n",
    "                feats[tweet_index][imp_ngram_index] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "N_CIMP_TASK1 = 3200\n",
    "N_CIMP_TASK2 = 370\n",
    "            \n",
    "X_CIMP_task1_train = \\\n",
    "np.concatenate((get_CIMP_feats(X_train, N_CIMP_TASK1, 'OTHER'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "\n",
    "X_CIMP_task2_train = \\\n",
    "np.concatenate((get_CIMP_feats(X_train, N_CIMP_TASK2, 'OTHER'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK2, 'ABUSE'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK2, 'INSULT'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK2, 'PROFANITY')), axis=1)\n",
    "\n",
    "\n",
    "X_CIMP_task1_mdp = \\\n",
    "np.concatenate((get_CIMP_feats(X_test_mdp,  N_CIMP_TASK1, 'OTHER'),\n",
    "                get_CIMP_feats(X_test_mdp,  N_CIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "\n",
    "X_CIMP_task2_mdp = \\\n",
    "np.concatenate((get_CIMP_feats(X_test_mdp,  N_CIMP_TASK2, 'OTHER'),\n",
    "                get_CIMP_feats(X_test_mdp,  N_CIMP_TASK2, 'ABUSE'),\n",
    "                get_CIMP_feats(X_test_mdp,  N_CIMP_TASK2, 'INSULT'),\n",
    "                get_CIMP_feats(X_test_mdp,  N_CIMP_TASK2, 'PROFANITY')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_CIMP_task1_mdp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_CNGR_train = pickle.load(open(PICKLE_FOLDER_PATH + \"X_CNGR_train.p\", \"rb\" ))\n",
    "# X_CNGR_mdp  = pickle.load(open(PICKLE_FOLDER_PATH + \"X_CNGR_mdp.p\", \"rb\" ))\n",
    "\n",
    "# X_TNGR_train = pickle.load(open(PICKLE_FOLDER_PATH + \"X_TNGR_train.p\", \"rb\" ))\n",
    "# X_TNGR_mdp  = pickle.load(open(PICKLE_FOLDER_PATH + \"X_TNGR_mdp.p\", \"rb\" ))\n",
    "\n",
    "\n",
    "# X_CIMP_task1_train = pickle.load(open(PICKLE_FOLDER_PATH + \"X_CIMP_task1_train.p\", \"rb\" ))\n",
    "# X_CIMP_task1_mdp  = pickle.load(open(PICKLE_FOLDER_PATH + \"X_CIMP_task1_mdp.p\", \"rb\" ))\n",
    "\n",
    "\n",
    "# X_CIMP_task2_train = pickle.load(open(PICKLE_FOLDER_PATH + \"X_CIMP_task2_train.p\", \"rb\" ))\n",
    "# X_CIMP_task2_mdp  = pickle.load(open(PICKLE_FOLDER_PATH + \"X_CIMP_task2_mdp.p\", \"rb\" ))\n",
    "\n",
    "\n",
    "# X_TIMP_task1_train = pickle.load(open(PICKLE_FOLDER_PATH + \"X_TIMP_task1_train.p\", \"rb\" ))\n",
    "# X_TIMP_task1_mdp  = pickle.load(open(PICKLE_FOLDER_PATH + \"X_TIMP_task1_mdp.p\", \"rb\" ))\n",
    "\n",
    "\n",
    "# X_TIMP_task2_train = pickle.load(open(PICKLE_FOLDER_PATH + \"X_TIMP_task2_train.p\", \"rb\" ))\n",
    "# X_TIMP_task2_mdp  = pickle.load(open(PICKLE_FOLDER_PATH + \"X_TIMP_task2_mdp.p\", \"rb\" ))\n",
    "\n",
    "\n",
    "# X_EMB_train = pickle.load(open(PICKLE_FOLDER_PATH + \"X_EMB_train.p\", \"rb\" ))\n",
    "# X_EMB_mdp  = pickle.load(open(PICKLE_FOLDER_PATH + \"X_EMB_mdp.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y1, y2 = get_train_data(TRAIN_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funktion für das Aufteilen in Train und Test Sample \n",
    "-> StratifiedKFold sorgt dafür, dass das prozentuale Verhältnis der Klassen im jeweiligen Sample (Test, Train) gleich ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "\n",
    "def get_META_feats(clf, X_train, mdp, y, seeds=[42]):\n",
    "    feats_train = []\n",
    "    for seed in seeds:\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        feats_train.append(cross_val_predict(clf, X_train, y=y, method='predict_proba', cv=skf, n_jobs=-1))\n",
    "    feats_train = np.mean(feats_train, axis=0)\n",
    "    print(len(feats_train))\n",
    "    print(clf)\n",
    "    clf.fit(X_train, y)\n",
    "    feats_mdp = clf.predict_proba(mdp)\n",
    "    print(len(feats_mdp))\n",
    "    \n",
    "    return feats_train, feats_mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1 - Base level predictions\n",
    "Die drei verschiedenen Classifier (clfs_task1) werden auf die Feature Vectoren (base_feats_task1) angewandt.\n",
    "Von einer 10-fold CrossVal wird für den Trainings Feature Satz der Durchschnitt genommen (jeder Spalte).\n",
    "Bei den Test-/mdp Daten wird keine Cross Val durchgeführt (keine y Variablen) sondern nur mit jedem Classifier eine prediction anhand der Feature Vektoren gemacht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clfs_task1 = [LogisticRegression(class_weight='balanced'),\n",
    "              ExtraTreesClassifier(n_estimators=100, criterion='entropy', n_jobs=-1),\n",
    "              ExtraTreesClassifier(n_estimators=100, criterion='gini', n_jobs=-1)]\n",
    "\n",
    "base_feats_task1 = [(X_CIMP_task1_train, X_CIMP_task1_mdp),\n",
    "                    (X_TIMP_task1_train, X_TIMP_task1_mdp),\n",
    "                    (X_CNGR_train, X_CNGR_mdp),\n",
    "                    (X_TNGR_train, X_TNGR_mdp),\n",
    "                    (X_EMB_train, X_EMB_mdp)]\n",
    "X_META_task1_train = []\n",
    "#X_META_task1_test  = []\n",
    "X_META_task1_mdp  = []\n",
    "for X_train, mdp in base_feats_task1:                 # X-train z.B X_CIMP_task1_train, mdp z.B X_CIMP_task1_mdp\n",
    "    for clf in clfs_task1:\n",
    "        feats = get_META_feats(clf, X_train, mdp, y1)\n",
    "\n",
    "        X_META_task1_train.append(feats[0])           # aus \"get_META_feats: feats_train\n",
    "        X_META_task1_mdp.append(feats[1])             # aus \"get_META_feats: feats_mdp\n",
    "        \n",
    "X_META_task1_train = np.concatenate(X_META_task1_train, axis=1)\n",
    "X_META_task1_mdp  = np.concatenate(X_META_task1_mdp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_META_task1_mdp.shape\n",
    "#X_META_task1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_task1 = LogisticRegression(C=0.17, class_weight='balanced')\n",
    "clf_task1.fit(X_META_task1_train, y1)\n",
    "\n",
    "# clf_task2 = LogisticRegression(C=0.2, class_weight='balanced')\n",
    "# clf_task2.fit(X_META_task2_train, y2)  \n",
    "\n",
    "preds_task1 = clf_task1.predict(X_META_task1_mdp)    \n",
    "# preds_task2 = clf_task2.predict(X_META_task1_mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "sample_df['predict'] = preds_task1\n",
    "# data = np.array([X_test_mdp, preds_task1])\n",
    "# df = pd.DataFrame({'tweets':data[0,], 'label':data[1,]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[sample_df['predict'] != 'OTHER' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(sample_df, open(\"name\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_partei = sample_df[['user_party', 'date', 'user_screen_name', 'full_text', 'predict']]\n",
    "mdp_partei = mdp_partei[mdp_partei['predict'] == 'OFFENSE']\n",
    "partei_piv = pd.pivot_table(mdp_partei, columns = ['user_party'], index = ['date'], aggfunc = np.count_nonzero)\n",
    "partei_piv['full_text']\n",
    "# partei_Piv = partei_piv['full_text']\n",
    "# partei_Piv.groupby('week').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "my_palette = ['blue', 'orange', 'green', 'purple', 'red', 'black']\n",
    "\n",
    "fig = plt.figure(figsize = (18, 16))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "num=0\n",
    "for column in partei_piv['full_text']:\n",
    "    ax.plot(partei_piv.index, partei_piv['full_text'][column], color = my_palette[num], alpha=0.7, label = column)\n",
    "    num+=1\n",
    "    \n",
    "ylab = ax.set_ylabel('Tweets pro Partei', size = 20)\n",
    "# = ax.set_xlabel('Datum', size = 20)\n",
    "plt.legend(loc = 2, ncol = 2, fontsize = 17)\n",
    "\n",
    "# y_tags\n",
    "# ytag = ax.set_yticklabels(np.arange(partei_piv.values.min()-51, partei_piv.values.max()+50, 50), fontsize = 18)\n",
    "# ytag = ax.set_yticklabels(np.arange(partei_piv.values.min()-90, partei_piv.values.max(), 200), fontsize = 18)\n",
    "\n",
    "#x_tags\n",
    "xtag = ax.set_xticklabels(partei_piv.index, fontsize = 14, rotation = 45)\n",
    "\n",
    "# months = mdates.MonthLocator()\n",
    "# months_fmt = mdates.DateFormatter('%b-%Y')\n",
    "\n",
    "# ax.xaxis.set_major_locator(months)\n",
    "# ax.xaxis.set_major_formatter(months_fmt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH lisa@desktop Python3.7",
   "language": "",
   "name": "rik_ssh_lisa_desktop_python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
