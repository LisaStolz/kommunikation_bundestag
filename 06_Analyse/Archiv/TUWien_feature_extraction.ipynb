{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import pickle, gensim, numpy as np\n",
    "\n",
    "from utilities import get_train_data, get_test_data, Tokenizer, find_subtoken\n",
    "\n",
    "PICKLE_FOLDER_PATH = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Learning_Alg/GermEval-2018-Data/'\n",
    "\n",
    "TRAIN_FILENAME = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/germeval2018.training.txt'\n",
    "TEST_FILENAME  = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/germeval2018.test.txt'\n",
    "mdp = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/mdp_tweets.txt'\n",
    "\n",
    "#------------------------------\n",
    "#source:\n",
    "#http://www.cl.uni-heidelberg.de/english/research/downloads/resource_pages/GermanTwitterEmbeddings/GermanTwitterEmbeddings_data.shtml\n",
    "MODEL_FILENAME  = \"/home/lisa/Darmstadt/Master Arbeit/06_Analyse/twitter-de_d100_w5_min10.bin\" # 821,8 MB\n",
    "MODEL_DIMENSION = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.97 s, sys: 672 ms, total: 10.6 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2vec_model  = gensim.models.KeyedVectors.load_word2vec_format(MODEL_FILENAME, binary=True)\n",
    "\n",
    "X_train, y_train_t1, y_train_t2 = get_train_data(TRAIN_FILENAME)\n",
    "X_test                          = get_test_data(TEST_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word2vec_model.vocab\n",
    "#word2vec_model['laufe']\n",
    "word2vec_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mdp = get_test_data(mdp)\n",
    "# X_test_mdp[X_test_mdp == ' ']\n",
    "X_test_mdp = X_test_mdp[X_test_mdp != '']\n",
    "X_test_mdp = X_test_mdp[X_test_mdp != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Meine Mutter hat mir erzählt, dass mein Vater einen Wahlkreiskandidaten nicht gewählt hat, weil der gegen die Homo-Ehe ist ☺\\tOTHER\\tOTHER',\n",
       "       '@Tom174_ @davidbest95 Meine Reaktion; |LBR| Nicht jeder Moslem ist ein Terrorist. Aber jeder Moslem glaubt an Überlieferungen, die Gewalt und Terror begünstigen.\\tOTHER\\tOTHER',\n",
       "       '#Merkel rollt dem Emir von #Katar, der islamistischen Terror unterstützt, den roten Teppich aus.Wir brauchen einen sofortigen #Waffenstopp!\\tOTHER\\tOTHER',\n",
       "       ...,\n",
       "       '@podilein Mannheim, weltoffen und kunterbunt. Und strunzdumme Multikultiliebhaber. Verblödete halt\\tOFFENSE\\tABUSE',\n",
       "       '@stephanweil was ist nun mit kostenlosen Kitas in der GROKO ???\\tOTHER\\tOTHER',\n",
       "       '@Bartzissey Denn Minderheiten sind für Linke ja nur dann interessant, wenn sie gegen den Westen instrumentalisiert werden können.\\tOTHER\\tOTHER'],\n",
       "      dtype='<U544')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGRAM FEATURES\n",
    " * Erstelle n-Gramme mit 3-7 Buchstaben (Funktion: char_vect)\n",
    " * Erstelle n-Gramme mit 1-3 Wörtern /Funkion: token_vect\n",
    " * Anwendung auf Training/ Test und mdp Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vect  = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 7), max_df=0.01, min_df=0.0002,\n",
    "                             preprocessor=Tokenizer(preserve_case=False, join=True).tokenize)\n",
    "\n",
    "token_vect = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 3), max_df=0.01, min_df=0.0002,\n",
    "                             tokenizer=Tokenizer(preserve_case=False, use_stemmer=True).tokenize)\n",
    "\n",
    "X_CNGR_train = char_vect.fit_transform(X_train)\n",
    "X_CNGR_test  = char_vect.transform(X_test)\n",
    "\n",
    "X_TNGR_train = token_vect.fit_transform(X_train)\n",
    "X_TNGR_test  = token_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(X_CNGR_train, open(PICKLE_FOLDER_PATH + \"X_CNGR_train.p\", \"wb\" ))\n",
    "# pickle.dump(X_CNGR_test,  open(PICKLE_FOLDER_PATH + \"X_CNGR_test.p\", \"wb\" ))\n",
    "\n",
    "# pickle.dump(X_TNGR_train, open(PICKLE_FOLDER_PATH + \"X_TNGR_train.p\", \"wb\" ))\n",
    "# pickle.dump(X_TNGR_test,  open(PICKLE_FOLDER_PATH + \"X_TNGR_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CNGR_mdp = char_vect.transform(X_test_mdp)\n",
    "X_TNGR_mdp  = token_vect.transform(X_test_mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9958, 207575)\n"
     ]
    }
   ],
   "source": [
    "print(X_CNGR_mdp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_CNGR_mdp,  open(PICKLE_FOLDER_PATH + \"X_CNGR_mdp.p\", \"wb\" ))\n",
    "pickle.dump(X_TNGR_mdp, open(PICKLE_FOLDER_PATH + \"X_TNGR_mdp.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMB FEATURES\n",
    "* Tweets werden in Token unterteilt\n",
    "* Prüfe ob die Token in einem Token im vortrainierten word2vec Model entsprechen\n",
    "* Wenn nicht, teile Token in Präfix und Suffix und prüfe für diese das word2vec Model (ggf. beide in emb)\n",
    "* emb enthält pro Tweet Vektoren für Token und wird normalisiert mit der Länge des Tweets + ggf extra Tokens\n",
    "* X_EMB enthält die normalisierten Vektoren pro Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EMB_feats(tweets):   \n",
    "    tknzr = Tokenizer(preserve_case=True)\n",
    "    tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "    \n",
    "    X_EMB = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        emb = np.zeros(MODEL_DIMENSION)\n",
    "        extra_tokens = 0\n",
    "        \n",
    "        for token in tweet:\n",
    "            try:\n",
    "                emb += word2vec_model[token]\n",
    "            except:\n",
    "                prefix = find_subtoken(token, word2vec_model, mode='initial')\n",
    "                suffix = find_subtoken(token, word2vec_model, mode='final')\n",
    "                    \n",
    "                if prefix != None and suffix != None:\n",
    "                    emb += word2vec_model[prefix] + word2vec_model[suffix]\n",
    "                    extra_tokens += 1\n",
    "                elif prefix != None and suffix == None:\n",
    "                    emb += word2vec_model[prefix]\n",
    "                elif prefix == None and suffix != None:\n",
    "                    emb += word2vec_model[suffix]\n",
    "                    \n",
    "        emb /= (len(tweet) + extra_tokens)\n",
    "        X_EMB.append(emb)\n",
    "        \n",
    "    return normalize(X_EMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.09 s, sys: 4.19 ms, total: 2.09 s\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_EMB_train = get_EMB_feats(X_train)\n",
    "X_EMB_test  = get_EMB_feats(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_EMB_mdp  = get_EMB_feats(X_test_mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3532, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_EMB_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_EMB_train, open(PICKLE_FOLDER_PATH + \"X_EMB_train.p\", \"wb\" ))\n",
    "pickle.dump(X_EMB_test,  open(PICKLE_FOLDER_PATH + \"X_EMB_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_EMB_mdp,  open(PICKLE_FOLDER_PATH + \"X_EMB_mdp.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIMP FEATURES\n",
    "* Finden der wichtigen Tokens - also derer die in Tweets der angegebenen Kategorie verwendet werden\n",
    "* Für diese wichtigsten Tokens werden die Features analog der EMB Features aus dem word2vec model abgeleitet\n",
    "* Außerdem werden für alle Tweets analog der EMB feats abgeleitet\n",
    "* Vergleiche mit der Cosine Similarity und gebe die höchsten und niedrigsten Werte pro Tweet zurück\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_imp_tokenlvl(k, category, max_df=0.01, min_df=0.0002):      \n",
    "    token_vect = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False,\n",
    "                                 max_df=max_df, min_df=min_df,\n",
    "                                 tokenizer=Tokenizer(preserve_case=True).tokenize)\n",
    "    \n",
    "    tfidf = token_vect.fit_transform(X_train)\n",
    "    \n",
    "    vocab = token_vect.vocabulary_\n",
    "    inv_vocab = {index: word for word, index in vocab.items()}\n",
    "    \n",
    "    if category in ['OTHER', 'OFFENSE']:\n",
    "        cat_ids = np.where(y_train_t1 == category)\n",
    "    elif category in ['PROFANITY', 'ABUSE', 'INSULT']:\n",
    "        cat_ids = np.where(y_train_t2 == category)\n",
    "        \n",
    "    most_imp_ids = np.argsort(np.asarray(np.mean(tfidf[cat_ids], axis=0)).flatten())[::-1]\n",
    "        \n",
    "    most_imp = []\n",
    "    for index in most_imp_ids:\n",
    "        most_imp.append(inv_vocab[index])\n",
    "\n",
    "    return most_imp[:k]\n",
    "\n",
    "def get_TIMP_feats(tweets, k, category, max_df=0.01, min_df=0.0002):\n",
    "    feats_max = []\n",
    "    feats_min = []\n",
    "           \n",
    "    imp_tokens_vectors = []\n",
    "    for imp_token in k_most_imp_tokenlvl(k, category, max_df=max_df, min_df=min_df):\n",
    "        try:\n",
    "            imp_tokens_vectors.append(word2vec_model[imp_token])\n",
    "        except:\n",
    "            imp_tokens_vectors.append(np.zeros(MODEL_DIMENSION))\n",
    "    \n",
    "    tknzr = Tokenizer(preserve_case=True)\n",
    "    tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        tweet_vectors = []\n",
    "        for token in tweet:\n",
    "            try:\n",
    "                tweet_vectors.append(word2vec_model[token])\n",
    "            except:\n",
    "                prefix = find_subtoken(token, word2vec_model, mode='initial')\n",
    "                suffix = find_subtoken(token, word2vec_model, mode='final')\n",
    "                 \n",
    "                if prefix != None and suffix != None:\n",
    "                    tweet_vectors.append(word2vec_model[prefix])\n",
    "                    tweet_vectors.append(word2vec_model[suffix])\n",
    "                elif prefix != None and suffix == None:\n",
    "                    tweet_vectors.append(word2vec_model[prefix])\n",
    "                elif prefix == None and suffix != None:\n",
    "                    tweet_vectors.append(word2vec_model[suffix])\n",
    "                else:\n",
    "                    tweet_vectors.append(np.zeros(MODEL_DIMENSION))\n",
    "                    \n",
    "        similarity = cosine_similarity(np.asarray(tweet_vectors), np.asarray(imp_tokens_vectors))\n",
    "        \n",
    "        feats_max.append(np.amax(similarity, axis=0))\n",
    "        feats_min.append(np.amin(similarity, axis=0))\n",
    "        \n",
    "    return np.concatenate((feats_max, feats_min), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 13s, sys: 1.98 s, total: 2min 15s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "N_TIMP_TASK1 = 1250\n",
    "N_TIMP_TASK2 = 170\n",
    "\n",
    "X_TIMP_task1_train = \\\n",
    "np.concatenate((get_TIMP_feats(X_train, N_TIMP_TASK1, 'OTHER'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "X_TIMP_task1_test = \\\n",
    "np.concatenate((get_TIMP_feats(X_test,  N_TIMP_TASK1, 'OTHER'),\n",
    "                get_TIMP_feats(X_test,  N_TIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "X_TIMP_task2_train = \\\n",
    "np.concatenate((get_TIMP_feats(X_train, N_TIMP_TASK2, 'OTHER'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK2, 'ABUSE'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK2, 'INSULT'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK2, 'PROFANITY')), axis=1)\n",
    "\n",
    "X_TIMP_task2_test = \\\n",
    "np.concatenate((get_TIMP_feats(X_test,  N_TIMP_TASK2, 'OTHER'),\n",
    "                get_TIMP_feats(X_test,  N_TIMP_TASK2, 'ABUSE'),\n",
    "                get_TIMP_feats(X_test,  N_TIMP_TASK2, 'INSULT'),\n",
    "                get_TIMP_feats(X_test,  N_TIMP_TASK2, 'PROFANITY')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TIMP_TASK1 = 1250\n",
    "N_TIMP_TASK2 = 170\n",
    "\n",
    "X_TIMP_task1_mdp = \\\n",
    "np.concatenate((get_TIMP_feats(X_test_mdp,  N_TIMP_TASK1, 'OTHER'),\n",
    "                get_TIMP_feats(X_test_mdp,  N_TIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "X_TIMP_task2_mdp = \\\n",
    "np.concatenate((get_TIMP_feats(X_test_mdp,  N_TIMP_TASK2, 'OTHER'),\n",
    "                get_TIMP_feats(X_test_mdp,  N_TIMP_TASK2, 'ABUSE'),\n",
    "                get_TIMP_feats(X_test_mdp,  N_TIMP_TASK2, 'INSULT'),\n",
    "                get_TIMP_feats(X_test_mdp,  N_TIMP_TASK2, 'PROFANITY')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9958, 1360)\n"
     ]
    }
   ],
   "source": [
    "print(X_TIMP_task2_mdp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_TIMP_task1_train, open(PICKLE_FOLDER_PATH + \"X_TIMP_task1_train.p\", \"wb\" ))\n",
    "pickle.dump(X_TIMP_task1_test,  open(PICKLE_FOLDER_PATH + \"X_TIMP_task1_test.p\", \"wb\" ))\n",
    "pickle.dump(X_TIMP_task2_train, open(PICKLE_FOLDER_PATH + \"X_TIMP_task2_train.p\", \"wb\" ))\n",
    "pickle.dump(X_TIMP_task2_test,  open(PICKLE_FOLDER_PATH + \"X_TIMP_task2_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_TIMP_task1_mdp,  open(PICKLE_FOLDER_PATH + \"X_TIMP_task1_mdp.p\", \"wb\" ))\n",
    "pickle.dump(X_TIMP_task2_mdp,  open(PICKLE_FOLDER_PATH + \"X_TIMP_task2_mdp.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIMP FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_imp_charlvl(k, category, max_df=0.01, min_df=0.0002):    \n",
    "    char_vect  = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 7), lowercase=False,\n",
    "                                 max_df=max_df, min_df=min_df,\n",
    "                                 preprocessor=Tokenizer(preserve_case=True, join=True).tokenize)\n",
    "\n",
    "    tfidf = char_vect.fit_transform(X_train)\n",
    "    \n",
    "    vocab = char_vect.vocabulary_\n",
    "    inv_vocab = {index: word for word, index in vocab.items()}\n",
    "    \n",
    "    if category in ['OTHER', 'OFFENSE']:\n",
    "        cat_ids = np.where(y_train_t1 == category)\n",
    "    elif category in ['PROFANITY', 'ABUSE', 'INSULT']:\n",
    "        cat_ids = np.where(y_train_t2 == category)       \n",
    "        \n",
    "    most_imp_ids = np.argsort(np.asarray(np.mean(tfidf[cat_ids], axis=0)).flatten())[::-1]\n",
    "        \n",
    "    most_imp = []\n",
    "    for index in most_imp_ids:\n",
    "        most_imp.append(inv_vocab[index])\n",
    "\n",
    "    return most_imp[:k]\n",
    "\n",
    "def get_CIMP_feats(tweets, k, category, max_df=0.01, min_df=0.0002):\n",
    "    feats = np.zeros((len(tweets), k))\n",
    "    for imp_ngram_index, imp_ngram in enumerate(k_most_imp_charlvl(k, category, max_df=max_df, min_df=min_df)):\n",
    "        for tweet_index, tweet in enumerate(tweets):\n",
    "            if tweet.find(imp_ngram) != -1:\n",
    "                feats[tweet_index][imp_ngram_index] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 18s, sys: 1.35 s, total: 4min 20s\n",
      "Wall time: 4min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "N_CIMP_TASK1 = 3200\n",
    "N_CIMP_TASK2 = 370\n",
    "            \n",
    "X_CIMP_task1_train = \\\n",
    "np.concatenate((get_CIMP_feats(X_train, N_CIMP_TASK1, 'OTHER'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "X_CIMP_task1_test = \\\n",
    "np.concatenate((get_CIMP_feats(X_test,  N_CIMP_TASK1, 'OTHER'),\n",
    "                get_CIMP_feats(X_test,  N_CIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "X_CIMP_task2_train = \\\n",
    "np.concatenate((get_CIMP_feats(X_train, N_CIMP_TASK2, 'OTHER'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK2, 'ABUSE'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK2, 'INSULT'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK2, 'PROFANITY')), axis=1)\n",
    "\n",
    "X_CIMP_task2_test = \\\n",
    "np.concatenate((get_CIMP_feats(X_test,  N_CIMP_TASK2, 'OTHER'),\n",
    "                get_CIMP_feats(X_test,  N_CIMP_TASK2, 'ABUSE'),\n",
    "                get_CIMP_feats(X_test,  N_CIMP_TASK2, 'INSULT'),\n",
    "                get_CIMP_feats(X_test,  N_CIMP_TASK2, 'PROFANITY')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CIMP_TASK1 = 3200\n",
    "N_CIMP_TASK2 = 370\n",
    "\n",
    "X_CIMP_task1_mdp = \\\n",
    "np.concatenate((get_CIMP_feats(X_test_mdp,  N_CIMP_TASK1, 'OTHER'),\n",
    "                get_CIMP_feats(X_test_mdp,  N_CIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "X_CIMP_task2_mdp = \\\n",
    "np.concatenate((get_CIMP_feats(X_test_mdp,  N_CIMP_TASK2, 'OTHER'),\n",
    "                get_CIMP_feats(X_test_mdp,  N_CIMP_TASK2, 'ABUSE'),\n",
    "                get_CIMP_feats(X_test_mdp,  N_CIMP_TASK2, 'INSULT'),\n",
    "                get_CIMP_feats(X_test_mdp,  N_CIMP_TASK2, 'PROFANITY')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_CIMP_task1_mdp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-632080184f29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_CIMP_task1_mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_CIMP_task1_mdp' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_CIMP_task1_mdp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_CIMP_task1_train, open(PICKLE_FOLDER_PATH + \"X_CIMP_task1_train.p\", \"wb\" ))\n",
    "pickle.dump(X_CIMP_task1_test,  open(PICKLE_FOLDER_PATH + \"X_CIMP_task1_test.p\", \"wb\" ))\n",
    "pickle.dump(X_CIMP_task2_train, open(PICKLE_FOLDER_PATH + \"X_CIMP_task2_train.p\", \"wb\" ))\n",
    "pickle.dump(X_CIMP_task2_test,  open(PICKLE_FOLDER_PATH + \"X_CIMP_task2_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_CIMP_task1_mdp,  open(PICKLE_FOLDER_PATH + \"X_CIMP_task1_mdp.p\", \"wb\" ))\n",
    "pickle.dump(X_CIMP_task2_mdp,  open(PICKLE_FOLDER_PATH + \"X_CIMP_task2_mdp.p\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH lisa@desktop Python3.7",
   "language": "",
   "name": "rik_ssh_lisa_desktop_python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
