{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2\n",
      "The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      "text     10000 non-null object\n",
      "label    10000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 156.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "data = open('corpus').read()\n",
    "\n",
    "#print(data[1:200])\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    #print(content[1])\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "print(labels[1])    # Basis Datensatz und Datensatz nach Splittiing haben nicht die gleiche Reihenfolge bei Zeilen?\n",
    "print(texts[1])\n",
    "\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "\n",
    "trainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "# x= text, y= labels?? (0,1)\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "type(valid_x)\n",
    "type(valid_y)\n",
    "print(valid_y[3:8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Feature Engineering\n",
    "\n",
    "* Count Vectors as features\n",
    "* TF-IDF Vectors as features\n",
    "    * Word level\n",
    "    * N-Gram level\n",
    "    * Character level\n",
    "* Word Embeddings as features\n",
    "* Text / NLP based features\n",
    "* Topic Models as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you've played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time's Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer's work (I haven't heard the Xenogears soundtrack, so I can't say for sure), and even if you've never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\n"
     ]
    }
   ],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "type(valid_x)\n",
    "print(valid_x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectors as features\n",
    "\n",
    "* TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "* IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "\n",
    "* **Word Level TF-IDF** : Matrix representing tf-idf scores of every term in different documents\n",
    "* **N-gram Level TF-IDF** : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\n",
    "* **Character Level TF-IDF** : Matrix representing tf-idf scores of character level n-grams in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.85373154  6.62692143  6.54687872 ...,  6.80924299  6.22145632\n",
      "  6.77645316]\n",
      "  (0, 4605)\t0.167223224346\n",
      "  (0, 4604)\t0.161358967884\n",
      "  (0, 4506)\t0.186531787928\n",
      "  (0, 4493)\t0.102205809599\n",
      "  (0, 4428)\t0.0403740960037\n",
      "  (0, 3881)\t0.159525726254\n",
      "  (0, 3692)\t0.178013849358\n",
      "  (0, 3545)\t0.0985895389479\n",
      "  (0, 3334)\t0.257408291743\n",
      "  (0, 3228)\t0.404033584067\n",
      "  (0, 2395)\t0.259406120238\n",
      "  (0, 2390)\t0.0972523201176\n",
      "  (0, 2384)\t0.0517053925238\n",
      "  (0, 2267)\t0.11674022144\n",
      "  (0, 2223)\t0.141836496768\n",
      "  (0, 2040)\t0.174209638487\n",
      "  (0, 1958)\t0.145607498984\n",
      "  (0, 1366)\t0.226394425582\n",
      "  (0, 1278)\t0.305110707971\n",
      "  (0, 962)\t0.232036843409\n",
      "  (0, 718)\t0.231282110223\n",
      "  (0, 717)\t0.143308155782\n",
      "  (0, 243)\t0.0440947485395\n",
      "  (0, 227)\t0.152295448069\n",
      "  (0, 222)\t0.375094180337\n",
      "  :\t:\n",
      "  (7498, 181)\t0.101200750574\n",
      "  (7498, 177)\t0.0604778270579\n",
      "  (7498, 161)\t0.136654825227\n",
      "  (7498, 92)\t0.0496084344305\n",
      "  (7498, 88)\t0.0941801923985\n",
      "  (7499, 4992)\t0.148008231288\n",
      "  (7499, 4735)\t0.122679685506\n",
      "  (7499, 4662)\t0.249392893737\n",
      "  (7499, 4648)\t0.313331259723\n",
      "  (7499, 4524)\t0.23176342698\n",
      "  (7499, 4443)\t0.316521222271\n",
      "  (7499, 4428)\t0.163557642508\n",
      "  (7499, 3061)\t0.0676083860027\n",
      "  (7499, 3009)\t0.144537611296\n",
      "  (7499, 2691)\t0.219871698459\n",
      "  (7499, 2384)\t0.0698204454772\n",
      "  (7499, 2361)\t0.165518514984\n",
      "  (7499, 2223)\t0.0638430339398\n",
      "  (7499, 2194)\t0.367632659784\n",
      "  (7499, 2071)\t0.135638932157\n",
      "  (7499, 1936)\t0.195280147586\n",
      "  (7499, 1697)\t0.205188251423\n",
      "  (7499, 1097)\t0.343635336607\n",
      "  (7499, 1019)\t0.381851997641\n",
      "  (7499, 243)\t0.119086804527\n"
     ]
    }
   ],
   "source": [
    "#type(tfidf_vect_ngram_chars)\n",
    "#print(tfidf_vect.idf_)\n",
    "\n",
    "print(tfidf_vect.idf_)\n",
    "print(xtrain_tfidf)\n",
    "\n",
    "#print(tfidf_vect_ngram.idf_)\n",
    "#print(xtrain_tfidf_ngram)\n",
    "\n",
    "#print(tfidf_vect_ngram_chars.idf_)\n",
    "#print(xtrain_tfidf_ngram_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be trained using the input corpus itself or can be generated using pre-trained word embeddings such as **Glove, FastText, and Word2Vec**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'i': 3,\n",
       " 'a': 4,\n",
       " 'to': 5,\n",
       " 'of': 6,\n",
       " 'it': 7,\n",
       " 'this': 8,\n",
       " 'is': 9,\n",
       " 'in': 10,\n",
       " 'for': 11,\n",
       " 'that': 12,\n",
       " 'was': 13,\n",
       " 'book': 14,\n",
       " 'you': 15,\n",
       " 'not': 16,\n",
       " 'but': 17,\n",
       " 'with': 18,\n",
       " 'on': 19,\n",
       " 'my': 20,\n",
       " 'have': 21,\n",
       " 'as': 22,\n",
       " 'are': 23,\n",
       " 'one': 24,\n",
       " 'be': 25,\n",
       " 'so': 26,\n",
       " 'all': 27,\n",
       " 'if': 28,\n",
       " 'very': 29,\n",
       " 'like': 30,\n",
       " 'read': 31,\n",
       " 'good': 32,\n",
       " 'great': 33,\n",
       " 'at': 34,\n",
       " 'movie': 35,\n",
       " 'they': 36,\n",
       " 'just': 37,\n",
       " 'about': 38,\n",
       " 'from': 39,\n",
       " 'or': 40,\n",
       " 'would': 41,\n",
       " 'an': 42,\n",
       " 'me': 43,\n",
       " 'out': 44,\n",
       " 'what': 45,\n",
       " 'has': 46,\n",
       " 'more': 47,\n",
       " 'by': 48,\n",
       " 'time': 49,\n",
       " 'had': 50,\n",
       " 'when': 51,\n",
       " 'get': 52,\n",
       " 'will': 53,\n",
       " \"it's\": 54,\n",
       " 'up': 55,\n",
       " 'there': 56,\n",
       " 'no': 57,\n",
       " 'only': 58,\n",
       " 'your': 59,\n",
       " 'can': 60,\n",
       " \"don't\": 61,\n",
       " 'his': 62,\n",
       " 'really': 63,\n",
       " 'who': 64,\n",
       " 'some': 65,\n",
       " 'he': 66,\n",
       " 'well': 67,\n",
       " 'first': 68,\n",
       " 'her': 69,\n",
       " 'much': 70,\n",
       " 'than': 71,\n",
       " 'even': 72,\n",
       " 'do': 73,\n",
       " 'story': 74,\n",
       " 'because': 75,\n",
       " 'them': 76,\n",
       " 'other': 77,\n",
       " 'after': 78,\n",
       " 'buy': 79,\n",
       " 'we': 80,\n",
       " 'were': 81,\n",
       " 'too': 82,\n",
       " 'which': 83,\n",
       " 'she': 84,\n",
       " 'how': 85,\n",
       " 'love': 86,\n",
       " 'these': 87,\n",
       " 'been': 88,\n",
       " 'better': 89,\n",
       " 'best': 90,\n",
       " 'could': 91,\n",
       " 'any': 92,\n",
       " 'into': 93,\n",
       " 'their': 94,\n",
       " 'did': 95,\n",
       " 'books': 96,\n",
       " 'am': 97,\n",
       " 'also': 98,\n",
       " 'work': 99,\n",
       " 'product': 100,\n",
       " 'think': 101,\n",
       " 'then': 102,\n",
       " 'way': 103,\n",
       " \"i'm\": 104,\n",
       " 'its': 105,\n",
       " 'most': 106,\n",
       " 'ever': 107,\n",
       " 'make': 108,\n",
       " 'little': 109,\n",
       " 'many': 110,\n",
       " 'bad': 111,\n",
       " 'over': 112,\n",
       " 'see': 113,\n",
       " 'cd': 114,\n",
       " 'money': 115,\n",
       " 'now': 116,\n",
       " 'never': 117,\n",
       " 'new': 118,\n",
       " 'people': 119,\n",
       " 'does': 120,\n",
       " 'back': 121,\n",
       " 'film': 122,\n",
       " 'music': 123,\n",
       " 'reading': 124,\n",
       " 'know': 125,\n",
       " 'should': 126,\n",
       " 'bought': 127,\n",
       " 'got': 128,\n",
       " 'use': 129,\n",
       " '2': 130,\n",
       " 'made': 131,\n",
       " 'want': 132,\n",
       " 'still': 133,\n",
       " 'off': 134,\n",
       " 'find': 135,\n",
       " 'recommend': 136,\n",
       " \"didn't\": 137,\n",
       " 'two': 138,\n",
       " 'album': 139,\n",
       " 'life': 140,\n",
       " 'game': 141,\n",
       " 'years': 142,\n",
       " 'dvd': 143,\n",
       " 'found': 144,\n",
       " 'say': 145,\n",
       " 'go': 146,\n",
       " \"i've\": 147,\n",
       " 'old': 148,\n",
       " 'again': 149,\n",
       " 'thought': 150,\n",
       " 'through': 151,\n",
       " 'same': 152,\n",
       " 'every': 153,\n",
       " 'while': 154,\n",
       " 'quality': 155,\n",
       " 'thing': 156,\n",
       " 'another': 157,\n",
       " \"can't\": 158,\n",
       " 'characters': 159,\n",
       " 'before': 160,\n",
       " 'down': 161,\n",
       " 'worth': 162,\n",
       " 'put': 163,\n",
       " 'something': 164,\n",
       " 'must': 165,\n",
       " 'why': 166,\n",
       " \"doesn't\": 167,\n",
       " 'written': 168,\n",
       " 'few': 169,\n",
       " 'long': 170,\n",
       " 'being': 171,\n",
       " 'version': 172,\n",
       " 'hard': 173,\n",
       " 'those': 174,\n",
       " 'give': 175,\n",
       " 'our': 176,\n",
       " 'where': 177,\n",
       " '1': 178,\n",
       " 'lot': 179,\n",
       " 'used': 180,\n",
       " 'going': 181,\n",
       " 'anyone': 182,\n",
       " 'makes': 183,\n",
       " 'waste': 184,\n",
       " 'nothing': 185,\n",
       " 'however': 186,\n",
       " '3': 187,\n",
       " 'world': 188,\n",
       " 'watch': 189,\n",
       " 'amazon': 190,\n",
       " 'looking': 191,\n",
       " 'far': 192,\n",
       " 'times': 193,\n",
       " 'here': 194,\n",
       " 'such': 195,\n",
       " 'fun': 196,\n",
       " 'look': 197,\n",
       " '5': 198,\n",
       " 'excellent': 199,\n",
       " 'need': 200,\n",
       " 'boring': 201,\n",
       " 'plot': 202,\n",
       " 'series': 203,\n",
       " 'year': 204,\n",
       " 'enough': 205,\n",
       " 'real': 206,\n",
       " 'easy': 207,\n",
       " 'end': 208,\n",
       " 'sound': 209,\n",
       " 'big': 210,\n",
       " 'interesting': 211,\n",
       " 'novel': 212,\n",
       " 'movies': 213,\n",
       " 'songs': 214,\n",
       " 'without': 215,\n",
       " 'price': 216,\n",
       " 'though': 217,\n",
       " 'own': 218,\n",
       " 'feel': 219,\n",
       " 'take': 220,\n",
       " 'day': 221,\n",
       " 'video': 222,\n",
       " 'disappointed': 223,\n",
       " 'since': 224,\n",
       " 'works': 225,\n",
       " 'original': 226,\n",
       " 'different': 227,\n",
       " 'right': 228,\n",
       " 'set': 229,\n",
       " 'last': 230,\n",
       " 'author': 231,\n",
       " 'things': 232,\n",
       " 'worst': 233,\n",
       " 'actually': 234,\n",
       " 'may': 235,\n",
       " 'around': 236,\n",
       " 'fan': 237,\n",
       " 'pretty': 238,\n",
       " 'classic': 239,\n",
       " 'nice': 240,\n",
       " '4': 241,\n",
       " 'him': 242,\n",
       " 'reviews': 243,\n",
       " 'sure': 244,\n",
       " 'us': 245,\n",
       " 'character': 246,\n",
       " 'stars': 247,\n",
       " 'seen': 248,\n",
       " 'part': 249,\n",
       " 'play': 250,\n",
       " 'come': 251,\n",
       " 'both': 252,\n",
       " 'away': 253,\n",
       " 'keep': 254,\n",
       " 'always': 255,\n",
       " 'said': 256,\n",
       " 'song': 257,\n",
       " 'high': 258,\n",
       " 'wonderful': 259,\n",
       " 'review': 260,\n",
       " 'enjoy': 261,\n",
       " 'seems': 262,\n",
       " 'each': 263,\n",
       " 'bit': 264,\n",
       " 'understand': 265,\n",
       " 'second': 266,\n",
       " 'try': 267,\n",
       " 'item': 268,\n",
       " 'problem': 269,\n",
       " 'writing': 270,\n",
       " 'information': 271,\n",
       " 'yet': 272,\n",
       " 'purchased': 273,\n",
       " 'maybe': 274,\n",
       " 'came': 275,\n",
       " 'anything': 276,\n",
       " 'believe': 277,\n",
       " 'loved': 278,\n",
       " 'almost': 279,\n",
       " 'fact': 280,\n",
       " 'show': 281,\n",
       " 'poor': 282,\n",
       " 'everything': 283,\n",
       " 'small': 284,\n",
       " 'once': 285,\n",
       " 'man': 286,\n",
       " 'quite': 287,\n",
       " 'tried': 288,\n",
       " 'action': 289,\n",
       " 'highly': 290,\n",
       " 'instead': 291,\n",
       " 'whole': 292,\n",
       " 'next': 293,\n",
       " 'less': 294,\n",
       " 'point': 295,\n",
       " 'special': 296,\n",
       " 'three': 297,\n",
       " \"couldn't\": 298,\n",
       " 'family': 299,\n",
       " 'getting': 300,\n",
       " 'using': 301,\n",
       " 'probably': 302,\n",
       " 'trying': 303,\n",
       " 'least': 304,\n",
       " 'true': 305,\n",
       " '10': 306,\n",
       " 'help': 307,\n",
       " 'done': 308,\n",
       " 'star': 309,\n",
       " 'gets': 310,\n",
       " 'full': 311,\n",
       " 'watching': 312,\n",
       " 'having': 313,\n",
       " 'able': 314,\n",
       " 'favorite': 315,\n",
       " 'fine': 316,\n",
       " 'size': 317,\n",
       " 'style': 318,\n",
       " 'live': 319,\n",
       " \"isn't\": 320,\n",
       " 'enjoyed': 321,\n",
       " 'ago': 322,\n",
       " 'especially': 323,\n",
       " 'job': 324,\n",
       " 'minutes': 325,\n",
       " 'amazing': 326,\n",
       " 'together': 327,\n",
       " 'wrong': 328,\n",
       " 'stories': 329,\n",
       " 'definitely': 330,\n",
       " 'went': 331,\n",
       " \"you're\": 332,\n",
       " 'let': 333,\n",
       " \"that's\": 334,\n",
       " 'might': 335,\n",
       " 'school': 336,\n",
       " 'pages': 337,\n",
       " 'perfect': 338,\n",
       " \"wasn't\": 339,\n",
       " 'ordered': 340,\n",
       " 'someone': 341,\n",
       " 'top': 342,\n",
       " 'toy': 343,\n",
       " 'beautiful': 344,\n",
       " 'although': 345,\n",
       " 'short': 346,\n",
       " 'cover': 347,\n",
       " 'liked': 348,\n",
       " 'terrible': 349,\n",
       " \"won't\": 350,\n",
       " 'else': 351,\n",
       " 'fit': 352,\n",
       " 'horrible': 353,\n",
       " 'piece': 354,\n",
       " 'place': 355,\n",
       " 'please': 356,\n",
       " 'buying': 357,\n",
       " 'several': 358,\n",
       " 'wish': 359,\n",
       " 'rather': 360,\n",
       " 'card': 361,\n",
       " 'kind': 362,\n",
       " 'wanted': 363,\n",
       " 'half': 364,\n",
       " 'heard': 365,\n",
       " 'already': 366,\n",
       " 'listen': 367,\n",
       " 'reason': 368,\n",
       " 'received': 369,\n",
       " 'until': 370,\n",
       " 'comes': 371,\n",
       " 'acting': 372,\n",
       " 'history': 373,\n",
       " 'months': 374,\n",
       " 'ok': 375,\n",
       " 'everyone': 376,\n",
       " 'mind': 377,\n",
       " 'tell': 378,\n",
       " 'class': 379,\n",
       " 'page': 380,\n",
       " 'computer': 381,\n",
       " 'between': 382,\n",
       " 'others': 383,\n",
       " 'days': 384,\n",
       " 'took': 385,\n",
       " 'purchase': 386,\n",
       " 'picture': 387,\n",
       " 'power': 388,\n",
       " 'reader': 389,\n",
       " 'gave': 390,\n",
       " 'christmas': 391,\n",
       " 'camera': 392,\n",
       " 'line': 393,\n",
       " 'order': 394,\n",
       " 'house': 395,\n",
       " 'start': 396,\n",
       " 'kids': 397,\n",
       " 'bed': 398,\n",
       " 'completely': 399,\n",
       " 'takes': 400,\n",
       " 'funny': 401,\n",
       " 'making': 402,\n",
       " 'simply': 403,\n",
       " 'saw': 404,\n",
       " 'night': 405,\n",
       " 'fast': 406,\n",
       " 'effects': 407,\n",
       " 'box': 408,\n",
       " 'either': 409,\n",
       " 'children': 410,\n",
       " 'difficult': 411,\n",
       " 'return': 412,\n",
       " 'happy': 413,\n",
       " 'collection': 414,\n",
       " 'idea': 415,\n",
       " 'light': 416,\n",
       " 'looks': 417,\n",
       " 'hours': 418,\n",
       " 'english': 419,\n",
       " 'overall': 420,\n",
       " 'cheap': 421,\n",
       " \"i'd\": 422,\n",
       " 'seem': 423,\n",
       " 'young': 424,\n",
       " 'needed': 425,\n",
       " 'myself': 426,\n",
       " 'son': 427,\n",
       " 'cannot': 428,\n",
       " 'future': 429,\n",
       " 'player': 430,\n",
       " \"you'll\": 431,\n",
       " 'couple': 432,\n",
       " 'air': 433,\n",
       " 'words': 434,\n",
       " 'felt': 435,\n",
       " 'absolutely': 436,\n",
       " 'save': 437,\n",
       " 'battery': 438,\n",
       " 'rest': 439,\n",
       " 'problems': 440,\n",
       " 'edition': 441,\n",
       " 'person': 442,\n",
       " 'home': 443,\n",
       " 'left': 444,\n",
       " 'started': 445,\n",
       " 'band': 446,\n",
       " 'lost': 447,\n",
       " 'worked': 448,\n",
       " 'truly': 449,\n",
       " 'wait': 450,\n",
       " 'ray': 451,\n",
       " 'side': 452,\n",
       " 'scenes': 453,\n",
       " 'working': 454,\n",
       " 'etc': 455,\n",
       " 'thinking': 456,\n",
       " 'simple': 457,\n",
       " 'women': 458,\n",
       " 'goes': 459,\n",
       " 'sense': 460,\n",
       " 'stuff': 461,\n",
       " 'stay': 462,\n",
       " 'fiction': 463,\n",
       " 'woman': 464,\n",
       " 'itself': 465,\n",
       " 'free': 466,\n",
       " 'title': 467,\n",
       " \"wouldn't\": 468,\n",
       " 'experience': 469,\n",
       " 'guess': 470,\n",
       " 'hear': 471,\n",
       " 'entire': 472,\n",
       " 'awesome': 473,\n",
       " 'track': 474,\n",
       " 'friends': 475,\n",
       " 'missing': 476,\n",
       " 'playing': 477,\n",
       " 'american': 478,\n",
       " 'course': 479,\n",
       " 'ending': 480,\n",
       " 'cool': 481,\n",
       " 'company': 482,\n",
       " 'case': 483,\n",
       " 'gives': 484,\n",
       " 'daughter': 485,\n",
       " 'under': 486,\n",
       " 'slow': 487,\n",
       " 'shows': 488,\n",
       " 'hope': 489,\n",
       " 'support': 490,\n",
       " 'extremely': 491,\n",
       " 'copy': 492,\n",
       " 'dont': 493,\n",
       " 'type': 494,\n",
       " 'based': 495,\n",
       " \"i'll\": 496,\n",
       " 'parts': 497,\n",
       " 'language': 498,\n",
       " 'horror': 499,\n",
       " 'complete': 500,\n",
       " 'main': 501,\n",
       " 'pictures': 502,\n",
       " 'yourself': 503,\n",
       " 'finally': 504,\n",
       " 'expected': 505,\n",
       " 'girl': 506,\n",
       " 'glad': 507,\n",
       " '6': 508,\n",
       " 'played': 509,\n",
       " 'past': 510,\n",
       " 'learn': 511,\n",
       " 'tracks': 512,\n",
       " 'self': 513,\n",
       " 'unfortunately': 514,\n",
       " 'along': 515,\n",
       " 'hold': 516,\n",
       " 'run': 517,\n",
       " 'comfortable': 518,\n",
       " 'ones': 519,\n",
       " 'care': 520,\n",
       " 'garbage': 521,\n",
       " 'says': 522,\n",
       " 'needs': 523,\n",
       " 'dark': 524,\n",
       " 'sounds': 525,\n",
       " 'name': 526,\n",
       " 'scary': 527,\n",
       " 'yes': 528,\n",
       " 'today': 529,\n",
       " 'unless': 530,\n",
       " 'easily': 531,\n",
       " 'during': 532,\n",
       " 'release': 533,\n",
       " 'totally': 534,\n",
       " 'huge': 535,\n",
       " 'entertaining': 536,\n",
       " 'available': 537,\n",
       " 'later': 538,\n",
       " 'doing': 539,\n",
       " 'view': 540,\n",
       " 'mr': 541,\n",
       " 'called': 542,\n",
       " '1984': 543,\n",
       " 'kept': 544,\n",
       " 'tv': 545,\n",
       " 'pay': 546,\n",
       " 'loves': 547,\n",
       " 'society': 548,\n",
       " 'remember': 549,\n",
       " 'black': 550,\n",
       " 'guy': 551,\n",
       " 'told': 552,\n",
       " 'gift': 553,\n",
       " \"there's\": 554,\n",
       " 'single': 555,\n",
       " 'heart': 556,\n",
       " 'given': 557,\n",
       " 'seemed': 558,\n",
       " 'five': 559,\n",
       " 'sometimes': 560,\n",
       " 'recommended': 561,\n",
       " 'war': 562,\n",
       " 'service': 563,\n",
       " 'write': 564,\n",
       " 'friend': 565,\n",
       " 'store': 566,\n",
       " 'turn': 567,\n",
       " 'opinion': 568,\n",
       " 'child': 569,\n",
       " 'color': 570,\n",
       " 'word': 571,\n",
       " 'expect': 572,\n",
       " 'sorry': 573,\n",
       " 'early': 574,\n",
       " 'disappointing': 575,\n",
       " 'within': 576,\n",
       " 'weeks': 577,\n",
       " 'description': 578,\n",
       " 'interested': 579,\n",
       " 'become': 580,\n",
       " 'deal': 581,\n",
       " 'text': 582,\n",
       " 'stop': 583,\n",
       " 'fans': 584,\n",
       " 'cut': 585,\n",
       " 'clear': 586,\n",
       " 'age': 587,\n",
       " 'decided': 588,\n",
       " 'often': 589,\n",
       " 'rock': 590,\n",
       " 'mean': 591,\n",
       " 'looked': 592,\n",
       " 'stupid': 593,\n",
       " 'head': 594,\n",
       " 'worse': 595,\n",
       " 'kindle': 596,\n",
       " 'beginning': 597,\n",
       " 'awful': 598,\n",
       " 'lots': 599,\n",
       " 'low': 600,\n",
       " 'paper': 601,\n",
       " 'perhaps': 602,\n",
       " 'follow': 603,\n",
       " 'human': 604,\n",
       " 'week': 605,\n",
       " 'print': 606,\n",
       " 'change': 607,\n",
       " 'important': 608,\n",
       " 'screen': 609,\n",
       " 'value': 610,\n",
       " 'science': 611,\n",
       " 'non': 612,\n",
       " 'level': 613,\n",
       " 'number': 614,\n",
       " 'ideas': 615,\n",
       " 'fantastic': 616,\n",
       " 'art': 617,\n",
       " 'games': 618,\n",
       " 'god': 619,\n",
       " 'wear': 620,\n",
       " 'modern': 621,\n",
       " 'john': 622,\n",
       " 'watched': 623,\n",
       " 'close': 624,\n",
       " 'strong': 625,\n",
       " 'supposed': 626,\n",
       " 'chapter': 627,\n",
       " 'arrived': 628,\n",
       " 'husband': 629,\n",
       " 'wrote': 630,\n",
       " 'dance': 631,\n",
       " 'listening': 632,\n",
       " 'charger': 633,\n",
       " 'voice': 634,\n",
       " 'u': 635,\n",
       " 'films': 636,\n",
       " 'tale': 637,\n",
       " \"haven't\": 638,\n",
       " 'add': 639,\n",
       " 'four': 640,\n",
       " 'month': 641,\n",
       " 'sad': 642,\n",
       " 'usually': 643,\n",
       " 'replacement': 644,\n",
       " 'hot': 645,\n",
       " 'ear': 646,\n",
       " 'printer': 647,\n",
       " 'quickly': 648,\n",
       " 'disc': 649,\n",
       " 'scene': 650,\n",
       " 'literature': 651,\n",
       " 'taking': 652,\n",
       " 'check': 653,\n",
       " '8': 654,\n",
       " 'cost': 655,\n",
       " 'material': 656,\n",
       " 's': 657,\n",
       " 'poorly': 658,\n",
       " 'disappointment': 659,\n",
       " 'oh': 660,\n",
       " 'large': 661,\n",
       " 'longer': 662,\n",
       " 'sent': 663,\n",
       " 'helpful': 664,\n",
       " 'finish': 665,\n",
       " 'spent': 666,\n",
       " 'soon': 667,\n",
       " 'writer': 668,\n",
       " '20': 669,\n",
       " 'audio': 670,\n",
       " 'room': 671,\n",
       " 'software': 672,\n",
       " 'e': 673,\n",
       " 'blu': 674,\n",
       " 'pieces': 675,\n",
       " 'plastic': 676,\n",
       " 'spend': 677,\n",
       " 'extra': 678,\n",
       " 'lives': 679,\n",
       " 'tape': 680,\n",
       " 'junk': 681,\n",
       " 'shipping': 682,\n",
       " 'forward': 683,\n",
       " 'useful': 684,\n",
       " 'actors': 685,\n",
       " 'fall': 686,\n",
       " 'library': 687,\n",
       " 'unit': 688,\n",
       " 'attention': 689,\n",
       " 'matter': 690,\n",
       " 'example': 691,\n",
       " '30': 692,\n",
       " 'otherwise': 693,\n",
       " 'annoying': 694,\n",
       " 'charge': 695,\n",
       " 'features': 696,\n",
       " 'certainly': 697,\n",
       " 'message': 698,\n",
       " 'albums': 699,\n",
       " 'boots': 700,\n",
       " 'interest': 701,\n",
       " 'hand': 702,\n",
       " 'white': 703,\n",
       " 'except': 704,\n",
       " 'control': 705,\n",
       " 'happened': 706,\n",
       " 'open': 707,\n",
       " 'group': 708,\n",
       " 'broke': 709,\n",
       " 'taken': 710,\n",
       " 'humor': 711,\n",
       " '15': 712,\n",
       " 'water': 713,\n",
       " 'previous': 714,\n",
       " 'apart': 715,\n",
       " 'against': 716,\n",
       " 'anyway': 717,\n",
       " 'pop': 718,\n",
       " 'plus': 719,\n",
       " 'c': 720,\n",
       " 'thank': 721,\n",
       " 'figure': 722,\n",
       " 'performance': 723,\n",
       " 'hate': 724,\n",
       " 'none': 725,\n",
       " '100': 726,\n",
       " 'beyond': 727,\n",
       " 'general': 728,\n",
       " 'dog': 729,\n",
       " 'brother': 730,\n",
       " 'guys': 731,\n",
       " 'actual': 732,\n",
       " 'adapter': 733,\n",
       " 'wonder': 734,\n",
       " 'suggest': 735,\n",
       " 'miss': 736,\n",
       " 'pick': 737,\n",
       " '451': 738,\n",
       " 'inside': 739,\n",
       " 'bottom': 740,\n",
       " 'happen': 741,\n",
       " 'turned': 742,\n",
       " 'system': 743,\n",
       " 'face': 744,\n",
       " 'novels': 745,\n",
       " 'paid': 746,\n",
       " 'agree': 747,\n",
       " 'known': 748,\n",
       " 'call': 749,\n",
       " 'test': 750,\n",
       " 'enjoyable': 751,\n",
       " 're': 752,\n",
       " 'due': 753,\n",
       " 'guide': 754,\n",
       " 'hour': 755,\n",
       " 'possible': 756,\n",
       " 'content': 757,\n",
       " 'exactly': 758,\n",
       " 'break': 759,\n",
       " 'blue': 760,\n",
       " 'returned': 761,\n",
       " 'space': 762,\n",
       " 'country': 763,\n",
       " 'decent': 764,\n",
       " 'creative': 765,\n",
       " 'record': 766,\n",
       " 'graphics': 767,\n",
       " 'readers': 768,\n",
       " 'expensive': 769,\n",
       " 'death': 770,\n",
       " 'wife': 771,\n",
       " '7': 772,\n",
       " \"they're\": 773,\n",
       " 'plug': 774,\n",
       " 'seller': 775,\n",
       " 'hoping': 776,\n",
       " 'sleep': 777,\n",
       " 'coming': 778,\n",
       " 'condition': 779,\n",
       " 'cable': 780,\n",
       " 'personal': 781,\n",
       " 'feeling': 782,\n",
       " 'leave': 783,\n",
       " 'included': 784,\n",
       " 'basic': 785,\n",
       " 'metal': 786,\n",
       " 'apple': 787,\n",
       " 'throughout': 788,\n",
       " 'clearly': 789,\n",
       " 'bother': 790,\n",
       " 'thanks': 791,\n",
       " 'brand': 792,\n",
       " 'greatest': 793,\n",
       " 'com': 794,\n",
       " 'stopped': 795,\n",
       " 'total': 796,\n",
       " 'sex': 797,\n",
       " 'effort': 798,\n",
       " 'crap': 799,\n",
       " 'quick': 800,\n",
       " 'season': 801,\n",
       " 'detail': 802,\n",
       " 'released': 803,\n",
       " 'giving': 804,\n",
       " 'wants': 805,\n",
       " 'confusing': 806,\n",
       " 'mine': 807,\n",
       " 'issues': 808,\n",
       " 'stand': 809,\n",
       " 'design': 810,\n",
       " 'beat': 811,\n",
       " 'production': 812,\n",
       " 'chance': 813,\n",
       " 'fire': 814,\n",
       " 'skin': 815,\n",
       " 'move': 816,\n",
       " 'hit': 817,\n",
       " 'products': 818,\n",
       " 'concert': 819,\n",
       " 'canon': 820,\n",
       " 'knew': 821,\n",
       " 'dead': 822,\n",
       " 'eyes': 823,\n",
       " 'lack': 824,\n",
       " 'sony': 825,\n",
       " 'major': 826,\n",
       " 'uses': 827,\n",
       " '9': 828,\n",
       " 'la': 829,\n",
       " 'somewhat': 830,\n",
       " 'foundation': 831,\n",
       " 'fell': 832,\n",
       " 'fascinating': 833,\n",
       " 'seeing': 834,\n",
       " 'middle': 835,\n",
       " 'third': 836,\n",
       " 'rice': 837,\n",
       " 'cave': 838,\n",
       " 'learned': 839,\n",
       " 'men': 840,\n",
       " 'surprised': 841,\n",
       " 'romance': 842,\n",
       " 'boy': 843,\n",
       " 'addition': 844,\n",
       " 'dull': 845,\n",
       " 'subject': 846,\n",
       " 'thin': 847,\n",
       " 'plain': 848,\n",
       " 'baby': 849,\n",
       " 'waiting': 850,\n",
       " 'starts': 851,\n",
       " 'hp': 852,\n",
       " 'twice': 853,\n",
       " 'talking': 854,\n",
       " 'older': 855,\n",
       " 'excited': 856,\n",
       " \"he's\": 857,\n",
       " 'talk': 858,\n",
       " 'despite': 859,\n",
       " 'note': 860,\n",
       " 'jack': 861,\n",
       " 'sort': 862,\n",
       " 'wow': 863,\n",
       " 'manson': 864,\n",
       " 'beware': 865,\n",
       " 'behind': 866,\n",
       " 'b': 867,\n",
       " 'running': 868,\n",
       " 'ended': 869,\n",
       " 'similar': 870,\n",
       " 'basically': 871,\n",
       " 'body': 872,\n",
       " 'broken': 873,\n",
       " 'orwell': 874,\n",
       " 'forget': 875,\n",
       " 'send': 876,\n",
       " 'pair': 877,\n",
       " 'memory': 878,\n",
       " 'incredible': 879,\n",
       " 'historical': 880,\n",
       " '0': 881,\n",
       " '50': 882,\n",
       " 'serious': 883,\n",
       " 'avoid': 884,\n",
       " 'de': 885,\n",
       " 'warning': 886,\n",
       " 'including': 887,\n",
       " 'amount': 888,\n",
       " 'genre': 889,\n",
       " 'parents': 890,\n",
       " 'knowledge': 891,\n",
       " 'bradbury': 892,\n",
       " 'soundtrack': 893,\n",
       " 'brilliant': 894,\n",
       " 'useless': 895,\n",
       " 'max': 896,\n",
       " 'list': 897,\n",
       " 'boot': 898,\n",
       " 'cast': 899,\n",
       " 'super': 900,\n",
       " 'rating': 901,\n",
       " 'customer': 902,\n",
       " 'truth': 903,\n",
       " 'stick': 904,\n",
       " 'fahrenheit': 905,\n",
       " 'america': 906,\n",
       " 'late': 907,\n",
       " 'reference': 908,\n",
       " 'moving': 909,\n",
       " 'skip': 910,\n",
       " 'trip': 911,\n",
       " 'living': 912,\n",
       " 'government': 913,\n",
       " 'okay': 914,\n",
       " 'hands': 915,\n",
       " 'finished': 916,\n",
       " 'alot': 917,\n",
       " 'car': 918,\n",
       " 'lyrics': 919,\n",
       " 'weight': 920,\n",
       " 'date': 921,\n",
       " 'radio': 922,\n",
       " 'students': 923,\n",
       " 'fi': 924,\n",
       " 'themselves': 925,\n",
       " 'waist': 926,\n",
       " 'stuck': 927,\n",
       " 'transformers': 928,\n",
       " 'ms': 929,\n",
       " 'digital': 930,\n",
       " 'insight': 931,\n",
       " 'pass': 932,\n",
       " '12': 933,\n",
       " 'impressed': 934,\n",
       " 'trash': 935,\n",
       " 'heavy': 936,\n",
       " 'changed': 937,\n",
       " 'weak': 938,\n",
       " 'events': 939,\n",
       " 'seriously': 940,\n",
       " 'replace': 941,\n",
       " 'clean': 942,\n",
       " 'shame': 943,\n",
       " 'authors': 944,\n",
       " 'questions': 945,\n",
       " 'correct': 946,\n",
       " 'mother': 947,\n",
       " 'study': 948,\n",
       " 'happens': 949,\n",
       " 'pleased': 950,\n",
       " 'keeps': 951,\n",
       " 'mix': 952,\n",
       " 'model': 953,\n",
       " 'form': 954,\n",
       " 'himself': 955,\n",
       " 'adventure': 956,\n",
       " 'informative': 957,\n",
       " 'shot': 958,\n",
       " 'recently': 959,\n",
       " 'haunting': 960,\n",
       " 'windows': 961,\n",
       " 'century': 962,\n",
       " 'alone': 963,\n",
       " 'bored': 964,\n",
       " 'exciting': 965,\n",
       " 'cute': 966,\n",
       " 'cause': 967,\n",
       " 'development': 968,\n",
       " 'details': 969,\n",
       " 'putting': 970,\n",
       " 'crazy': 971,\n",
       " 'masterpiece': 972,\n",
       " 'imagine': 973,\n",
       " 'grade': 974,\n",
       " 'straight': 975,\n",
       " 'ridiculous': 976,\n",
       " 'background': 977,\n",
       " 'flat': 978,\n",
       " 'across': 979,\n",
       " 'research': 980,\n",
       " 'issue': 981,\n",
       " 'appears': 982,\n",
       " 'sci': 983,\n",
       " 'learning': 984,\n",
       " 'advice': 985,\n",
       " 'front': 986,\n",
       " 'wasted': 987,\n",
       " 'forever': 988,\n",
       " 'mattress': 989,\n",
       " 'means': 990,\n",
       " 'turns': 991,\n",
       " 'comedy': 992,\n",
       " 'difference': 993,\n",
       " 'nearly': 994,\n",
       " 'reviewers': 995,\n",
       " 'incredibly': 996,\n",
       " 'recording': 997,\n",
       " 'obviously': 998,\n",
       " 'business': 999,\n",
       " 'introduction': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://medium.com/@Petuum/embeddings-a-matrix-of-meaning-4de877c9aa27\n",
    "\n",
    "type(embedding_matrix)\n",
    "#train_x\n",
    "#train_seq_x\n",
    "embedding_matrix\n",
    "word_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Models as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['es wealth tribe im7 dede dimensions den kino artec elite',\n",
       " 'la de un y el los del que jewish organ',\n",
       " 'sexual listened foot email detailed till allow award wrong incorrect',\n",
       " 'tess j smith hardy term grow heck bd cornwell arms',\n",
       " '1984 orwell brother winston timeless relevant aspects big commentary spanish',\n",
       " 'stargate haiku holmes meets alive yellow exceptional hound explanation acted',\n",
       " 'error simpletech topics chemistry card manage alarm designs firmware poignant',\n",
       " 'crawford harry junior noir jam kelly joan vcr gluten palance',\n",
       " 'manson max mad account range artists mountain blah office kerouac',\n",
       " 'dr intelligent cat prime labor keel diabetes gammell stephen instructor',\n",
       " 'the i it and to a this is for of',\n",
       " 'river tism michigan ca rounded visually naader danner vein navy',\n",
       " 'ear sandler adam g4 nights powerbook william headset cake castle',\n",
       " 'freud intelligence bluray economy liking users lunchbox assembled anthology neanderthal',\n",
       " 'chris bebel napoleon profanity pat captain wished di tolerate yello',\n",
       " 'nirvana shirt knock pretentious land psychological dummy kurt biography ap',\n",
       " 'continues hasn pratchett janet gods met laughs cities discworld whoever',\n",
       " 'hockey tomcat usmle offering outdoor criminals clippers seldom trading klein',\n",
       " 'c diane lane en scenery le h stretch sandra plate',\n",
       " 'the and a of to i this is it in']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(topic_word)\n",
    "topic_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Model Building\n",
    "\n",
    "The final step in the text classification framework is to train a classifier using the features created in the previous step. There are many different choices of machine learning models which can be used to train a final model. We will implement following different classifiers for this purpose:\n",
    "\n",
    "    * Naive Bayes Classifier\n",
    "    * Linear Classifier\n",
    "    * Support Vector Machine\n",
    "    * Bagging Models\n",
    "    * Boosting Models\n",
    "    * Shallow Neural Networks\n",
    "    * Deep Neural Networks\n",
    "        * Convolutional Neural Network (CNN)\n",
    "        * Long Short Term Modelr (LSTM)\n",
    "        * Gated Recurrent Unit (GRU)\n",
    "        * Bidirectional RNN\n",
    "        * Recurrent Convolutional Neural Network (RCNN)\n",
    "        * Other Variants of Deep Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
