{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse der Tweets von Bundestagsabgeordneten\n",
    "## 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "\n",
    "db = client['Twitter']\n",
    "All_Tweets_collection = db['Twitter_mdp_extend_datetime']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "\n",
    "\n",
    "# sample_df = pd.DataFrame(list(All_Tweets_collection.aggregate([ {\"$sample\": {\"size\": 50 }}], \n",
    "#                                      allowDiskUse=True\n",
    "#                                    )))\n",
    "\n",
    "\n",
    "time_sample_df = pd.DataFrame(list(All_Tweets_collection.find( {\n",
    "            'created_at_datetime': {'$gte': datetime.datetime(2020,2,1,0,0,0),\n",
    "                                    '$lt': datetime.datetime(2020,5,1,0,0,0)},\n",
    "            'retweeted_status': None\n",
    "            })\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new Column date_time - erledigt in Datenbank\n",
    "\n",
    "created_at is saved in Mongodb as string and needs to be converted to a datetime format\n",
    "\n",
    "https://www.programiz.com/python-programming/datetime/strptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# created_at_datetime = []\n",
    "# for date in sample_df.created_at:\n",
    "#     date_string = date\n",
    "#     date_object = datetime.strptime(date_string, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "#     created_at_datetime.append(date_object)\n",
    "    \n",
    "# sample_df[\"date_time\"] = created_at_datetime\n",
    "# #sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Retweets - erledig bei laden in Notebook\n",
    "Zu erkennen sind Retweets am \"RT\" vor dem Text bzw. an der Spalte \"retweeted_status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_text = sample_df[[\"created_at_datetime\", \"full_text\", 'retweeted_status']]\n",
    "# text_df = pd.DataFrame(sample_text)\n",
    "# text_nRT = sample_text[pd.isnull(sample_text['retweeted_status'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      wiedereröffnung meiner büros in gera erfurt sömmerda ab morgen 14 april 2020 freilich unter beachtung der vorschriften v a zu abstand personenzahl hygienepositionspapier der afd-bundestagsfraktion hier                                                          \n",
       "1      hat eigentlich schon irgendeine altpartei oder altfraktion einen plan/ eine strategie den stillstand zu beenden und  wieder hochzufahren die afd hat auch hier mal wieder die nase vorn                                                                            \n",
       "2      kein mensch braucht umfaller/ fdp weder in der politik noch in den medien                                                                                                                                                                                          \n",
       "3      ganz einfachweil es zumal auf dauer niemals ein datenschutzgerechtes tool geben und das ein weiterer großer schritt richtung überwachungsstaat sein wirdich wünsche mir dringend etwas mehr problembewusstsein und kritische distanz beim datenschutzbeauftragten  \n",
       "4      wer hat noch nicht wer will nochmal brandnerauftelegram afd berlin bundestag brandner                                                                                                                                                                              \n",
       "5      fordern wir schon lange endlich bewegen sich auch die altparteien afdwirkt                                                                                                                                                                                         \n",
       "6      zeitonline_pol fordern wir schon lange endlich bewegen sich auch die altparteien afdwirkt                                                                                                                                                                          \n",
       "7      fordern wir schon lange endlich bewegen sich auch die altparteien afdwirkt                                                                                                                                                                                         \n",
       "8      faznet fordern wir schon lange endlich bewegen sich auch die altparteien afdwirkt                                                                                                                                                                                  \n",
       "9      nicht mehr ganz so neu aber nach wie vor sehr gut lesbar hier runterladen                                                                                                                                                                                          \n",
       "10     froheostern erst jetzt auferstehung fastenzeitvorbei leckeressenvongaßmann erfurterlieblingsfleischer                                                                                                                                                              \n",
       "11     +++5fragen5antworten ==gt der weg aus der krise das osterfest feiern lassen lockerungen ab 1442020 angemessenesverhalten  hygiene fördern wirtschaft wieder hoch fahren+++                                                                                         \n",
       "12     junge männer                                                                                                                                                                                                                                                       \n",
       "13     hab’s leider verpaßtgrölten fsf oder kiz im hintergrund und hat er irgendwelche konzerte angekündigt                                                                                                                                                               \n",
       "14     von wegen einthemenpartei afd hier eine kleine  auswahl allein von demwas ich  so angestoßen habehinzukommen noch 88 weitere mdb-kollegen  zwei dutzend arbeitskreisedie karten gibt’s bei den fraktionsveranstaltungenzb bigafdwirkt berlin bundestag brandner    \n",
       "15     was für eine frageer darf nicht nur er mußund auch nicht nur mitreden sondern darüber entscheiden und zwar mit kanzlermehrheitafd nurnochafd demokratie berlin bundestag brandner                                                                                  \n",
       "16     20 männer attackieren frankfurter corona-polizeistreife                                                                                                                                                                                                            \n",
       "17     der ist doch bei den sozis mitglied oder                                                                                                                                                                                                                           \n",
       "18     fastenzeit neigt sich dem ende da darf man schon mal zu seinem erfurterlieblingsfleischer -metzger einkaufen und sich etwas beschenken lassen und das ist nur fürs frühstück - grillgut kommt später  an gaßmann - ich werd’s mir schmecken lassenerfurt thüringen \n",
       "19     lamentierlindner wieder mal auf afd-kurs - ein paar tage später freilich er mußte ja unser positionspapier erstmal lesen und kapierennurnochafd freiheit grundrechte berlin bundestag brandner                                                                     \n",
       "20     faznet c_lindner lamentierlindner wieder mal auf afd-kurs - ein paar tage später freilich er mußte ja unser positionspapier erstmal lesen und kapierennurnochafd freiheit grundrechte berlin bundestag brandner                                                    \n",
       "21     31-jähriger erstochen in auto gefunden                                                                                                                                                                                                                             \n",
       "22     liebe regierungen behörden und verfassungsgerichte artikel 19 abs 2 unseres grundgesetzes ist euch geläufigoderund warum fallen mir in letzter zeit immer wieder die karlsbaderbeschlüsse ein - obwohl sich geschichte doch angeblich nicht wiederholtfreiheit afd \n",
       "23     leipzig mutter 37 stirbt nach attacke auf sich und ihr baby                                                                                                                                                                                                        \n",
       "24     da besteht offenbar regelungsbedarfich mach mir mal gedanken dazu                                                                                                                                                                                                  \n",
       "25     wie wir von der afd immer schon gesagt haben - und damit auch hier wie immer richtiglagennurnochafd klimahysterie klimaner                                                                                                                                         \n",
       "26     auch erfurt an vielen stellen in der stadt entstehen neue - allerdings ziemlich kleine - grünflächen                                                                                                                                                               \n",
       "27     parkautomat in erfurt                                                                                                                                                                                                                                              \n",
       "28     das ist nicht wahr odercoronavirus sachsen will quarantäne-verweigerer in psychiatrien sperrenwer solche verrückten ideen hat sollte dort eingesperrt werdenrechtsstaat grundrechte                                                                                \n",
       "29     letzten mittwoch sprechstunden  aus meinem erfurt|er büro wer nicht durchgekommen istzu den özanrufen  rrverlangen e-post ankontaktbrandner-im-bundestagde o frage über                                                                                            \n",
       "                                                                                         ...                                                                                                                                                                              \n",
       "170    garten gera                                                                                                                                                                                                                                                        \n",
       "171    bestimmt nichtlinkeeinheitsmedien                                                                                                                                                                                                                                  \n",
       "172    das ging ja flott kam da ’ne anweisung - mit hinweis auf maaßensschicksal - aus dem kanzleramt                                                                                                                                                                     \n",
       "173    wiesbaden schüsse in biebrich - polizei mit neuen details zur tatwaffe                                                                                                                                                                                             \n",
       "174    wenn es nicht so traurig wäre danke an                                                                                                                                                                                                                             \n",
       "175    heil dieser heuchler hätte einfach unserem antrag zustimmen können/ sollen dann müßte er nun nicht so kindisch lamentierenaltparteien spd sozis                                                                                                                    \n",
       "176    tagesspiegel                                                                                                                                                                                                                                                       \n",
       "177                                                                                                                                                                                                                                                                       \n",
       "178    und zwar in übelster art und weise instrumentalisiert                                                                                                                                                                                                              \n",
       "179    jetzt wird das wirklich wichtige geregeltaltparteien                                                                                                                                                                                                               \n",
       "180                                                                                                                                                                                                                                                                       \n",
       "181    tagesspiegel                                                                                                                                                                                                                                                       \n",
       "182    huch meinungsverschiedenheiten zwischen den blockparteienramelüringen thüringen altparteien                                                                                                                                                                        \n",
       "183    läuft auch da was aus dem ruder                                                                                                                                                                                                                                    \n",
       "184                                                                                                                                                                                                                                                                       \n",
       "185    udohemmelgarn spdde hubertus_heil                                                                                                                                                                                                                                  \n",
       "186    spdde hubertus_heil                                                                                                                                                                                                                                                \n",
       "187    klimawandel  in zeiten von corona oder einfach mit klimahysterie übertriebenguten morgen aus dem verschneiten gera                                                                                                                                                 \n",
       "188    faz_politik                                                                                                                                                                                                                                                        \n",
       "189    drumheadberlin bild                                                                                                                                                                                                                                                \n",
       "190    afdimbundestag                                                                                                                                                                                                                                                     \n",
       "191    junge_freiheit                                                                                                                                                                                                                                                     \n",
       "192    panthea2019 brandy1vsop                                                                                                                                                                                                                                            \n",
       "193    immer wieder für ‘ne überraschung gut diese wirtschaftsweisen                                                                                                                                                                                                      \n",
       "194    markus_krall ronaldglaeser                                                                                                                                                                                                                                         \n",
       "195    fpiatov juergenbraunafd annabayern                                                                                                                                                                                                                                 \n",
       "196    hat die pis nicht die mehrheit für von der leyen gesichert                                                                                                                                                                                                         \n",
       "197                                                                                                                                                                                                                                                                       \n",
       "198    juergenbraunafd                                                                                                                                                                                                                                                    \n",
       "199    stimmt ja die klimamärchen gibt’s ja auch nochdanke an                                                                                                                                                                                                             \n",
       "Name: full_text_processed, Length: 200, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "import re\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # https://www.compart.com/de/unicode/block/U+1F900\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "sample_df=time_sample_df\n",
    "# Remove punctuation\n",
    "sample_df['full_text_processed'] = sample_df['full_text'].map(lambda x: re.sub('[,\\.!?#@\\\\n\"“„\\:;&\\(\\)]', '', x))\n",
    "# Remove Links\n",
    "sample_df['full_text_processed'] = sample_df['full_text_processed'].map(lambda x: re.sub('http.*', '', x))\n",
    "\n",
    "sample_df['full_text_processed'] = sample_df['full_text_processed'].map(lambda x: re.sub('amp', '', x))\n",
    "# Convert the titles to lowercase\n",
    "sample_df['full_text_processed'] = sample_df['full_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "sample_df['full_text_processed'] = sample_df['full_text_processed'].map(lambda x: remove_emoji(x))\n",
    "# Print out the first rows of papers\n",
    "sample_df['full_text_processed'].head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Durchsuche nach Tweets und sortiere aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print words that match certain words:\n",
    "#sample_df['full_text_processed'][sample_df['full_text_processed'].str.match(r'.*fröhlicher gruss.*')==True]\n",
    "\n",
    "# Remove Tweets that match certain words:\n",
    "sample_df = sample_df[~sample_df.full_text_processed.str.contains(\"fröhlicher gruss\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stop_words import get_stop_words\n",
    "#stop_words = get_stop_words('de')\n",
    "\n",
    "import stopwordsiso as stopwords\n",
    "stop_words = list(stopwords.stopwords([\"de\"])) \n",
    "\n",
    "mehr_sw = ['der', 'die', 'das', 'ist' 'es', 'gibt', 'und', 'für', 'auf', 'aus', 'mit', 'dem', 'tb']\n",
    "for word in mehr_sw:\n",
    "    stop_words.append(word)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning \n",
    "Diskretisierung von date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sortieren nach Datum/Zeit\n",
    "sample_df = sample_df.sort_values(by = ['created_at_datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verschiedene Binning Ansätze - aus verschiedenen Gründen nicht verwendet\n",
    "* Bins mit vorgegebenen Breiten (Y, M, ...) -> Labels können nicht mitgenommen werden\n",
    "* Binning mit value_count -> Direkte Ausgabe der Anzahl pro Bin\n",
    "* Binning mit groupby -> Ausgabe der Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut_bins = pd.interval_range(start=pd.Timestamp('2020-02-01'), periods=9, freq='M')\n",
    "# cut_labels = [i for i in range(2009,2021)]\n",
    "# pd.cut(text_nRT['created_at_datetime'], bins=cut_bins, labels=cut_labels)\n",
    "\n",
    "\n",
    "# text_nRT['created_at_datetime'].value_counts(bins=9, sort=False)\n",
    "\n",
    "# bins = text_nRT.groupby(pd.cut(text_nRT['created_at_datetime'], bins=9, labels=list(range(1,10))))\n",
    "# for b in bins.indices:\n",
    "#     print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unterteilung in Bins und erstellen eines Dictionarys für Zeiträume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'Bis': Timestamp('2020-02-09 03:54:03'),\n",
       "  'Von': Timestamp('2020-02-01 00:05:57')},\n",
       " 2: {'Bis': Timestamp('2020-02-17 08:02:21'),\n",
       "  'Von': Timestamp('2020-02-09 05:19:13')},\n",
       " 3: {'Bis': Timestamp('2020-02-25 12:01:24'),\n",
       "  'Von': Timestamp('2020-02-17 08:05:34')},\n",
       " 4: {'Bis': Timestamp('2020-03-04 15:59:52'),\n",
       "  'Von': Timestamp('2020-02-25 12:04:34')},\n",
       " 5: {'Bis': Timestamp('2020-03-12 19:57:35'),\n",
       "  'Von': Timestamp('2020-03-04 16:02:03')},\n",
       " 6: {'Bis': Timestamp('2020-03-20 23:26:27'),\n",
       "  'Von': Timestamp('2020-03-12 19:59:51')},\n",
       " 7: {'Bis': Timestamp('2020-03-28 23:51:16'),\n",
       "  'Von': Timestamp('2020-03-21 01:11:25')},\n",
       " 8: {'Bis': Timestamp('2020-04-06 07:52:32'),\n",
       "  'Von': Timestamp('2020-03-29 04:01:42')},\n",
       " 9: {'Bis': Timestamp('2020-04-14 11:54:05'),\n",
       "  'Von': Timestamp('2020-04-06 07:59:22')}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['bins'] = pd.cut(sample_df['created_at_datetime'], bins=9, labels=list(range(1,10)))\n",
    "\n",
    "labels=list(range(1,10))\n",
    "weeks = {}\n",
    "    \n",
    "for i in labels:\n",
    "    weeks['week_'+str(i)] = sample_df[sample_df['bins'] == i]    \n",
    "# Unsaubere Variante - jeweils Variable erstellen:\n",
    "# globals()['week_'+str(i)] = text_nRT[text_nRT['bins'] == i]\n",
    "\n",
    "bin_sum_per_week = {}\n",
    "\n",
    "for label in range(1,10): \n",
    "    df = sample_df[sample_df['bins'] == label]\n",
    "    head = list(df.created_at_datetime.head(1))[0]\n",
    "    tail = list(df.created_at_datetime.tail(1))[0]\n",
    "    #print(head.to_datetime)\n",
    "    #print(type(head))\n",
    "    bin_sum_per_week[label] = {}\n",
    "    bin_sum_per_week[label]['Von'] = head\n",
    "    bin_sum_per_week[label]['Bis'] = tail\n",
    "bin_sum_per_week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weitere Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in labels:\n",
    "    weeks['week_'+str(i)] = sample_df[sample_df['bins'] == i] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(sample_df['full_text_processed'])\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', stopwords=stop_words)\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()\n",
    "\n",
    "#fig, axs = plt.subplots(1,2)\n",
    "\n",
    "#df['korisnika'].plot(ax=axs[0])\n",
    "#df['osiguranika'].plot(ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud Gegenüberstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "for i in labels:\n",
    "    weeks['week_'+str(i)] = sample_df[sample_df['bins'] == i]  \n",
    "\n",
    "def FktWordCloud(long_string):\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', stopwords=stop_words)\n",
    "    wordcloud.generate(long_string)\n",
    "    wordcloud.to_image()\n",
    "    return wordcloud\n",
    "\n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "for i, n in zip(weeks, range(1,10)):\n",
    "    title = \"%s - %s\" % (bin_sum_per_week[n]['Von'].strftime('%d.%m.'),\n",
    "                         bin_sum_per_week[n]['Bis'].strftime('%d.%m.%Y'))\n",
    "    ax = fig.add_subplot(3,3,n, title = title)\n",
    "    sample_df=weeks[i]\n",
    "    long_string = ','.join(sample_df['full_text_processed'])\n",
    "    wordcloud = FktWordCloud(long_string)\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the library with the CountVectorizer method\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# sns.set_style('whitegrid')\n",
    "# %matplotlib inline\n",
    "\n",
    "# # Helper function\n",
    "# def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     words = count_vectorizer.get_feature_names()\n",
    "#     total_counts = np.zeros(len(words))\n",
    "#     for t in count_data:\n",
    "#         total_counts+=t.toarray()[0]\n",
    "    \n",
    "#     count_dict = (zip(words, total_counts))\n",
    "#     count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "#     words = [w[0] for w in count_dict]\n",
    "#     counts = [w[1] for w in count_dict]\n",
    "#     x_pos = np.arange(len(words)) \n",
    "    \n",
    "#     plt.figure(2, figsize=(15, 15/1.6180))\n",
    "#     plt.subplot(title='10 most common words')\n",
    "#     sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "#     sns.barplot(x_pos, counts, palette='husl')\n",
    "#     plt.xticks(x_pos, words, rotation=90) \n",
    "#     plt.xlabel('words')\n",
    "#     plt.ylabel('counts')\n",
    "#     plt.show()\n",
    "    \n",
    "# # Initialise the count vectorizer with the german stop words\n",
    "# count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# # Fit and transform the processed titles\n",
    "# count_data = count_vectorizer.fit_transform(weeks['week_9']['full_text_processed'])\n",
    "# # Visualise the 10 most common words\n",
    "# plot_10_most_common_words(count_data, count_vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot Gegenüberstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the library with the CountVectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0] # total_counts = total_counts + t.toarray\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words))\n",
    "    \n",
    "    with plt.xkcd():\n",
    "        ax = fig.add_subplot(3,3,n, title =  title)\n",
    "        fig.tight_layout()\n",
    "        ax = sns.barplot(x_pos, counts, palette=\"GnBu_d\")\n",
    "        ax.set_xticklabels(words, rotation = 45, fontsize=10)\n",
    "        return ax\n",
    "\n",
    "    \n",
    "# Initialise the count vectorizer with the german stop words\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "for i, n in zip(weeks, range(1,10)):\n",
    "    title = \"%s - %s\" % (bin_sum_per_week[n]['Von'].strftime('%d.%m.'),\n",
    "                         bin_sum_per_week[n]['Bis'].strftime('%d.%m.%Y'))\n",
    "    \n",
    "    # Fit and transform the processed titles\n",
    "    count_data = count_vectorizer.fit_transform(weeks[i]['full_text_processed'])\n",
    "    plot_10_most_common_words(count_data, count_vectorizer, title)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 5\n",
    "number_words = 10\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import os\n",
    "import pyLDAvis\n",
    "LDAvis_data_filepath = os.path.join('/home/lisa/Darmstadt/Master Arbeit/06_Analyse/ldavis_prepared_'+str(number_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import collections\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize \n",
    "from nltk.util import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "bigram=[]\n",
    "   \n",
    "\n",
    "def make_bigrams_network (df):\n",
    "    for tweet in df:\n",
    "        word_data = tweet\n",
    "        tokens = nltk.word_tokenize(word_data)                            # -> Einzelne Wörter (Tokens)\n",
    "        tweets_nsw = [word for word in tokens if not word in stop_words]  # -> Ohne StopWords\n",
    "        terms_bigram = list(nltk.bigrams(tweets_nsw))                     # -> Zweierpärchen (Bigrams) \n",
    "        bigram.append(terms_bigram)\n",
    "    \n",
    "    terms_bigram = bigram\n",
    "    #return terms_bigram\n",
    "    # Flatten list of bigrams in clean tweets\n",
    "    bigrams = list(itertools.chain(*terms_bigram))\n",
    "    # Create counter of words in clean bigrams\n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    bigram_counts.most_common(20)\n",
    "    bigram_df = pd.DataFrame(bigram_counts.most_common(20), columns=['bigram', 'count'])\n",
    "\n",
    "    # Create dictionary of bigrams and their counts\n",
    "    d = bigram_df.set_index('bigram').T.to_dict('records')\n",
    "    \n",
    "    # Create network plot \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Create connections between nodes\n",
    "    for k, v in d[0].items():\n",
    "        G.add_edge(k[0], k[1], weight=(v * 10))\n",
    "        \n",
    "    ax = fig.add_subplot(5,2,n, title =  title)\n",
    "    fig.tight_layout()\n",
    "    pos = nx.spring_layout(G, k=1)\n",
    "    nx.draw_networkx(G, pos,\n",
    "                 font_size=16, width=3, edge_color='grey', node_color='darkblue', with_labels = False, ax=ax)\n",
    "    \n",
    "    for key, value in pos.items():\n",
    "        x, y = value[0]+.135, value[1]+.045\n",
    "        ax.text(x, y, s=key, bbox=dict(alpha=0.25), horizontalalignment='center', fontsize=10)\n",
    "    \n",
    "        \n",
    "fig = plt.figure(figsize = (15, 25))\n",
    "\n",
    "for i, n in zip(weeks, range(1,10)):\n",
    "    title = \"%s - %s\" % (bin_sum_per_week[n]['Von'].strftime('%d.%m.'),\n",
    "                         bin_sum_per_week[n]['Bis'].strftime('%d.%m.%Y'))\n",
    "    \n",
    "    make_bigrams_network(weeks[i]['full_text_processed'])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyse - Wörterbuch\n",
    "## Simple TextBlob Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob_de import TextBlobDE as TextBlob\n",
    "\n",
    "blob = TextBlob(sample_df['full_text_processed'][8]) \n",
    "\n",
    "# print(blob.sentences)\n",
    "# print(blob.tokens)\n",
    "# print(blob.tags)\n",
    "# print(blob.noun_phrases)\n",
    "# print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@himanshu_23732/sentiment-analysis-with-textblob-6bc2eb9ec4ab\n",
    "def sentiment(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "sample_df['Polarity']     = sample_df['full_text_processed'].apply(sentiment).apply(lambda x: x[0])\n",
    "sample_df['Subjectivity'] = sample_df['full_text_processed'].apply(sentiment).apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sentiment scores (polarity) and labels\n",
    "sentiment_scores = sample_df['Polarity']\n",
    "sentiment_category = ['positive' if score > 0 \n",
    "                             else 'negative' if score < 0 \n",
    "                                 else 'neutral' \n",
    "                                     for score in sentiment_scores]\n",
    "\n",
    "\n",
    "# sentiment statistics per news category\n",
    "df = pd.DataFrame([list(sample_df['bins']), list(sentiment_scores), list(sentiment_category)]).T\n",
    "df.columns = ['category', 'sentiment_score', 'sentiment_category']\n",
    "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
    "df.groupby(by=['category']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i, n in zip(weeks, range(1,10)):\n",
    "    label = \"%s - %s\" % (bin_sum_per_week[n]['Von'].strftime('%d.%m.'),\n",
    "                         bin_sum_per_week[n]['Bis'].strftime('%d.%m.%Y'))\n",
    "    labels.append(label)\n",
    "\n",
    "ax = sns.catplot(x=\"category\", hue=\"sentiment_category\", height=13,\n",
    "                    data=df, kind=\"count\", \n",
    "                    palette={\"negative\": \"#FE2020\", \n",
    "                             \"positive\": \"#BADD07\", \n",
    "                             \"neutral\": \"#68BFF5\"})\n",
    "ax.set_xticklabels(labels, rotation = 45, fontsize=12)\n",
    "\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ekel</th>\n",
       "      <th>Freude</th>\n",
       "      <th>Furcht</th>\n",
       "      <th>Trauer</th>\n",
       "      <th>Ueberraschung</th>\n",
       "      <th>Verachtung</th>\n",
       "      <th>Wut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aalglatt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aasgeier</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrechen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbruchreif</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbügeln</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Ekel Freude Furcht Trauer Ueberraschung Verachtung Wut\n",
       "aalglatt     0    0      0      0      0             1          0 \n",
       "aasgeier     0    0      0      0      0             1          0 \n",
       "abbrechen    0    0      0      1      0             0          0 \n",
       "abbruchreif  0    0      0      0      0             1          0 \n",
       "abbügeln     0    0      0      0      0             1          0 "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "    \n",
    "######### Erstelle Emotions-Matrix\n",
    "\n",
    "filepath = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Lexicon_based/german-emotion-dictionary/fundamental/'\n",
    "emo_df = []\n",
    "words = []\n",
    "emotion = ['Ekel', 'Freude', 'Furcht', 'Trauer', 'Ueberraschung', 'Verachtung', 'Wut']\n",
    "emotion_list= {}\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "\n",
    "for emo in emotion:\n",
    "    with open(filepath + emo + '.txt', newline='\\n') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = [item for sublist in reader for item in sublist] \n",
    "        emotion_list[str(emo)] = data\n",
    "        words.extend(data)\n",
    "        \n",
    "words = list(set(words)) # remove Duplicates which come from overlapping EmotionLists\n",
    "emo_df  = pd.DataFrame(index=words, columns=emotion)\n",
    "\n",
    "words = []\n",
    "for word in emo_df.index:\n",
    "    for emo in emo_df.columns:\n",
    "        if word in emotion_list[emo]:\n",
    "            emo_df.at[word, emo] = 1\n",
    "        else:\n",
    "            emo_df.at[word, emo] = 0\n",
    "    words.append(word.lower())\n",
    " \n",
    "emo_df.index= words\n",
    "emo_df = emo_df.sort_index(ascending=True)\n",
    "emo_df = pd.DataFrame(emo_df)\n",
    "\n",
    "emo_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_debug = False\n",
    "# def debug(string):\n",
    "#     if print_debug == True:\n",
    "#         print(string)\n",
    "\n",
    "# for word in emo_df.index:\n",
    "#      debug(\"'%s'\" % (word))\n",
    "#      debug(type(word))\n",
    "#      #for emo in emotion_list:\n",
    "#      for emo in emo_df.columns:\n",
    "#          #print(emo)\n",
    "#          #print(type(emo))\n",
    "#          if word in emotion_list[emo]:\n",
    "#              debug(\"Word '%s' is of emotion: '%s'\" % (word, emo))\n",
    "#              #print(emo_df[emo][word])\n",
    "#              #print(emo_df.columns)\n",
    "#              #print(emo_df.ix[1])\n",
    "#              emo_df.at[word, emo] = 1\n",
    "#              debug(emo_df.loc[word])\n",
    "#              debug(\"\\n\")\n",
    "\n",
    "#          #else:\n",
    "#          #    emo_df.at[str(emo)][word] = 0\n",
    "\n",
    "# emo_df[1:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text_processed</th>\n",
       "      <th>Ekel</th>\n",
       "      <th>Freude</th>\n",
       "      <th>Furcht</th>\n",
       "      <th>Trauer</th>\n",
       "      <th>Ueberraschung</th>\n",
       "      <th>Verachtung</th>\n",
       "      <th>Wut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>raus aber nicht weg zum brexitday ein konvolut der faznet in dem sich ein klassiker versteckt die beschreibung des gurkensandwich von jakob serra y strobel ist so wunderbar und auch cricket spießt liebevoll den britischen lifestyle auf brexit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22795</th>\n",
       "      <td>kleine rückfrage an trolle im netz die fakes über mein privatleben verbreiten und behaupten ich würde meine ehefrau betrügen nur damit ich alles richtig mache welche ehefrau  ansonsten danke für die ganze kohle die ich mit den strafanzeigen verdiene ich geh jetzt feiern</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17543</th>\n",
       "      <td>sehenswert ruhige info zu corona zu gast prof dr christian drosten - leiter des instituts für virologie an der berliner charité - talk aus berlin | rbb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>guten morgen deutschland die briten england hat es geschafft glückwunsch zum brexit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>so kommentieren die gez-komiker des zdf den brexit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                    full_text_processed  \\\n",
       "5923   raus aber nicht weg zum brexitday ein konvolut der faznet in dem sich ein klassiker versteckt die beschreibung des gurkensandwich von jakob serra y strobel ist so wunderbar und auch cricket spießt liebevoll den britischen lifestyle auf brexit                                 \n",
       "22795  kleine rückfrage an trolle im netz die fakes über mein privatleben verbreiten und behaupten ich würde meine ehefrau betrügen nur damit ich alles richtig mache welche ehefrau  ansonsten danke für die ganze kohle die ich mit den strafanzeigen verdiene ich geh jetzt feiern     \n",
       "17543  sehenswert ruhige info zu corona zu gast prof dr christian drosten - leiter des instituts für virologie an der berliner charité - talk aus berlin | rbb                                                                                                                            \n",
       "1339   guten morgen deutschland die briten england hat es geschafft glückwunsch zum brexit                                                                                                                                                                                                \n",
       "2405   so kommentieren die gez-komiker des zdf den brexit                                                                                                                                                                                                                                 \n",
       "\n",
       "       Ekel  Freude  Furcht  Trauer  Ueberraschung  Verachtung  Wut  \n",
       "5923   0     1       0       0       0              0           0    \n",
       "22795  0     1       0       0       0              0           0    \n",
       "17543  0     0       0       0       0              0           0    \n",
       "1339   0     1       0       0       0              0           0    \n",
       "2405   0     0       0       0       0              0           0    "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = sample_df['full_text_processed']\n",
    "#GS: print(new_df.index)\n",
    "df_emo = pd.DataFrame(0, index=new_df.index, columns=emotion)\n",
    "\n",
    "for i in new_df.index:\n",
    "    document = word_tokenize(new_df.loc[i])\n",
    "    for word in document:\n",
    "        word = stemmer.stem(word.lower())             # Stemming des einzelnen Wortes aus Tweet um es...\n",
    "        #print(word)\n",
    "        emo_score = emo_df[emo_df.index == word]      # ... mit den Lexikon Worten zu vergleichen -> emo_score \n",
    "        #print(type(emo_score))\n",
    "        if not emo_score.empty:                       # der emo score eines Worts z.B. [0 0 1 1 0 0 0] \n",
    "            #print(emo_score)\n",
    "            for emot in emotion:                      # wird zum Score des Tweets dazugezählt z.B.: [2 0 4 1 0 0 0]\n",
    "                #print (\"trying word '%s' and emo '%s'\" % (word, emot))\n",
    "                #print(type(df_emo.at[word, emot]))   # GS failed, weil der Index in df_emo eine Zahl ist und kein Wort, siehe Zeile 2\n",
    "                #print(type(emo_score[emot]))\n",
    "                #print(emo_score[emot])\n",
    "                df_emo.at[i, emot] += emo_score[emot] # am Ende exisitiert zu einem Tweet \n",
    "                \n",
    "\n",
    "new_df = pd.concat([new_df, df_emo], axis=1)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text_processed</th>\n",
       "      <th>Ekel</th>\n",
       "      <th>Freude</th>\n",
       "      <th>Furcht</th>\n",
       "      <th>Trauer</th>\n",
       "      <th>Ueberraschung</th>\n",
       "      <th>Verachtung</th>\n",
       "      <th>Wut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>raus aber nicht weg zum brexitday ein konvolut der faznet in dem sich ein klassiker versteckt die beschreibung des gurkensandwich von jakob serra y strobel ist so wunderbar und auch cricket spießt liebevoll den britischen lifestyle auf brexit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22795</th>\n",
       "      <td>kleine rückfrage an trolle im netz die fakes über mein privatleben verbreiten und behaupten ich würde meine ehefrau betrügen nur damit ich alles richtig mache welche ehefrau  ansonsten danke für die ganze kohle die ich mit den strafanzeigen verdiene ich geh jetzt feiern</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17543</th>\n",
       "      <td>sehenswert ruhige info zu corona zu gast prof dr christian drosten - leiter des instituts für virologie an der berliner charité - talk aus berlin | rbb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>guten morgen deutschland die briten england hat es geschafft glückwunsch zum brexit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>so kommentieren die gez-komiker des zdf den brexit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>die spätfolgen des kommunismus in einem bild</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403</th>\n",
       "      <td>man braucht in deutschland keine islamische partei gründen diesen job übernehmen die grünen die farbe passt auch vollverschleierung</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26196</th>\n",
       "      <td>st_klaene linksfraktion für die ersten fragen bin ich zu müde obzwar ich dezidiert antwort geben könnte die letzte ist hingegen höchst interessant und nachdenkenswert</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38246</th>\n",
       "      <td>buongiorno aus franken  allen ein entspanntes wochenende</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11141</th>\n",
       "      <td>watchu28020892 wer ist ihr informant und was wollen sie kontrollieren</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15843</th>\n",
       "      <td>now that they are gone - where did they go brexit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11140</th>\n",
       "      <td>babsi0909 watchu28020892 wenn nur einmal getestet wird und nicht wiederholt na klar gibt es falsch negative testungen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11139</th>\n",
       "      <td>getrichguru watchu28020892 definieren sie bitte außer kontrolle das ist nur panikmache was sie da treiben wie stehen sie zur grippe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>es ist wieder soweit afd aschermittwoch niederbayern</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>klauskontra walter29340877 die dame hat sich passend aus dem bundestag ins europaparlament abgeseiltum nochmal so richtig kasse zu machen ist halt eine typischespezialdemokratinbrexitday eu brüssel barleyspd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11138</th>\n",
       "      <td>watchu28020892 nein sie behaupten ja die regierung unterschätzt die gefahr sie kennen offensichtlich die daten die die chinesische regierung aus welchen gründen auch immer nicht teil es fehlen daten über die todesursachen die brauchen wir um die situation besser einzuschätzen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11137</th>\n",
       "      <td>watchu28020892 schar_mutzel nein so habe ich mich nicht geäußert es existieren mehrere arten der quarantäne und ich habe mich für eine risikoadaptierte quarantäne ausgesprochen mit regelmässigen testungen gerade bei asymptomatischen personen mindestens bis wir mehr daten über die todesursachen haben</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>philipplickert welt sie dürfen vom erfolg des brexitausgehen suchen sie sich schonmal einen neuen job herr webercdu csu weber brexit eueuropa greendeal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>ahmad_omeirate die spd hat fertig</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5922</th>\n",
       "      <td>rheinreporter schon aber es heißt jetzt little england 󠁧󠁢󠁥󠁮󠁧󠁿</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>geraldgrosz aber soll heute nacht in england so dunkel gewesen sein</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11136</th>\n",
       "      <td>bislang war ich überzeugt dass parteiübergreifend im us-senat man sich bei prozessabläufen einig ist traurig wenn aus parteitaktischen überlegungen offensichtlich zeugenbefragungen nicht stattfinden mein glauben an check  balances in  ist erschüttert</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45655</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11135</th>\n",
       "      <td>watchu28020892 babsi0909 welche fehlerrate meinen sie will nicht dozieren aber hier spielen parameter wie negative oder positive vorhersagewertigkeit sensitivität etc eine rolle wiederholte testungen verbessern diese parameter dann kommt man schon in die nähe von 100% bzw 0% wir brauchen mehr daten</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11134</th>\n",
       "      <td>watchu28020892 bislang deutet einiges darauf hin dass das virus ähnlich ansteckend ist wie die grippe jedoch sind husten schnupfen fieber  heiserkeit nichts schlimmes aber der tod schon  wir wissen da zu wenig deshalb gelten erstmal die gleichen schutzregeln wie bei grippe dontpanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27754</th>\n",
       "      <td>im bundestag so entscheidungen ü leben  tod nicht maschinen überlassen killerroboter verbieten moratorium jetzt dfgvk_bv bankillerrobots netzwerkfrieden ippnwgermany amnesty_de</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44232</th>\n",
       "      <td>danke für die klasse arbeit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11133</th>\n",
       "      <td>watchu28020892 schar_mutzel noch einmal im klartext quarantäne ja aber halt nicht zentral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>das war eine niederlage mit ansageund das ist gut soim impeachment kassieren die demokraten eine bittere niederlage</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30993</th>\n",
       "      <td>abwechslungsreicher tag erst infostand in eschbach  dann mit dem team_luftwaffe nach germersheim und dann festbankett 100 jahre olympia rheinzabern  in der südpfalz hast du als mdb nie langeweile</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13714</th>\n",
       "      <td>in meinem neuen newsletter gruppe grüne eisenbahner gegründet bahn für geschäftsreisende nachtzüge stuttgart21 gäubahn alternative kraftstoffe kraftstoffstrategie etretroller beim tüv bonus für fahrrad-nutzung</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12922</th>\n",
       "      <td>sich einmischen für ein lebendiges gemeinwesen und starke grüne vor ort am ende der sitzungswoche ein abstecher zu grünen nach hitzackerklimaschutzgute bahnverbindung finanzausstattung der kommunen naturschutz gleichwertige lebensverhältnisse - es gibt viel zu tun für uns</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6951</th>\n",
       "      <td>2341 jahre scheuer die rheinpfalz mag es nicht wenn ich rechne - bei andischeuer rechne ich weiter es geht schließlich um das geld der steuerzahler</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12921</th>\n",
       "      <td>es ist leider so weit  grossbritannien tritt aus der europäischen union aus wir hätten uns das wirklich anders gewünscht brexit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30206</th>\n",
       "      <td>russian government suspends rusaf recognition after aiu report but government funding for competitions in russia and training cs will continue  4  weeks suspension but  funding goes on you‘re kidding us again and again right nodoping</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>das wort philantroph im zusammenhang mit george soros zu gebrauchen ist schon mehr als abenteuerlich spalter und übler spekulant wären treffendersoroslies ngo brexit eu</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27989</th>\n",
       "      <td>guten flug und vielen dank an team_luftwaffe und an unsere kolleginnen  kollegen vom aa_sicherreisen für die hervorgegangene zusammenarbeit auswaertigesamt bundeswehrinfo coronarovirus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553</th>\n",
       "      <td>heute ist der changeyourpasswordday - einfach zu merkende aber hochkomplexe passworte sind grundlage für die eigene cybersicherheit  deutscher bundestag</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45206</th>\n",
       "      <td>ich verurteile den feigen anschlag auf meinen freund falkone1 und seine frau zutiefst gewalt darf niemals ein mittel der politischen auseinandersetzung sein die cduberlin lässt sich nicht einschüchtern wir werden weiter konsequent für demokratie und rechtsstaat einstehen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14308</th>\n",
       "      <td>bei der geschwindigkeit brauchen wir 32 jahre bis 70% der bahn -strecken in bayern elektrifiziert sind mein kollege der_buechler und ich dazu heute az_augsburg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43388</th>\n",
       "      <td>gleich kommen 60 schüler und lehrer des amg viersen in den bundestag - gespräch über politische arbeit - wahlkreis erdet heimat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43441</th>\n",
       "      <td>wahlrecht der praktikabelste weg ist das echte zwei-stimmenwahlrecht darüber wird wenig berichtet die diskutierten vorschläge beschäftigen sich nur mit der verkleinerung des bundestages durch eine reduzierung der wahlkreise</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38608</th>\n",
       "      <td>über den vier türmen des deutschen bundestags wehen vier fahnen drei deutsche und die europäische - genau wie immer alles gute britannia brexit uk</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27806</th>\n",
       "      <td>großbritannien ist aus der eu ausgetreten wer trotz brexit meint in der eu weiter militarisierung  das neoliberale projekt vorantreiben zu wollen wird die fliehkräfte verstärken wir brauchen einen neustart der eu  ein soziales  friedliches europa</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42229</th>\n",
       "      <td>anschlag auf den wagen der frau von falkolieke⁩ der sich als stadtrat in neukölln wie kein zweiter gegen linksextreme u arabische clans einsetzt nicht einschüchtern lassen-wir stehen zusammen⁦falkone1⁩ ⁦cduberlin⁩ ⁦tspcheckpoint</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45071</th>\n",
       "      <td>mitgliederversammlung cdu-stadtverband zwickau bürgermeisterin kathrin köhler übergibt den vorsitz der stadtverband bereitet sich für die oberbürgermeisterwahl vor</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28495</th>\n",
       "      <td>bei der fachkräftesicherung auf europa zu setzen wie es bamf_dialog - chef sommer vorschlägt ist natürlich einfacher raubt aber anderen arbeitskräfte die sie selbst brauchen ganz europa ist  altersschwach wir brauchen einwanderung von außen und keinen braindrain</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22794</th>\n",
       "      <td>spass_gorilla trolle sind kein arbeitendes volk sondern trolle geld lässt sich nur verdienen wenn offensichtlich unwahrheiten verbreitet werden beim bka gibt es eine sicherungsgruppe die für den bundestag zuständig ist und sich gerne mit jedem account befasst auch mit ihrem</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41946</th>\n",
       "      <td>kein zurück zu den antipersonenminen eu ist größter geber für minenräumen deutschland ist größter eu-beitragszahler an der finanzierung der humanitären hilfsmaßnahmen der eukom bei humanitären minen- und kfmittelräumung</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2402</th>\n",
       "      <td>bürgerliche parteien verteidigen hindenburg gegen die vereinigte linkeberlin streicht ex-reichspräsident hindenburg von ehrenbürgerliste  – kritik von  -</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                full_text_processed  \\\n",
       "5923   raus aber nicht weg zum brexitday ein konvolut der faznet in dem sich ein klassiker versteckt die beschreibung des gurkensandwich von jakob serra y strobel ist so wunderbar und auch cricket spießt liebevoll den britischen lifestyle auf brexit                                                             \n",
       "22795  kleine rückfrage an trolle im netz die fakes über mein privatleben verbreiten und behaupten ich würde meine ehefrau betrügen nur damit ich alles richtig mache welche ehefrau  ansonsten danke für die ganze kohle die ich mit den strafanzeigen verdiene ich geh jetzt feiern                                 \n",
       "17543  sehenswert ruhige info zu corona zu gast prof dr christian drosten - leiter des instituts für virologie an der berliner charité - talk aus berlin | rbb                                                                                                                                                        \n",
       "1339   guten morgen deutschland die briten england hat es geschafft glückwunsch zum brexit                                                                                                                                                                                                                            \n",
       "2405   so kommentieren die gez-komiker des zdf den brexit                                                                                                                                                                                                                                                             \n",
       "2404   die spätfolgen des kommunismus in einem bild                                                                                                                                                                                                                                                                   \n",
       "2403   man braucht in deutschland keine islamische partei gründen diesen job übernehmen die grünen die farbe passt auch vollverschleierung                                                                                                                                                                            \n",
       "26196  st_klaene linksfraktion für die ersten fragen bin ich zu müde obzwar ich dezidiert antwort geben könnte die letzte ist hingegen höchst interessant und nachdenkenswert                                                                                                                                         \n",
       "38246  buongiorno aus franken  allen ein entspanntes wochenende                                                                                                                                                                                                                                                       \n",
       "11141  watchu28020892 wer ist ihr informant und was wollen sie kontrollieren                                                                                                                                                                                                                                          \n",
       "15843  now that they are gone - where did they go brexit                                                                                                                                                                                                                                                              \n",
       "11140  babsi0909 watchu28020892 wenn nur einmal getestet wird und nicht wiederholt na klar gibt es falsch negative testungen                                                                                                                                                                                          \n",
       "11139  getrichguru watchu28020892 definieren sie bitte außer kontrolle das ist nur panikmache was sie da treiben wie stehen sie zur grippe                                                                                                                                                                            \n",
       "1338   es ist wieder soweit afd aschermittwoch niederbayern                                                                                                                                                                                                                                                           \n",
       "1941   klauskontra walter29340877 die dame hat sich passend aus dem bundestag ins europaparlament abgeseiltum nochmal so richtig kasse zu machen ist halt eine typischespezialdemokratinbrexitday eu brüssel barleyspd                                                                                                \n",
       "11138  watchu28020892 nein sie behaupten ja die regierung unterschätzt die gefahr sie kennen offensichtlich die daten die die chinesische regierung aus welchen gründen auch immer nicht teil es fehlen daten über die todesursachen die brauchen wir um die situation besser einzuschätzen                           \n",
       "11137  watchu28020892 schar_mutzel nein so habe ich mich nicht geäußert es existieren mehrere arten der quarantäne und ich habe mich für eine risikoadaptierte quarantäne ausgesprochen mit regelmässigen testungen gerade bei asymptomatischen personen mindestens bis wir mehr daten über die todesursachen haben   \n",
       "1940   philipplickert welt sie dürfen vom erfolg des brexitausgehen suchen sie sich schonmal einen neuen job herr webercdu csu weber brexit eueuropa greendeal                                                                                                                                                        \n",
       "1939   ahmad_omeirate die spd hat fertig                                                                                                                                                                                                                                                                              \n",
       "5922   rheinreporter schon aber es heißt jetzt little england 󠁧󠁢󠁥󠁮󠁧󠁿                                                                                                                                                                                                                                                  \n",
       "1938   geraldgrosz aber soll heute nacht in england so dunkel gewesen sein                                                                                                                                                                                                                                            \n",
       "11136  bislang war ich überzeugt dass parteiübergreifend im us-senat man sich bei prozessabläufen einig ist traurig wenn aus parteitaktischen überlegungen offensichtlich zeugenbefragungen nicht stattfinden mein glauben an check  balances in  ist erschüttert                                                     \n",
       "45655                                                                                                                                                                                                                                                                                                                 \n",
       "11135  watchu28020892 babsi0909 welche fehlerrate meinen sie will nicht dozieren aber hier spielen parameter wie negative oder positive vorhersagewertigkeit sensitivität etc eine rolle wiederholte testungen verbessern diese parameter dann kommt man schon in die nähe von 100% bzw 0% wir brauchen mehr daten    \n",
       "11134  watchu28020892 bislang deutet einiges darauf hin dass das virus ähnlich ansteckend ist wie die grippe jedoch sind husten schnupfen fieber  heiserkeit nichts schlimmes aber der tod schon  wir wissen da zu wenig deshalb gelten erstmal die gleichen schutzregeln wie bei grippe dontpanic                    \n",
       "27754  im bundestag so entscheidungen ü leben  tod nicht maschinen überlassen killerroboter verbieten moratorium jetzt dfgvk_bv bankillerrobots netzwerkfrieden ippnwgermany amnesty_de                                                                                                                               \n",
       "44232  danke für die klasse arbeit                                                                                                                                                                                                                                                                                    \n",
       "11133  watchu28020892 schar_mutzel noch einmal im klartext quarantäne ja aber halt nicht zentral                                                                                                                                                                                                                      \n",
       "1937   das war eine niederlage mit ansageund das ist gut soim impeachment kassieren die demokraten eine bittere niederlage                                                                                                                                                                                            \n",
       "30993  abwechslungsreicher tag erst infostand in eschbach  dann mit dem team_luftwaffe nach germersheim und dann festbankett 100 jahre olympia rheinzabern  in der südpfalz hast du als mdb nie langeweile                                                                                                            \n",
       "13714  in meinem neuen newsletter gruppe grüne eisenbahner gegründet bahn für geschäftsreisende nachtzüge stuttgart21 gäubahn alternative kraftstoffe kraftstoffstrategie etretroller beim tüv bonus für fahrrad-nutzung                                                                                              \n",
       "12922  sich einmischen für ein lebendiges gemeinwesen und starke grüne vor ort am ende der sitzungswoche ein abstecher zu grünen nach hitzackerklimaschutzgute bahnverbindung finanzausstattung der kommunen naturschutz gleichwertige lebensverhältnisse - es gibt viel zu tun für uns                               \n",
       "6951   2341 jahre scheuer die rheinpfalz mag es nicht wenn ich rechne - bei andischeuer rechne ich weiter es geht schließlich um das geld der steuerzahler                                                                                                                                                            \n",
       "12921  es ist leider so weit  grossbritannien tritt aus der europäischen union aus wir hätten uns das wirklich anders gewünscht brexit                                                                                                                                                                                \n",
       "30206  russian government suspends rusaf recognition after aiu report but government funding for competitions in russia and training cs will continue  4  weeks suspension but  funding goes on you‘re kidding us again and again right nodoping                                                                      \n",
       "1936   das wort philantroph im zusammenhang mit george soros zu gebrauchen ist schon mehr als abenteuerlich spalter und übler spekulant wären treffendersoroslies ngo brexit eu                                                                                                                                       \n",
       "27989  guten flug und vielen dank an team_luftwaffe und an unsere kolleginnen  kollegen vom aa_sicherreisen für die hervorgegangene zusammenarbeit auswaertigesamt bundeswehrinfo coronarovirus                                                                                                                       \n",
       "6553   heute ist der changeyourpasswordday - einfach zu merkende aber hochkomplexe passworte sind grundlage für die eigene cybersicherheit  deutscher bundestag                                                                                                                                                       \n",
       "45206  ich verurteile den feigen anschlag auf meinen freund falkone1 und seine frau zutiefst gewalt darf niemals ein mittel der politischen auseinandersetzung sein die cduberlin lässt sich nicht einschüchtern wir werden weiter konsequent für demokratie und rechtsstaat einstehen                                \n",
       "14308  bei der geschwindigkeit brauchen wir 32 jahre bis 70% der bahn -strecken in bayern elektrifiziert sind mein kollege der_buechler und ich dazu heute az_augsburg                                                                                                                                                \n",
       "43388  gleich kommen 60 schüler und lehrer des amg viersen in den bundestag - gespräch über politische arbeit - wahlkreis erdet heimat                                                                                                                                                                                \n",
       "43441  wahlrecht der praktikabelste weg ist das echte zwei-stimmenwahlrecht darüber wird wenig berichtet die diskutierten vorschläge beschäftigen sich nur mit der verkleinerung des bundestages durch eine reduzierung der wahlkreise                                                                                \n",
       "38608  über den vier türmen des deutschen bundestags wehen vier fahnen drei deutsche und die europäische - genau wie immer alles gute britannia brexit uk                                                                                                                                                             \n",
       "27806  großbritannien ist aus der eu ausgetreten wer trotz brexit meint in der eu weiter militarisierung  das neoliberale projekt vorantreiben zu wollen wird die fliehkräfte verstärken wir brauchen einen neustart der eu  ein soziales  friedliches europa                                                         \n",
       "42229  anschlag auf den wagen der frau von falkolieke⁩ der sich als stadtrat in neukölln wie kein zweiter gegen linksextreme u arabische clans einsetzt nicht einschüchtern lassen-wir stehen zusammen⁦falkone1⁩ ⁦cduberlin⁩ ⁦tspcheckpoint                                                                           \n",
       "45071  mitgliederversammlung cdu-stadtverband zwickau bürgermeisterin kathrin köhler übergibt den vorsitz der stadtverband bereitet sich für die oberbürgermeisterwahl vor                                                                                                                                            \n",
       "28495  bei der fachkräftesicherung auf europa zu setzen wie es bamf_dialog - chef sommer vorschlägt ist natürlich einfacher raubt aber anderen arbeitskräfte die sie selbst brauchen ganz europa ist  altersschwach wir brauchen einwanderung von außen und keinen braindrain                                         \n",
       "22794  spass_gorilla trolle sind kein arbeitendes volk sondern trolle geld lässt sich nur verdienen wenn offensichtlich unwahrheiten verbreitet werden beim bka gibt es eine sicherungsgruppe die für den bundestag zuständig ist und sich gerne mit jedem account befasst auch mit ihrem                             \n",
       "41946  kein zurück zu den antipersonenminen eu ist größter geber für minenräumen deutschland ist größter eu-beitragszahler an der finanzierung der humanitären hilfsmaßnahmen der eukom bei humanitären minen- und kfmittelräumung                                                                                    \n",
       "2402   bürgerliche parteien verteidigen hindenburg gegen die vereinigte linkeberlin streicht ex-reichspräsident hindenburg von ehrenbürgerliste  – kritik von  -                                                                                                                                                      \n",
       "\n",
       "       Ekel  Freude  Furcht  Trauer  Ueberraschung  Verachtung  Wut  \n",
       "5923   0     1       0       0       0              0           0    \n",
       "22795  0     1       0       0       0              0           0    \n",
       "17543  0     0       0       0       0              0           0    \n",
       "1339   0     1       0       0       0              0           0    \n",
       "2405   0     0       0       0       0              0           0    \n",
       "2404   0     0       0       0       0              0           0    \n",
       "2403   0     0       0       0       0              0           0    \n",
       "26196  0     0       0       0       0              0           0    \n",
       "38246  0     0       0       0       0              0           0    \n",
       "11141  0     0       0       0       0              0           0    \n",
       "15843  0     0       0       0       0              0           0    \n",
       "11140  0     0       0       0       0              1           0    \n",
       "11139  0     0       0       0       0              0           0    \n",
       "1338   0     0       0       0       0              0           0    \n",
       "1941   0     0       0       0       0              0           0    \n",
       "11138  0     0       0       0       0              0           0    \n",
       "11137  0     0       0       0       0              0           0    \n",
       "1940   0     0       0       0       1              0           0    \n",
       "1939   0     0       0       0       0              0           0    \n",
       "5922   0     0       0       0       0              0           0    \n",
       "1938   0     0       1       0       0              0           0    \n",
       "11136  0     0       0       1       0              0           0    \n",
       "45655  0     0       0       0       0              0           0    \n",
       "11135  0     0       0       0       0              0           0    \n",
       "11134  0     0       0       1       0              1           0    \n",
       "27754  0     0       0       0       0              0           0    \n",
       "44232  0     0       0       0       0              0           0    \n",
       "11133  0     0       0       0       0              0           0    \n",
       "1937   0     1       0       0       0              0           0    \n",
       "30993  0     0       0       0       0              0           0    \n",
       "13714  0     0       0       0       1              0           0    \n",
       "12922  0     0       0       0       0              0           0    \n",
       "6951   0     0       1       0       0              0           0    \n",
       "12921  0     0       0       0       0              0           0    \n",
       "30206  0     0       0       0       0              0           0    \n",
       "1936   0     0       0       0       0              0           0    \n",
       "27989  0     1       0       0       0              0           0    \n",
       "6553   0     0       0       0       0              1           0    \n",
       "45206  0     0       1       0       0              1           0    \n",
       "14308  0     0       0       0       1              0           0    \n",
       "43388  0     0       0       0       0              0           0    \n",
       "43441  0     0       0       0       0              0           0    \n",
       "38608  0     1       0       1       0              0           0    \n",
       "27806  0     1       0       0       0              0           0    \n",
       "42229  0     0       0       0       0              0           0    \n",
       "45071  0     0       0       0       0              0           0    \n",
       "28495  0     0       0       0       0              1           0    \n",
       "22794  0     0       0       0       0              1           0    \n",
       "41946  0     0       0       0       0              0           0    \n",
       "2402   0     0       0       0       0              0           0    "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "troll\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "# fuz = fuzz.ratio('trolle', 'troll')\n",
    "# print(fuz)\n",
    "\n",
    "#emo_df[emo_df.index == 'Trolle']\n",
    "for word in emo_df.index:\n",
    "    fuz = fuzz.ratio(str(word), 'trolle')\n",
    "    if fuz > 90:\n",
    "        print(word)\n",
    "        print(fuz)\n",
    "    \n",
    "\n",
    "# for word in emo_df.index:\n",
    "#     print(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b0c539adb666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text_processed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
     ]
    }
   ],
   "source": [
    "new_df['word_count'] = new_df['full_text_processed'].apply(tokenize.word_tokenize).apply(len)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample_df['full_text_processed'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
