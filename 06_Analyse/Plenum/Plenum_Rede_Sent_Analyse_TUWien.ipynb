{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse der Reden von Bundestagsabgeordneten\n",
    "## 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gensim, numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "PICKLE_FOLDER_PATH = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Plenum/pickle_TUWien_plenar/'\n",
    "df = pickle.load(open(\"/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Plenum/pickle_plenar/df_comment_all.pickle_date\", \"rb\" ))\n",
    "\n",
    "import re\n",
    "\n",
    "df['Fraktion'] = df['Fraktion'].replace(to_replace=r'(BÜNDNIS(SES)*\\s90\\/DIE\\sGRÜNEN)[$:,]*(?i)', value='BÜNDNIS_90/DIE_GRÜNEN', regex=True)\n",
    "df['Fraktion'] = df['Fraktion'].replace(to_replace=r'((DIE )*LINKE)[N$:,]*', value='DIE_LINKE', regex=True)\n",
    "df['Fraktion'] = df['Fraktion'].replace(to_replace=r'(Fraktionslos(?i))', value='Fraktionslos', regex=True)\n",
    "\n",
    "\n",
    "# Bereits analysierte Reden werden nicht erneut durchlaufen - lese Liste der pickle Files:\n",
    "import csv\n",
    "\n",
    "with open(PICKLE_FOLDER_PATH+'plenar_pickle.txt') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        processed_IDs = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bearbeite/ Vereinheitliche Rede Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/bin/ipython:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/anaconda3/bin/ipython:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "#time_sample_df = df.loc[(df['Datum'] >= datetime.datetime(2020,1,1,0,0,0)) &\n",
    "#                        (df['Datum'] <= datetime.datetime(2020,1,30,0,0,0))]\n",
    "\n",
    "\n",
    "#sample_df = time_sample_df\n",
    "sample_df = df[df['Rede_Text'].isna() != True]\n",
    "\n",
    "# Remove punctuation\n",
    "sample_df['full_text_processed'] = sample_df['Rede_Text'].map(lambda x: re.sub('§[\\n\\t,\\.!?#@\\\\n\"“„\\:;&\\(\\)]', '', x))\n",
    "\n",
    "# # Convert the titles to lowercase\n",
    "sample_df['full_text_processed'] = sample_df['full_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "sample_df = sample_df[sample_df['full_text_processed'] != '']\n",
    "sample_df = sample_df[sample_df['full_text_processed'] != ' ']\n",
    "sample_df = sample_df[sample_df['full_text_processed'] != '   ']\n",
    "\n",
    "# Print out the first rows of papers\n",
    "# sample_df['full_text_processed'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reden zusammenfügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rede_ID = set(df['Rede_ID'])\n",
    "df_Reden = sample_df[['Rede_ID', 'full_text_processed']]\n",
    "df_Reden = df_Reden.groupby('Rede_ID')['full_text_processed'].apply(' '.join).reset_index()\n",
    "df_Reden.index = df_Reden['Rede_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "satz = df_Reden[\"full_text_processed\"].str.split('.').apply(pd.Series,1).stack()\n",
    "satz.index = satz.index.droplevel(-1) # to line up with df's index\n",
    "satz.name = 'full_text_processed' # needs a name to join\n",
    "\n",
    "# There are blank or emplty cell values after above process. Removing them\n",
    "satz.replace('', np.nan, inplace=True)\n",
    "satz.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "Code von TUWien angepasst für die Anwendung auf eigene Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import pickle, gensim, numpy as np\n",
    "\n",
    "from utilities import get_train_data, get_test_data, Tokenizer, find_subtoken\n",
    "\n",
    "PICKLE_FOLDER_PATH = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Plenum/pickle_TUWien_plenar'\n",
    "\n",
    "TRAIN_FILENAME = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/germeval2018.training_all.txt'\n",
    "#mdp = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/mdp_tweets.txt'\n",
    "\n",
    "#------------------------------\n",
    "#source:\n",
    "#http://www.cl.uni-heidelberg.de/english/research/downloads/resource_pages/GermanTwitterEmbeddings/GermanTwitterEmbeddings_data.shtml\n",
    "MODEL_FILENAME  = \"/home/lisa/Darmstadt/Master Arbeit/06_Analyse/twitter-de_d100_w5_min10.bin\" # 821,8 MB\n",
    "MODEL_DIMENSION = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(satz.index)\n",
    "# satz[0:int((0.5*len(satz[satz.index == 'ID1916912700'])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:26:20: starting labeling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID1911500400\n",
      "['OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER'\n",
      " 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER'\n",
      " 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER'\n",
      " 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER'\n",
      " 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER'\n",
      " 'OFFENSE']\n",
      "08:36:55: end labeling\n",
      "08:36:55: starting labeling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID192611800\n",
      "['OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER' 'OTHER']\n",
      "09:47:07: end labeling\n",
      "09:47:07: starting labeling\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/62658215/convergencewarning-lbfgs-failed-to-converge-status-1-stop-total-no-of-iter\n",
    "from datetime import datetime\n",
    "\n",
    "for ID in set(satz.index):\n",
    "    if ID not in processed_IDs:\n",
    "        print(\"%s: starting labeling\" % (datetime.now().strftime(\"%H:%M:%S\")))\n",
    "        sample_mdp = np.array(satz[0:int(len(satz[satz.index == ID]))])\n",
    "        word2vec_model  = gensim.models.KeyedVectors.load_word2vec_format(MODEL_FILENAME, binary=True)\n",
    "\n",
    "        X_train, y_train_t1, y_train_t2 = get_train_data(TRAIN_FILENAME)\n",
    "        X_test_mdp                      = sample_mdp\n",
    "\n",
    "    ### NGRAM FEATURES\n",
    "    #  * Erstelle n-Gramme mit 3-7 Buchstaben (Funktion: char_vect)\n",
    "    #  * Erstelle n-Gramme mit 1-3 Wörtern /Funkion: token_vect\n",
    "    #  * Anwendung auf Training/ Test und mdp Daten\n",
    "\n",
    "        char_vect  = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 7), max_df=0.01, min_df=0.0002,\n",
    "                                     preprocessor=Tokenizer(preserve_case=False, join=True).tokenize)\n",
    "\n",
    "        token_vect = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 3), max_df=0.01, min_df=0.0002,\n",
    "                                     tokenizer=Tokenizer(preserve_case=False, use_stemmer=True).tokenize)\n",
    "\n",
    "        X_CNGR_train = char_vect.fit_transform(X_train)\n",
    "        X_CNGR_mdp = char_vect.transform(X_test_mdp)\n",
    "\n",
    "        X_TNGR_train = token_vect.fit_transform(X_train)\n",
    "        X_TNGR_mdp  = token_vect.transform(X_test_mdp)\n",
    "\n",
    "    ### EMB FEATURES\n",
    "    # * Tweets werden in Token unterteilt\n",
    "    # * Prüfe ob die Token in einem Token im vortrainierten word2vec Model entsprechen\n",
    "    # * Wenn nicht, teile Token in Präfix und Suffix und prüfe für diese das word2vec Model (ggf. beide in emb)\n",
    "    # * emb enthält pro Tweet Vektoren für Token und wird normalisiert mit der Länge des Tweets + ggf extra Tokens\n",
    "    # * X_EMB enthält die normalisierten Vektoren pro Tweet\n",
    "\n",
    "        def get_EMB_feats(tweets):   \n",
    "            tknzr = Tokenizer(preserve_case=True)\n",
    "            tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "\n",
    "            X_EMB = []\n",
    "\n",
    "            for tweet, i in zip(tweets, range(0,len(tweets))):\n",
    "                emb = np.zeros(MODEL_DIMENSION)\n",
    "                extra_tokens = 0\n",
    "\n",
    "                for token in tweet:\n",
    "                    try:\n",
    "                        emb += word2vec_model[token]\n",
    "                    except:\n",
    "                        prefix = find_subtoken(token, word2vec_model, mode='initial')\n",
    "                        suffix = find_subtoken(token, word2vec_model, mode='final')\n",
    "\n",
    "                        if prefix != None and suffix != None:\n",
    "                            emb += word2vec_model[prefix] + word2vec_model[suffix]\n",
    "                            extra_tokens += 1\n",
    "                        elif prefix != None and suffix == None:\n",
    "                            emb += word2vec_model[prefix]\n",
    "                        elif prefix == None and suffix != None:\n",
    "                            emb += word2vec_model[suffix]           \n",
    "                emb /= (len(tweet) + extra_tokens)\n",
    "        #         print(i)\n",
    "        #         print(len(tweet))\n",
    "                X_EMB.append(emb)\n",
    "\n",
    "            return normalize(X_EMB)\n",
    "\n",
    "        X_EMB_train = get_EMB_feats(X_train)\n",
    "        X_EMB_mdp  = get_EMB_feats(X_test_mdp)\n",
    "\n",
    "    # ### TIMP FEATURES\n",
    "    # * Finden der wichtigen Tokens - also derer die in Tweets der angegebenen Kategorie verwendet werden\n",
    "    # * Für diese wichtigsten Tokens werden die Features analog der EMB Features aus dem word2vec model abgeleitet\n",
    "    # * Außerdem werden für alle Tweets analog der EMB feats abgeleitet\n",
    "    # * Vergleiche mit der Cosine Similarity und gebe die höchsten und niedrigsten Werte pro Tweet zurück\n",
    "\n",
    "        def k_most_imp_tokenlvl(k, category, max_df=0.01, min_df=0.0002):      \n",
    "            token_vect = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False,\n",
    "                                         max_df=max_df, min_df=min_df,\n",
    "                                         tokenizer=Tokenizer(preserve_case=True).tokenize)\n",
    "\n",
    "            tfidf = token_vect.fit_transform(X_train)\n",
    "\n",
    "            vocab = token_vect.vocabulary_\n",
    "            inv_vocab = {index: word for word, index in vocab.items()}\n",
    "\n",
    "            if category in ['OTHER', 'OFFENSE']:\n",
    "                cat_ids = np.where(y_train_t1 == category)\n",
    "            elif category in ['PROFANITY', 'ABUSE', 'INSULT']:\n",
    "                cat_ids = np.where(y_train_t2 == category)\n",
    "\n",
    "            most_imp_ids = np.argsort(np.asarray(np.mean(tfidf[cat_ids], axis=0)).flatten())[::-1]\n",
    "\n",
    "            most_imp = []\n",
    "            for index in most_imp_ids:\n",
    "                most_imp.append(inv_vocab[index])\n",
    "\n",
    "            return most_imp[:k]\n",
    "\n",
    "        def get_TIMP_feats(tweets, k, category, max_df=0.01, min_df=0.0002):\n",
    "            feats_max = []\n",
    "            feats_min = []\n",
    "\n",
    "            imp_tokens_vectors = []\n",
    "            for imp_token in k_most_imp_tokenlvl(k, category, max_df=max_df, min_df=min_df):\n",
    "                try:\n",
    "                    imp_tokens_vectors.append(word2vec_model[imp_token])\n",
    "                except:\n",
    "                    imp_tokens_vectors.append(np.zeros(MODEL_DIMENSION))\n",
    "\n",
    "            tknzr = Tokenizer(preserve_case=True)\n",
    "            tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "\n",
    "            for tweet in tweets:\n",
    "                tweet_vectors = []\n",
    "                for token in tweet:\n",
    "                    try:\n",
    "                        tweet_vectors.append(word2vec_model[token])\n",
    "                    except:\n",
    "                        prefix = find_subtoken(token, word2vec_model, mode='initial')\n",
    "                        suffix = find_subtoken(token, word2vec_model, mode='final')\n",
    "\n",
    "                        if prefix != None and suffix != None:\n",
    "                            tweet_vectors.append(word2vec_model[prefix])\n",
    "                            tweet_vectors.append(word2vec_model[suffix])\n",
    "                        elif prefix != None and suffix == None:\n",
    "                            tweet_vectors.append(word2vec_model[prefix])\n",
    "                        elif prefix == None and suffix != None:\n",
    "                            tweet_vectors.append(word2vec_model[suffix])\n",
    "                        else:\n",
    "                            tweet_vectors.append(np.zeros(MODEL_DIMENSION))\n",
    "\n",
    "                similarity = cosine_similarity(np.asarray(tweet_vectors), np.asarray(imp_tokens_vectors))\n",
    "\n",
    "                feats_max.append(np.amax(similarity, axis=0))\n",
    "                feats_min.append(np.amin(similarity, axis=0))\n",
    "\n",
    "            return np.concatenate((feats_max, feats_min), axis=1)\n",
    "        N_TIMP_TASK1 = 1250\n",
    "        N_TIMP_TASK2 = 170\n",
    "\n",
    "        X_TIMP_task1_train = \\\n",
    "        np.concatenate((get_TIMP_feats(X_train, N_TIMP_TASK1, 'OTHER'),\n",
    "                        get_TIMP_feats(X_train, N_TIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "\n",
    "        X_TIMP_task1_mdp = \\\n",
    "        np.concatenate((get_TIMP_feats(X_test_mdp,  N_TIMP_TASK1, 'OTHER'),\n",
    "                        get_TIMP_feats(X_test_mdp,  N_TIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "    ##### CIMP Features\n",
    "\n",
    "        def k_most_imp_charlvl(k, category, max_df=0.01, min_df=0.0002):    \n",
    "            char_vect  = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 7), lowercase=False,\n",
    "                                         max_df=max_df, min_df=min_df,\n",
    "                                         preprocessor=Tokenizer(preserve_case=True, join=True).tokenize)\n",
    "\n",
    "            tfidf = char_vect.fit_transform(X_train)\n",
    "\n",
    "            vocab = char_vect.vocabulary_\n",
    "            inv_vocab = {index: word for word, index in vocab.items()}\n",
    "\n",
    "            if category in ['OTHER', 'OFFENSE']:\n",
    "                cat_ids = np.where(y_train_t1 == category)\n",
    "            elif category in ['PROFANITY', 'ABUSE', 'INSULT']:\n",
    "                cat_ids = np.where(y_train_t2 == category)       \n",
    "\n",
    "            most_imp_ids = np.argsort(np.asarray(np.mean(tfidf[cat_ids], axis=0)).flatten())[::-1]\n",
    "\n",
    "            most_imp = []\n",
    "            for index in most_imp_ids:\n",
    "                most_imp.append(inv_vocab[index])\n",
    "\n",
    "            return most_imp[:k]\n",
    "\n",
    "        def get_CIMP_feats(tweets, k, category, max_df=0.01, min_df=0.0002):\n",
    "            feats = np.zeros((len(tweets), k))\n",
    "            for imp_ngram_index, imp_ngram in enumerate(k_most_imp_charlvl(k, category, max_df=max_df, min_df=min_df)):\n",
    "                for tweet_index, tweet in enumerate(tweets):\n",
    "                    if tweet.find(imp_ngram) != -1:\n",
    "                        feats[tweet_index][imp_ngram_index] = 1\n",
    "            return feats\n",
    "\n",
    "        N_CIMP_TASK1 = 3200\n",
    "        N_CIMP_TASK2 = 370\n",
    "\n",
    "        X_CIMP_task1_train = \\\n",
    "        np.concatenate((get_CIMP_feats(X_train, N_CIMP_TASK1, 'OTHER'),\n",
    "                        get_CIMP_feats(X_train, N_CIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "\n",
    "        X_CIMP_task1_mdp = \\\n",
    "        np.concatenate((get_CIMP_feats(X_test_mdp,  N_CIMP_TASK1, 'OTHER'),\n",
    "                        get_CIMP_feats(X_test_mdp,  N_CIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "    #### Predictions\n",
    "        _, y1, y2 = get_train_data(TRAIN_FILENAME)\n",
    "\n",
    "\n",
    "    ## Funktion für das Aufteilen in Train und Test Sample \n",
    "    # -> StratifiedKFold sorgt dafür, dass das prozentuale Verhältnis der Klassen im jeweiligen Sample (Test, Train) gleich ist\n",
    "\n",
    "\n",
    "        from sklearn.ensemble import ExtraTreesClassifier\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "        from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "\n",
    "        def get_META_feats(clf, X_train, mdp, y, seeds=[42]):\n",
    "            feats_train = []\n",
    "            for seed in seeds:\n",
    "                skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "                feats_train.append(cross_val_predict(clf, X_train, y=y, method='predict_proba', cv=skf, n_jobs=-1))\n",
    "            feats_train = np.mean(feats_train, axis=0)\n",
    "            #print(len(feats_train))\n",
    "            #print(clf)\n",
    "            clf.fit(X_train, y)\n",
    "            feats_mdp = clf.predict_proba(mdp)\n",
    "            #print(len(feats_mdp))\n",
    "\n",
    "            return feats_train, feats_mdp\n",
    "\n",
    "    # ## TASK 1 - Base level predictions\n",
    "    # Die drei verschiedenen Classifier (clfs_task1) werden auf die Feature Vectoren (base_feats_task1) angewandt.\n",
    "    # Von einer 10-fold CrossVal wird für den Trainings Feature Satz der Durchschnitt genommen (jeder Spalte).\n",
    "    # Bei den Test-/mdp Daten wird keine Cross Val durchgeführt (keine y Variablen) sondern nur mit jedem Classifier eine prediction anhand der Feature Vektoren gemacht\n",
    "\n",
    "\n",
    "        clfs_task1 = [LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "                      ExtraTreesClassifier(n_estimators=100, criterion='entropy', n_jobs=-1),\n",
    "                      ExtraTreesClassifier(n_estimators=100, criterion='gini', n_jobs=-1)]\n",
    "\n",
    "        base_feats_task1 = [(X_CIMP_task1_train, X_CIMP_task1_mdp),\n",
    "                            (X_TIMP_task1_train, X_TIMP_task1_mdp),\n",
    "                            (X_CNGR_train, X_CNGR_mdp),\n",
    "                            (X_TNGR_train, X_TNGR_mdp),\n",
    "                            (X_EMB_train, X_EMB_mdp)]\n",
    "        X_META_task1_train = []\n",
    "        #X_META_task1_test  = []\n",
    "        X_META_task1_mdp  = []\n",
    "        for X_train, mdp in base_feats_task1:                 # X-train z.B X_CIMP_task1_train, mdp z.B X_CIMP_task1_mdp\n",
    "            for clf in clfs_task1:\n",
    "                feats = get_META_feats(clf, X_train, mdp, y1)\n",
    "\n",
    "                X_META_task1_train.append(feats[0])           # aus \"get_META_feats: feats_train\n",
    "                X_META_task1_mdp.append(feats[1])             # aus \"get_META_feats: feats_mdp\n",
    "\n",
    "        X_META_task1_train = np.concatenate(X_META_task1_train, axis=1)\n",
    "        X_META_task1_mdp  = np.concatenate(X_META_task1_mdp, axis=1)\n",
    "\n",
    "        clf_task1 = LogisticRegression(C=0.17, class_weight='balanced', max_iter=1000)\n",
    "        clf_task1.fit(X_META_task1_train, y1) \n",
    "\n",
    "        preds_task1 = clf_task1.predict(X_META_task1_mdp) \n",
    "\n",
    "        pd.set_option('display.max_colwidth', 0)\n",
    "        satz['predict'] = preds_task1\n",
    "        print(ID)\n",
    "        print(satz['predict'])\n",
    "        print(\"%s: end labeling\" % (datetime.now().strftime(\"%H:%M:%S\")))\n",
    "        \n",
    "    else:\n",
    "        print(\"ID: \" + ID + \" wurde bereits verwendet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darstellung der Ergebnisse\n",
    "### Laden und zusammenfügen der DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "# Bereits analysierte Reden werden nicht erneut durchlaufen - lese Liste der pickle Files:\n",
    "import csv\n",
    "\n",
    "with open(PICKLE_FOLDER_PATH+'pickle_Reden.txt') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        processed_IDs = row\n",
    "        \n",
    "with open(PICKLE_FOLDER_PATH+'pickle_IDs.txt') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        IDs = row\n",
    "        \n",
    "PicklePath = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Plenum/pickle_TUWien_plenar/'\n",
    "\n",
    "dic_reden = {}\n",
    "i = 0\n",
    "for file, ID in zip(processed_IDs, IDs):\n",
    "    #print(ID)\n",
    "    df_file = pickle.load(open(PicklePath+file, \"rb\" ))         # Öffne file (enthält schlecht strukturierte Daten)\n",
    "    df = df_file[df_file.index == ID]                           # Einschränken auf aktuelle ID weil df alle IDs enthält aber nur predict für genannte!\n",
    "    pred_array = list(df_file[df_file.index == 'predict'])[0]   # Speichere predictions (stehen am Ende des files) in eigenem array\n",
    "    df = pd.DataFrame(df)\n",
    "    df['predict'] = pred_array                                  # Füge prediction array als Spalte an df an\n",
    "    dic_reden[ID] = df\n",
    "    \n",
    "reden_df = pd.concat(dic_reden, axis = 0)\n",
    "reden_df.index = reden_df.index.get_level_values(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gensim, numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "PICKLE_ERSTER_DURCHLAUF = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Plenum/Plenar_TUWien_ersterDurchlauf/'\n",
    "#pickle.dump(reden_df, open(PICKLE_ERSTER_DURCHLAUF + 'reden_sent_TUWien_all.p', \"wb\" ))\n",
    "reden_df = pickle.load(open(PICKLE_ERSTER_DURCHLAUF + 'reden_sent_TUWien_all.p', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ToDo:\n",
    "## - Erstelle df mit Labeln aus compare folder -> Durchläufe nur mit halbem trainingsset\n",
    "## - Überarbeite LabelAlgo - damit nicht als chaos-df gespeichert\n",
    "## - Lasse mit neuem/final Trainingssett durchlaufen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "# Bereits analysierte Reden werden nicht erneut durchlaufen - lese Liste der pickle Files:\n",
    "import csv\n",
    "\n",
    "PicklePath = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Plenum/Plenar_TUWien_ersterDurchlauf/compare_half_trainSet/'\n",
    "\n",
    "with open('/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Plenum/Plenar_TUWien_ersterDurchlauf/compare_half_trainSet/pickle_rede_comp.txt') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        processed_IDs = row\n",
    "        \n",
    "#with open(PicklePath+'pickle_rede_comp_IDs.txt') as csv_file:\n",
    "with open('/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Plenum/Plenar_TUWien_ersterDurchlauf/compare_half_trainSet/pickle_rede_comp_IDs.txt') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        IDs = row\n",
    "        \n",
    "\n",
    "dic_reden = {}\n",
    "i = 0\n",
    "for file, ID in zip(processed_IDs, IDs):\n",
    "    #print(ID)\n",
    "    df_file = pickle.load(open(PicklePath+file, \"rb\" ))         # Öffne file (enthält schlecht strukturierte Daten)\n",
    "    df = df_file[df_file.index == ID]                           # Einschränken auf aktuelle ID weil df alle IDs enthält aber nur predict für genannte!\n",
    "    pred_array = list(df_file[df_file.index == 'predict'])[0]   # Speichere predictions (stehen am Ende des files) in eigenem array\n",
    "    df = pd.DataFrame(df)\n",
    "    df['predict'] = pred_array                                  # Füge prediction array als Spalte an df an\n",
    "    dic_reden[ID] = df\n",
    "    \n",
    "reden_df_comp = pd.concat(dic_reden, axis = 0)\n",
    "reden_df_comp.index = reden_df_comp.index.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text_processed</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rede_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [full_text_processed, predict]\n",
       "Index: []"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reden_df_comp.loc[reden_df_comp['predict'] == 'OFFENSE'] \n",
    "# Bei Training mit halbem Trainingssatz ist kein Satz als OFFENSE gelabelt worden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text_processed</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rede_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID1910106500</th>\n",
       "      <td>diese regierung und diese eu bewirken das gegenteil davon</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID1910402400</th>\n",
       "      <td>wir sind nicht der auffassung, dass beide das alleine können</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID191200200</th>\n",
       "      <td>es ist wichtig, gleichzeitig den haushalt weiter zu konsolidieren</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID1912502900</th>\n",
       "      <td>es geht darum, dass wir die inländische wertschöpfung in den nächsten jahren steigern, dass wir die lebensqualität der deutschen bzw</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID1913000600</th>\n",
       "      <td>gerade deshalb erfordert es ein neues denken, einen neuen politikansatz, der eben nicht die vermeintlichen gegensätze sucht, der nicht den kleinsten gemeinsamen nenner findet</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID1913004600</th>\n",
       "      <td>ich finde, parlamentarische kontrolle gehört ins parlament, und am ende des tages werden auch nicht alle fraktionen im aufsichtsrat vertreten sein können</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID1913412100</th>\n",
       "      <td>ich will ihnen gerne auch noch mal zurufen: schließen sie sich der hohen geschwindigkeit der koalition an! sie können es noch schaffen</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID191401500</th>\n",
       "      <td>deshalb die lehre aus dem desaster in syrien: macht europa stark! lasst uns im europäischen verbund agieren! dann können wir uns in dieser region engagieren, und dann können wir dafür sorgen, dass das leid geringer wird</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID1915206400</th>\n",
       "      <td>– dass möglicherweise anträge abgelehnt werden, gibt es auch im sozialrecht</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID192915700</th>\n",
       "      <td>ich bin mir sehr sicher, dass svenja schulze als umweltministerin das hervorragend weitermachen wird</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID193301900</th>\n",
       "      <td>ein ganz oder teilweise verdecktes gesicht im gerichtsverfahren werden wir nicht hinnehmen</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID193703800</th>\n",
       "      <td>das ist legitim</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID194003800</th>\n",
       "      <td>meine damen und herren, die union schlägt anderes vor</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID196104700</th>\n",
       "      <td>wir alle haben gedacht, dass wir diesen irrtum langsam überwunden hätten</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID197200300</th>\n",
       "      <td>deshalb haben wir das in unserem änderungsantrag zum gesetz noch einmal verdeutlicht</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID197808200</th>\n",
       "      <td>vielen dank</td>\n",
       "      <td>OFFENSE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                       full_text_processed  \\\n",
       "Rede_ID                                                                                                                                                                                                                                      \n",
       "ID1910106500   diese regierung und diese eu bewirken das gegenteil davon                                                                                                                                                                     \n",
       "ID1910402400   wir sind nicht der auffassung, dass beide das alleine können                                                                                                                                                                  \n",
       "ID191200200    es ist wichtig, gleichzeitig den haushalt weiter zu konsolidieren                                                                                                                                                             \n",
       "ID1912502900   es geht darum, dass wir die inländische wertschöpfung in den nächsten jahren steigern, dass wir die lebensqualität der deutschen bzw                                                                                          \n",
       "ID1913000600   gerade deshalb erfordert es ein neues denken, einen neuen politikansatz, der eben nicht die vermeintlichen gegensätze sucht, der nicht den kleinsten gemeinsamen nenner findet                                                \n",
       "ID1913004600   ich finde, parlamentarische kontrolle gehört ins parlament, und am ende des tages werden auch nicht alle fraktionen im aufsichtsrat vertreten sein können                                                                     \n",
       "ID1913412100   ich will ihnen gerne auch noch mal zurufen: schließen sie sich der hohen geschwindigkeit der koalition an! sie können es noch schaffen                                                                                        \n",
       "ID191401500    deshalb die lehre aus dem desaster in syrien: macht europa stark! lasst uns im europäischen verbund agieren! dann können wir uns in dieser region engagieren, und dann können wir dafür sorgen, dass das leid geringer wird   \n",
       "ID1915206400   – dass möglicherweise anträge abgelehnt werden, gibt es auch im sozialrecht                                                                                                                                                   \n",
       "ID192915700    ich bin mir sehr sicher, dass svenja schulze als umweltministerin das hervorragend weitermachen wird                                                                                                                          \n",
       "ID193301900    ein ganz oder teilweise verdecktes gesicht im gerichtsverfahren werden wir nicht hinnehmen                                                                                                                                    \n",
       "ID193703800    das ist legitim                                                                                                                                                                                                               \n",
       "ID194003800    meine damen und herren, die union schlägt anderes vor                                                                                                                                                                         \n",
       "ID196104700    wir alle haben gedacht, dass wir diesen irrtum langsam überwunden hätten                                                                                                                                                      \n",
       "ID197200300    deshalb haben wir das in unserem änderungsantrag zum gesetz noch einmal verdeutlicht                                                                                                                                          \n",
       "ID197808200    vielen dank                                                                                                                                                                                                                   \n",
       "\n",
       "              predict  \n",
       "Rede_ID                \n",
       "ID1910106500  OFFENSE  \n",
       "ID1910402400  OFFENSE  \n",
       "ID191200200   OFFENSE  \n",
       "ID1912502900  OFFENSE  \n",
       "ID1913000600  OFFENSE  \n",
       "ID1913004600  OFFENSE  \n",
       "ID1913412100  OFFENSE  \n",
       "ID191401500   OFFENSE  \n",
       "ID1915206400  OFFENSE  \n",
       "ID192915700   OFFENSE  \n",
       "ID193301900   OFFENSE  \n",
       "ID193703800   OFFENSE  \n",
       "ID194003800   OFFENSE  \n",
       "ID196104700   OFFENSE  \n",
       "ID197200300   OFFENSE  \n",
       "ID197808200   OFFENSE  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_rows', 0)\n",
    "reden_df.loc[reden_df['predict'] == 'OFFENSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gensim, numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "pickle.dump(reden_df_comp, open(PicklePath + 'comp_reden_sent_TUWien_all.p', \"wb\" ))\n",
    "#reden_df_comp = pickle.load(open(PicklePath + 'comp_ reden_sent_TUWien_all.p', \"rb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
