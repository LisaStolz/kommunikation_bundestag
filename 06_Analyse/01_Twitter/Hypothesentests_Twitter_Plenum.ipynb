{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesentests\n",
    "* H01: Die Mitglieder der AfD twittern genauso viele oder weniger OFFENSE-Tweets als die Mitglieder jeder anderen Partei.\n",
    "* H02: Die Häufigkeit von OFFENSE-Tweets vor dem Corona Lockdown unterscheidet sich NICHT von der während dem Lockdown.\n",
    "* H03: Die Häufigkeit von OFFENSE-Tweets nach dem Corona Lockdown unterscheidet sich NICHT von der während dem Lockdown.\n",
    "* H04: Die Häufigkeit von OFFENSE-Tweets vor dem Corona Lockdown unterscheidet sich NICHT von der nach dem Lockdown.\n",
    "* H05: Die Mitglieder der AfD rufen im Parlament höchstens genauso oft OFFENSE-Zurufe hinein wie die Mitglieder jeder anderen Partei.\n",
    "* H06: Die Mitglieder jeder Partei rufen im Plenum höchstens genauso oft OFFENSE-Zurufe hinein wie sie OFFENSE-Tweets absetzen.\n",
    "* H07: Der Anteil von OFFENSE-Zurufen vor Corona ist genauso groß oder kleiner als währenddessen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data -Twitter all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle, gensim, numpy as np\n",
    "\n",
    "sent_df_Wien = pickle.load(open('/home/lisa/Darmstadt/Master Arbeit/06_Analyse/01_Twitter/sent_df_Wien_final.p', \"rb\" ))\n",
    "# Tweets von Johannes Kahrs werdne entfernt um Normalverteilung zu erhalten\n",
    "sent_df_Wien = sent_df_Wien.loc[sent_df_Wien['user_screen_name'] != 'kahrs'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "time = ['month', 'week', 'date'] # 15 Monate, 67 Wochen, 454 Tage\n",
    "cat = ['', 'OFFENSE', 'OTHER']\n",
    "twitter_dic = {}\n",
    "\n",
    "\n",
    "for c in cat: \n",
    "    if c == '':\n",
    "        sent_df = sent_df_Wien\n",
    "    else:\n",
    "        sent_df = sent_df_Wien.loc[sent_df_Wien['predict'] == c]\n",
    "    for t in time:\n",
    "        twitter_dic['piv_'+t+c] = pd.pivot_table(sent_df, index = [t], columns = ['user_party'], aggfunc = np.count_nonzero)['full_text'].fillna(0)  \n",
    "\n",
    "twitter_dic.keys()\n",
    "\n",
    "for t in time:\n",
    "    # Normiere OFFENSE Tweets\n",
    "    twitter_dic['piv_'+t+'_rel'] = (twitter_dic['piv_'+t+'OFFENSE'] / twitter_dic['piv_'+t])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Normality - Twitter all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/\n",
    "# generate gaussian data\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "# seed the random number generator\n",
    "seed(1)\n",
    "# generate univariate observations\n",
    "data = 5 * randn(100) + 50\n",
    "\n",
    "# summarize\n",
    "print('mean=%.3f stdv=%.3f' % (mean(data), std(data)))\n",
    "print('mean=%.3f stdv=%.3f' % (mean(twitter_dic['piv_week']['Union']), std(twitter_dic['piv_week']['Union'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durch = []\n",
    "partei = []\n",
    "daten = []\n",
    "\n",
    "for dat in twitter_dic.keys():\n",
    "    for party in twitter_dic['piv_month'].columns:\n",
    "        daten.append(dat)\n",
    "        partei.append(party)\n",
    "        durch.append('Mean: '+ str(mean(twitter_dic[dat][party]).round(3)) + ' Std:'+str(std(twitter_dic[dat][party]).round(3)))\n",
    "\n",
    "data_mean_std = pd.DataFrame({'Datensatz': daten, 'Partei': partei, 'Mean': durch})\n",
    "data_mean_std_piv = pd.pivot_table(data_mean_std, values = 'Mean', index = 'Datensatz', columns = 'Partei', aggfunc = min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean_std_piv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "datensatz = twitter_dic['piv_month_rel']\n",
    "pyplot.style.use('seaborn-darkgrid')\n",
    "my_palette = ['blue', 'orange', 'green', 'purple', 'red', 'black']\n",
    "\n",
    "fig = pyplot.figure(figsize = (16,10))\n",
    "fig.suptitle('Scatterplots der monatlichen Summe an Tweets', fontsize = 18)\n",
    "\n",
    "num = 0\n",
    "for party, n in zip(datensatz.columns, range(1,7)):\n",
    "    ax = fig.add_subplot(2,3,n)\n",
    "    ax.scatter(datensatz.index, datensatz[party], color = my_palette[num])\n",
    "    x_tags = pyplot.xticks(datensatz.index, fontsize = 7, rotation = 45, ha = \"right\")\n",
    "    ax.set_title(party)\n",
    "    num = num + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datensatz = twitter_dic['piv_week_rel']\n",
    "pyplot.style.use('seaborn-darkgrid')\n",
    "my_palette = ['blue', 'orange', 'green', 'purple', 'red', 'black']\n",
    "Parteien = ['AFD', 'FDP', 'Gruene', 'Linke', 'SPD', 'Union']\n",
    "\n",
    "fig = pyplot.figure(figsize = (13,8))\n",
    "fig.suptitle('Histogramme der relativen Häufigkeit von OFFENSE-Tweets pro Woche', fontsize = 18)\n",
    "\n",
    "num = 0\n",
    "for party, n in zip(datensatz.columns, range(1,7)):\n",
    "    ax = fig.add_subplot(2,3,n)\n",
    "    ax.hist(datensatz[party], color = my_palette[num])\n",
    "    ax.set_title(party)\n",
    "    num = num + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQ Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ Plot\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "qqplot(twitter_dic['piv_week_rel']['SPD'], line='s')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk\n",
    "Ergebnis: \n",
    "\n",
    "* Die meisten sind nicht Normalverteilt - überraschend, garnicht bei piv_date Datensätzen! -> Ergibt Sinn, weil Shapiro Wilk nur für < 50 Beobachtungen!\n",
    "* piv_month und piv_monthOTHER komplett Normalverteilt - Union, Linke und AFD auch bei OFFENSE Tweets\n",
    "* piv_week und piv_weekOTHER ebenfalls komplett - Nur AfD auch bei OFFENSE Tweets normalverteilt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Shapiro-Wilk Test\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "partei = []\n",
    "dataset = []\n",
    "result = []\n",
    "\n",
    "for party in twitter_dic['piv_month'].columns:\n",
    "    for dat in twitter_dic.keys():\n",
    "        partei.append(party)\n",
    "        dataset.append(dat)\n",
    "# normality test\n",
    "        stat, p = shapiro(twitter_dic[dat][party])\n",
    "\n",
    "# interpret\n",
    "        alpha = 0.05\n",
    "        if p > alpha:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + ' Gaussian')\n",
    "        else:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + ' NOT Gaussian')\n",
    "            \n",
    "Shapiro_Wilk_df = pd.DataFrame({'Partei':partei, 'Daten':dataset, 'Ergebnis':result})\n",
    "Shapiro_Wilk_piv = pd.pivot_table(Shapiro_Wilk_df, values='Ergebnis', index=['Daten'], columns=['Partei'], aggfunc=max)\n",
    "Shapiro_Wilk_piv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corellationstests - Twitter all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman’s Rank Correlation\n",
    "-> Pearson würde zu gleichen Ergebnisse führen, aber Varianze nicht gleich und nur teilweise Normalverteilt (Verletzt Voraussetzungen)!\n",
    "\n",
    "*   Observations in each sample are independent and identically distributed (iid).\n",
    "*   Observations in each sample can be ranked.\n",
    "\n",
    "\n",
    "*   H0: the two samples are independent.\n",
    "*   H1: there is a dependency between the samples.\n",
    "\n",
    "Ergibt Sinn, dass fasst alle dependent sind! -> Getrieben von äußeren Events auf die fast alle Politiker*innen reagieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "partei = []\n",
    "dataset = []\n",
    "result = []\n",
    "\n",
    "for party in twitter_dic['piv_month'].columns:\n",
    "    for dat in twitter_dic.keys():\n",
    "        partei.append(party)\n",
    "        dataset.append(dat)\n",
    "# normality test\n",
    "        stat, p = spearmanr(twitter_dic[dat][party], twitter_dic[dat]['AFD'])\n",
    "\n",
    "        if p > 0.05:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + 'INDEPENDENT')\n",
    "        else:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + 'Probably dependent')\n",
    "            \n",
    "Corr_Spearman_df = pd.DataFrame({'Partei':partei, 'Daten':dataset, 'Ergebnis':result})\n",
    "Corr_Spearman_piv =  pd.pivot_table(Corr_Spearman_df, values='Ergebnis', index=['Daten'], columns=['Partei'], aggfunc=max) \n",
    "# Ergibt Sinn, dass ALLE dependent sind!\n",
    "# -> Getrieben von äußeren Events auf die fast alle Politiker*innen reagieren\n",
    "Corr_Spearman_piv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared Test\n",
    "\n",
    "Tests whether two categorical variables are related or independent.\n",
    "\n",
    "*   Observations used in the calculation of the contingency table are independent.\n",
    "*   25 or more examples in each cell of the contingency table.\n",
    "\n",
    "-> Es gibt eine Abhängigkeit zwischen Partei und Häufigkeit von OFFENSE/ OTHER Tweets (Bei Tests zwischen einzelnen Parteien nicht immer vorhanden, z.B Grüne und Linke sind dependent, SPD und FDP nicht!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.pivot_table(sent_df_Wien, index = 'user_party', columns = 'predict', aggfunc=np.count_nonzero)['full_text']\n",
    "# sent_df_Wien_noAfD = sent_df_Wien.loc[(sent_df_Wien['user_party'] == 'Gruene')|(sent_df_Wien['user_party'] == 'Linke')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "stat, p, dof, expected = chi2_contingency(contingency_table)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "if p > 0.05:\n",
    "    print('Probably independent')\n",
    "else:\n",
    "    print('Probably dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H01: Die Mitglieder der AfD twittern genauso viele oder weniger OFFENSE-Tweets als die Mitglieder jeder anderen Partei.\n",
    "* Zur beantwortung der Frage werden "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student’s t-test\n",
    "\n",
    "Tests whether the means of two independent samples are significantly different.\n",
    "\n",
    "*    Observations in each sample are independent and identically distributed (iid).\n",
    "*    Observations in each sample are normally distributed.\n",
    "*    Observations in each sample have the same variance.\n",
    "\n",
    "Interpretation von stats.ttest_ind(B,A)\n",
    "\n",
    "*    H0: A >= B (AFD hat insgesamt mehr Tweets als anderen Parteien)\n",
    "*    H1: B > A  (AFD hat weniger Tweets als andere Parteien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "partei = []\n",
    "dataset = []\n",
    "result = []\n",
    "\n",
    "for party in twitter_dic['piv_month_rel'].columns:\n",
    "    for dat in twitter_dic.keys():\n",
    "        partei.append(party)\n",
    "        dataset.append(dat)\n",
    "# normality test\n",
    "        stat, p = ttest_ind(twitter_dic[dat]['AFD'], (twitter_dic[dat][party]))\n",
    "        if p/2 > 0.05 or stat < 0:    # p/2 für Einseitigen t-Test\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + party +' gleich oder mehr als AfD' )\n",
    "        else:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + party +' hat WENIGER als AfD')\n",
    "                        \n",
    "H01_df = pd.DataFrame({'Partei':partei, 'Daten':dataset, 'Ergebnis':result})\n",
    "H01_piv =  pd.pivot_table(H01_df, values='Ergebnis', index=['Daten'], columns=['Partei'], aggfunc=max) \n",
    "H01_piv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "dataset = twitter_dic['piv_week_rel']\n",
    "\n",
    "stat, p = f_oneway(dataset['AFD'], dataset['FDP'], dataset['Union'])\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "    print('Probably the same distribution')\n",
    "else:\n",
    "    print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nichtparametrische - Mann-Whitney U Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example of the Mann-Whitney U Test\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "partei = []\n",
    "dataset = []\n",
    "result = []\n",
    "\n",
    "for party in twitter_dic['piv_month_rel'].columns:\n",
    "    for dat in twitter_dic.keys():\n",
    "        partei.append(party)\n",
    "        dataset.append(dat)\n",
    "# normality test\n",
    "        stat, p = mannwhitneyu(twitter_dic[dat]['AFD'], twitter_dic[dat][party])\n",
    "        if p > 0.05:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + party +' gleich oder weniger als AfD' )\n",
    "        else:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + party +' AfD hat mehr')\n",
    "\n",
    "          \n",
    "Mann_Whitney_df = pd.DataFrame({'Partei':partei, 'Daten':dataset, 'Ergebnis':result})\n",
    "Mann_Whitney_piv =  pd.pivot_table(Mann_Whitney_df, values='Ergebnis', index=['Daten'], columns=['Partei'], aggfunc=max) \n",
    "Mann_Whitney_piv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pingouin as pg\n",
    "from pingouin import ttest\n",
    "#https://pingouin-stats.org/generated/pingouin.mwu.html\n",
    "#https://pingouin-stats.org/generated/pingouin.ttest.html#pingouin.ttest\n",
    "\n",
    "\n",
    "for party in twitter_dic['piv_month_rel'].columns:\n",
    "    dat = twitter_dic['piv_week_rel']\n",
    "    partei.append(party)\n",
    "    dataset.append(dat)\n",
    "# normality test\n",
    "    print(party)\n",
    "    print(mannwhitneyu(dat['AFD'], dat[party]))\n",
    "    print(pg.mwu(dat[party], dat['AFD']))\n",
    "    #print(ttest(dat['AFD'], dat[party], paired=True, tail='greater').round(2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H02: Die Häufigkeit von OFFENSE-Tweets vor dem Corona Lockdown unterscheidet sich NICHT von der während des Lockdowns.\n",
    "\n",
    "=> WICHTIG: Für H02-H04 muss beachtet werden, dass es sich um verbundene Stichproben handelt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingrenzen der relevanten Zeiträume\n",
    "sent_Corona = sent_df_Wien.loc[(sent_df_Wien['date'] >= datetime.date(2020,3,13))\n",
    "                & (sent_df_Wien['date'] < datetime.date(2020,5,6))]\n",
    "\n",
    "sent_before_Corona = sent_df_Wien.loc[(sent_df_Wien['date'] < datetime.date(2020,3,13))]\n",
    "sent_after_Corona = sent_df_Wien.loc[(sent_df_Wien['date'] >= datetime.date(2020,5,6))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle Dictionarys für Zeträume\n",
    "import pandas as pd\n",
    "\n",
    "Corona_dic = {}\n",
    "before_Corona_dic = {}\n",
    "after_Corona_dic = {}\n",
    "\n",
    "dics =[Corona_dic, before_Corona_dic, after_Corona_dic]\n",
    "datasets = [sent_Corona, sent_before_Corona, sent_after_Corona]\n",
    "\n",
    "for dataset, dic in zip(datasets, dics):\n",
    "    time = ['month', 'week', 'date']\n",
    "    cat = ['', 'OFFENSE', 'OTHER']\n",
    "    \n",
    "    for c in cat: \n",
    "        if c == '':\n",
    "            sent_df = dataset\n",
    "        else:\n",
    "            sent_df = dataset.loc[dataset['predict'] == c]\n",
    "        for t in time:\n",
    "            dic['piv_'+t+c] = pd.pivot_table(sent_df, index = [t], columns = ['user_party'], aggfunc = np.count_nonzero)['full_text'].fillna(0)         \n",
    "            \n",
    "for t in time:\n",
    "    for dic in dics:\n",
    "        # Normiere OFFENSE Tweets\n",
    "        dic['piv_'+t+'_rel'] = (dic['piv_'+t+'OFFENSE'] / dic['piv_'+t])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "durch = []\n",
    "abw = []\n",
    "partei = []\n",
    "daten = []\n",
    "dic = before_Corona_dic\n",
    "\n",
    "for dat, n in zip(dic, dic.keys()):\n",
    "    for party in dic['piv_month'].columns:\n",
    "        daten.append(n)\n",
    "        partei.append(party)\n",
    "        durch.append(mean(dic[dat][party]).round())\n",
    "        abw.append(std(dic[dat][party]))\n",
    "\n",
    "mean_std = pd.DataFrame({'Datensatz': daten, 'Partei': partei, 'Mean': durch, 'Std': abw})\n",
    "#mean_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "datensatz = after_Corona_dic['piv_weekOFFENSE']\n",
    "pyplot.style.use('seaborn-darkgrid')\n",
    "my_palette = ['blue', 'orange', 'green', 'purple', 'red', 'black']\n",
    "\n",
    "fig = pyplot.figure(figsize = (16,10))\n",
    "fig.suptitle('Scatterplots der monatlichen Summe an Tweets', fontsize = 18)\n",
    "\n",
    "num = 0\n",
    "for party, n in zip(Corona_dic['piv_date'].columns, range(1,7)):\n",
    "    ax = fig.add_subplot(2,3,n)\n",
    "    ax.scatter(datensatz.index, datensatz[party], color = my_palette[num])\n",
    "    x_tags = pyplot.xticks(datensatz.index, fontsize = 7, rotation = 45, ha = \"right\")\n",
    "    ax.set_title(party)\n",
    "    num = num + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datensatz =  after_Corona_dic['piv_date_rel']\n",
    "pyplot.style.use('seaborn-darkgrid')\n",
    "my_palette = ['blue', 'orange', 'green', 'purple', 'red', 'black']\n",
    "Parteien = ['AFD', 'FDP', 'Gruene', 'Linke', 'SPD', 'Union']\n",
    "\n",
    "fig = pyplot.figure(figsize = (16,10))\n",
    "fig.suptitle('Histogramme der Summe an Zurufen', fontsize = 18)\n",
    "\n",
    "num = 0\n",
    "for party, n in zip(Corona_dic['piv_week'].columns, range(1,7)):\n",
    "    ax = fig.add_subplot(2,3,n)\n",
    "    ax.hist(datensatz[party], color = my_palette[num])\n",
    "    ax.set_title(party)\n",
    "    num = num + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "partei = []\n",
    "dataset = []\n",
    "result = []\n",
    "dic = after_Corona_dic\n",
    "\n",
    "for party in Corona_dic['piv_date'].columns:\n",
    "    for dat in dic.keys():\n",
    "        partei.append(party)\n",
    "        dataset.append(dat)\n",
    "# normality test H0: Dataset frwan from Gausian distribution\n",
    "        stat, p = shapiro(dic[dat][party])\n",
    "\n",
    "        alpha = 0.05\n",
    "        if p > alpha:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + ' Gaussian') # Weise H0 nicht zurück\n",
    "        else:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + ' NOT Gaussian')\n",
    "            \n",
    "Shapiro_Wilk_Corona_df = pd.DataFrame({'Partei':partei, 'Daten':dataset, 'Ergebnis':result})\n",
    "Shapiro_Wilk_Corona_piv = pd.pivot_table(Shapiro_Wilk_Corona_df, values='Ergebnis', index=['Daten'], columns=['Partei'], aggfunc=max)\n",
    "Shapiro_Wilk_Corona_piv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_Corona = pd.pivot_table(sent_after_Corona, index = 'user_party', columns = 'predict', aggfunc=np.count_nonzero)['full_text']\n",
    "contingency_Corona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "stat, p, dof, expected = chi2_contingency(contingency_Corona)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "if p > 0.05:\n",
    "    print('Probably independent')\n",
    "else:\n",
    "    print('Probably dependent')\n",
    "# Häufigkeit OFFENSE oder OTHER ist abhängig von der Partei "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## T-Test eigentlich schlecht - weil nicht normalverteilt!\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "partei = []\n",
    "result = []    \n",
    "\n",
    "for party in before_Corona_rel_piv.columns:\n",
    "        partei.append(party)\n",
    "# normality test\n",
    "        stat, p = ttest_ind(before_Corona_rel_piv[party], Corona_rel_piv[party])\n",
    "        if p/2 > 0.05 or stat < 0:    # p/2 für Einseitigen t-Test\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' Gleich oder mehr als vor dem Lockdown' )\n",
    "        else:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' WENIGER als vor dem Lockdown')\n",
    "          \n",
    "                        \n",
    "H03_bef_Corona_df = pd.DataFrame({'Partei':partei, 'Ergebnis':result})\n",
    "H03_bef_Corona_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wilcoxon Rang Test\n",
    "\n",
    "Da getestet werden soll, ob vor- bzw. nach dem Lockdown MEHR OFFENSE-Tweets abgesetzt wurden, werden einseitige Tests durchgeführt!\n",
    "\n",
    "ABER: Es wird nicht die option \"alternative = 'greater'\" (scipy) oder \"tail='greater'\" (pinguin) verwendet! Beide passen den p-Wert automatisch an und zeigen die aufsummierten Ränge von der größeren Statistik. Das entspricht nicht den gängigen Anleitungen, nach denen der kleinere Wert der Statistik unter einem Schwellenwert liegen muss. Der p Wert wird also \"manuell\" halbiert und mit dem Ergebnis verglichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Mann-Whitney U Test\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "partei = []\n",
    "result = []\n",
    "\n",
    "for party in before_Corona_rel_piv.columns:\n",
    "        partei.append(party)\n",
    "# normality test\n",
    "        stat, p = wilcoxon(before_Corona_dic['piv_week_rel'][38:][party], Corona_dic['piv_week_rel'][party])\n",
    "        #stat, p = wilcoxon(before_Corona_dic['piv_date_rel'][260:][party], Corona_dic['piv_date_rel'][party])\n",
    "        if p/2 > 0.05:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' Gleich oder mehr als vor dem Lockdown')\n",
    "        else:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' WENIGER als vor dem Lockdown')\n",
    "\n",
    "          \n",
    "wilcoxon_df = pd.DataFrame({'Partei':partei, 'Ergebnis':result})\n",
    "wilcoxon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pingouin as pg\n",
    "from pingouin import ttest\n",
    "#https://pingouin-stats.org/generated/pingouin.mwu.html\n",
    "#https://pingouin-stats.org/generated/pingouin.ttest.html#pingouin.ttest\n",
    "\n",
    "for party in before_Corona_dic['piv_date_rel'].columns:\n",
    "# normality test\n",
    "        #stat, p = wilcoxon(before_Corona_dic['piv_week_rel'][38:][party], Corona_dic['piv_week_rel'][party])\n",
    "        print(party)\n",
    "        #print(pg.wilcoxon(before_Corona_dic['piv_date_rel'][260:][party], Corona_dic['piv_date_rel'][party]))\n",
    "        print(pg.wilcoxon(before_Corona_dic['piv_week_rel'][38:][party], Corona_dic['piv_week_rel'][party]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_Corona_dic['piv_week_rel'][38:]\n",
    "#before_Corona_dic['piv_week_rel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomisierte ziehung der neun Wochen aus den Wochen vor dem Lockdown\n",
    "import random\n",
    "\n",
    "sample_before = random.sample(list(before_Corona_dic['piv_week_rel'].index), 9)\n",
    "before_week_sample = before_Corona_dic['piv_week_rel'].loc[sample_before]\n",
    "\n",
    "for party in before_Corona_dic['piv_date_rel'].columns:\n",
    "# normality test\n",
    "        #stat, p = wilcoxon(before_Corona_dic['piv_week_rel'][38:][party], Corona_dic['piv_week_rel'][party])\n",
    "        print(party)\n",
    "        #print(pg.wilcoxon(before_Corona_dic['piv_date_rel'][260:][party], Corona_dic['piv_date_rel'][party]))\n",
    "        print(pg.wilcoxon(before_week_sample[party], Corona_dic['piv_week_rel'][party]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H03: Die Häufigkeit von OFFENSE-Tweets während dem Corona Lockdown unterscheidet sich NICHT von der nach dem Lockdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## T-Test eigentlich schlecht - weil nicht normalverteilt!\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "partei = []\n",
    "result = []    \n",
    "\n",
    "for party in after_Corona_rel_piv.columns:\n",
    "        partei.append(party)\n",
    "# normality test\n",
    "        stat, p = ttest_ind(after_Corona_rel_piv[party], Corona_rel_piv[party])\n",
    "        if p/2 > 0.05 or stat < 0:    # p/2 für Einseitigen t-Test\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' Gleich oder mehr als nach dem Lockdown' )\n",
    "        else:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' WENIGER als nach dem Lockdown')\n",
    "          \n",
    "                        \n",
    "H03_bef_Corona_df = pd.DataFrame({'Partei':partei, 'Ergebnis':result})\n",
    "H03_bef_Corona_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Mann-Whitney U Test\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "partei = []\n",
    "result = []\n",
    "\n",
    "for party in before_Corona_rel_piv.columns:\n",
    "        partei.append(party)\n",
    "# normality test\n",
    "        stat, p = wilcoxon(after_Corona_dic['piv_week_rel'][:9][party], Corona_dic['piv_week_rel'][party])\n",
    "        #stat, p = wilcoxon(after_Corona_dic['piv_date_rel'][:54][party], Corona_dic['piv_date_rel'][party])\n",
    "        if p/2 > 0.05:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' Gleich oder mehr als nach dem Lockdown')\n",
    "        else:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' WENIGER als nach dem Lockdown')\n",
    "\n",
    "          \n",
    "wilcoxon_df = pd.DataFrame({'Partei':partei, 'Ergebnis':result})\n",
    "wilcoxon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pingouin as pg\n",
    "from pingouin import ttest\n",
    "#https://pingouin-stats.org/generated/pingouin.mwu.html\n",
    "#https://pingouin-stats.org/generated/pingouin.ttest.html#pingouin.ttest\n",
    "\n",
    "for party in before_Corona_dic['piv_date_rel'].columns:\n",
    "# normality test\n",
    "        #stat, p = wilcoxon(before_Corona_dic['piv_week_rel'][38:][party], Corona_dic['piv_week_rel'][party])\n",
    "        print(party)\n",
    "        #print(pg.wilcoxon(after_Corona_dic['piv_date_rel'][:54][party], Corona_dic['piv_date_rel'][party]))\n",
    "        print(pg.wilcoxon(after_Corona_dic['piv_week_rel'][:9][party], Corona_dic['piv_week_rel'][party]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H04: Die Häufigkeit von OFFENSE-Tweets vor dem Corona Lockdown unterscheidet sich NICHT von der nach dem Lockdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_Corona_rel_piv = (before_Corona_dic['piv_dateOFFENSE'] / before_Corona_dic['piv_date'])*100\n",
    "after_Corona_rel_piv = ((after_Corona_dic['piv_dateOFFENSE']/ after_Corona_dic['piv_date'])*100).fillna(0)\n",
    "bef_Corona_rel_piv = before_Corona_rel_piv[228:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(before_Corona_rel_piv) - len(after_Corona_rel_piv)\n",
    "len(before_Corona_rel_piv[228:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "partei = []\n",
    "result = []    \n",
    "\n",
    "for party in after_Corona_rel_piv.columns:\n",
    "        partei.append(party)\n",
    "# normality test\n",
    "        stat, p = ttest_rel(after_Corona_rel_piv[party], bef_Corona_rel_piv[party])\n",
    "        if p > 0.05:    \n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' Gleiche Häufigkeit vor und nach dem Lockdown' )\n",
    "        else:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' Andere Häufigkeiten vor und nach dem Lockdown')\n",
    "          \n",
    "                        \n",
    "H04_Corona_df = pd.DataFrame({'Partei':partei, 'Ergebnis':result})\n",
    "H04_Corona_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "partei = []\n",
    "result = []\n",
    "\n",
    "for party in before_Corona_rel_piv.columns:\n",
    "        partei.append(party)\n",
    "# normality test\n",
    "        stat, p = wilcoxon(after_Corona_dic['piv_week_rel'][party], before_Corona_dic['piv_week_rel'][34:][party])\n",
    "        #stat, p = wilcoxon(after_Corona_dic['piv_date_rel'][party], before_Corona_dic['piv_date_rel'][228:][party])\n",
    "        if p/2 > 0.05:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' Davor weniger oder gleich')\n",
    "        else:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' Davor MEHR')\n",
    "\n",
    "          \n",
    "wilcoxon_df = pd.DataFrame({'Partei':partei, 'Ergebnis':result})\n",
    "wilcoxon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pingouin as pg\n",
    "from pingouin import ttest\n",
    "#https://pingouin-stats.org/generated/pingouin.mwu.html\n",
    "#https://pingouin-stats.org/generated/pingouin.ttest.html#pingouin.ttest\n",
    "\n",
    "for party in before_Corona_dic['piv_date_rel'].columns:\n",
    "# normality test\n",
    "        #stat, p = wilcoxon(before_Corona_dic['piv_week_rel'][38:][party], Corona_dic['piv_week_rel'][party])\n",
    "        print(party)\n",
    "        #print(pg.wilcoxon(after_Corona_dic['piv_date_rel'][party], before_Corona_dic['piv_date_rel'][228:][party], tail='less'))\n",
    "        print(pg.wilcoxon(before_Corona_dic['piv_week_rel'][34:][party], after_Corona_dic['piv_week_rel'][party]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gensim, numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "#df_kom = mdp_partei_emo\n",
    "PICKLE_FOLDER_PATH = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Plenum/'\n",
    "#pickle.dump(df_kom, open(PICKLE_FOLDER_PATH + 'Plenum_Kom_all_Sent', \"wb\" ))\n",
    "\n",
    "df_kom = pickle.load(open(PICKLE_FOLDER_PATH + 'Plenum_Kom_all_Sent', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "time = ['Monat', 'Woche', 'date_str']\n",
    "cat = ['', 'OFFENSE', 'OTHER']\n",
    "plenum_dic = {}\n",
    "\n",
    "\n",
    "for c in cat: \n",
    "    if c == '':\n",
    "        sent_df = df_kom\n",
    "    else:\n",
    "        sent_df = df_kom.loc[df_kom['predict'] == c]\n",
    "    for t in time: \n",
    "        piv = pd.pivot_table(sent_df, index = [t], columns = ['Partei'], aggfunc = np.count_nonzero)['Beschreibung1'].fillna(0)\n",
    "        \n",
    "        plenum_dic['piv_'+t+c] =  piv       \n",
    "            \n",
    "for woche in ['17-w43', '18-w03', '18-w36', '19-w29', '19-w36']:\n",
    "    plenum_dic['piv_Woche'] = plenum_dic['piv_Woche'].loc[plenum_dic['piv_Woche'].index != woche]\n",
    "    plenum_dic['piv_WocheOFFENSE'] = plenum_dic['piv_WocheOFFENSE'].loc[plenum_dic['piv_WocheOFFENSE'].index != woche]\n",
    "\n",
    "# Normiere OFFENSE Tweets\n",
    "plenum_dic['piv_Woche_rel'] = (plenum_dic['piv_WocheOFFENSE'] / plenum_dic['piv_Woche'])*100\n",
    "    \n",
    "plenum_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "datensatz = plenum_dic['piv_Woche_rel']\n",
    "pyplot.style.use('seaborn-darkgrid')\n",
    "my_palette = ['blue', 'orange', 'green', 'purple', 'red', 'black']\n",
    "\n",
    "fig = pyplot.figure(figsize = (16,10))\n",
    "fig.suptitle('Scatterplots der monatlichen Summe an Tweets', fontsize = 18)\n",
    "\n",
    "num = 0\n",
    "for party, n in zip(plenum_dic['piv_Woche_rel'].columns, range(1,7)):\n",
    "    ax = fig.add_subplot(2,3,n)\n",
    "    ax.scatter(datensatz.index, datensatz[party], color = my_palette[num])\n",
    "    x_tags = pyplot.xticks(datensatz.index, fontsize = 7, rotation = 45, ha = \"right\")\n",
    "    ax.set_title(party)\n",
    "    num = num + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datensatz =  plenum_dic['piv_Woche_rel']\n",
    "pyplot.style.use('seaborn-darkgrid')\n",
    "my_palette = ['blue', 'green', 'black', 'purple', 'orange', 'red']\n",
    "\n",
    "fig = pyplot.figure(figsize = (13,8))\n",
    "fig.suptitle('Histogramme der Summe an Zurufen', fontsize = 18)\n",
    "\n",
    "num = 0\n",
    "for party, n in zip(plenum_dic['piv_Woche_rel'].columns, range(1,7)):\n",
    "    ax = fig.add_subplot(2,3,n)\n",
    "    ax.hist(datensatz[party], color = my_palette[num])\n",
    "    ax.set_title(party)\n",
    "    num = num + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "partei = []\n",
    "durch = []\n",
    "abw = []\n",
    "\n",
    "for party in plenum_dic['piv_Woche_rel'].columns:\n",
    "        partei.append(party)\n",
    "        durch.append('Mean: '+ str(mean(plenum_dic['piv_Woche_rel'][party]).round(3)))\n",
    "        abw.append('Std:'+str(std(plenum_dic['piv_Woche_rel'][party]).round(3)))\n",
    "        \n",
    "plenum_data_mean_std = pd.DataFrame({'Partei': partei, 'Mean': durch, 'Std': abw})\n",
    "plenum_data_mean_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "partei = []\n",
    "dataset = []\n",
    "result = []\n",
    "\n",
    "for party in plenum_dic['piv_date_str'].columns:\n",
    "    for dat in plenum_dic.keys():\n",
    "        partei.append(party)\n",
    "        dataset.append(dat)\n",
    "# normality test\n",
    "        stat, p = shapiro(plenum_dic[dat][party])\n",
    "\n",
    "        alpha = 0.05\n",
    "        if p > alpha:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + ' Gaussian')\n",
    "        else:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + ' NOT Gaussian')\n",
    "            \n",
    "Shapiro_Wilk_plen_df = pd.DataFrame({'Partei':partei, 'Daten':dataset, 'Ergebnis':result})\n",
    "Shapiro_Wilk_plen_piv = pd.pivot_table(Shapiro_Wilk_plen_df, values='Ergebnis', index=['Daten'], columns=['Partei'], aggfunc=max)\n",
    "Shapiro_Wilk_plen_piv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_plenum = pd.pivot_table(df_kom, index = 'Partei', columns = 'predict', aggfunc=np.count_nonzero)['Beschreibung1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "stat, p, dof, expected = chi2_contingency(contingency_plenum)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "if p > 0.05:\n",
    "    print('Probably independent')\n",
    "else:\n",
    "    print('Probably dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H05: Die Mitglieder der AfD rufen im Parlament höchstens genauso oft OFFENSE-Zurufe hinein wie die Mitglieder jeder anderen Partei.\n",
    "\n",
    "Für diese Hypothese wird noch der komplette Datensatz verwendet - also alle Wochen aus der betrachteten Legislaturperiode. Bei H06 hingegen, werden Twitter und Plenardaten verglichen. Dafür werden die Wochen in denen Plenarsitzungen stattgefunden haben ab Woche 19-w18, auf Twitter und im Plenum betrachtet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EMann-Whitney für alle Datensätze (Woche, Monat, Tag, OFFENSE, OTHER, ...)\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "partei = []\n",
    "dataset = []\n",
    "result = []\n",
    "\n",
    "for party in plenum_dic['piv_date_str'].columns:\n",
    "    for dat in plenum_dic.keys():\n",
    "        partei.append(party)\n",
    "        dataset.append(dat)\n",
    "# normality test\n",
    "        stat, p = mannwhitneyu(plenum_dic[dat]['AfD'], plenum_dic[dat][party])\n",
    "        if p/2 > 0.05:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' Gleich oder mehr als AfD' )\n",
    "        else:\n",
    "            result.append('stat=%.3f, p=%.3f' % (stat, p) + ' WENIGER als AfD')\n",
    "\n",
    "          \n",
    "Mann_Whitney_df = pd.DataFrame({'Partei':partei, 'Daten':dataset, 'Ergebnis':result})\n",
    "Mann_Whitney_piv =  pd.pivot_table(Mann_Whitney_df, values='Ergebnis', index=['Daten'], columns=['Partei'], aggfunc=max) \n",
    "#Mann_Whitney_piv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse Mann_Whitney für relative OFFENSE-Zurufe (Woche)\n",
    "# Hinweis - es wurden alle NaNs im Voraus entfernt und ein Extremwert bei der AfD (100% OFFENSE) \n",
    "# -> andernfalls ergeben sich kleine Unterschiede zwischen zwischen scipy und pingu\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "partei = []\n",
    "dataset = []\n",
    "result = []\n",
    "\n",
    "for party in plenum_dic['piv_date_str'].columns:\n",
    "    for dat in ['piv_Woche_rel']:\n",
    "        print(party)\n",
    "# normality test\n",
    "        print(mannwhitneyu(plenum_dic[dat]['AfD'], plenum_dic[dat][party]))\n",
    "        print(pg.mwu(plenum_dic[dat][party], plenum_dic[dat]['AfD']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(plenum_dic['piv_Woche_rel']['AfD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H06: Die Mitglieder jeder Partei rufen im Plenum höchstens genauso oft OFFENSE-Zurufe hinein wie sie OFFENSE-Tweets absetzen.\n",
    "\n",
    "Es werden Tweets und Zurufe auf Wochenbasis miteinander verglichen. Die Sitzungen im Bundestag haben zwar selten das selbe Thema, das aktuell auf Twitter aktuell ist, aber potenziell sind Stimmungen der Abgeordneten trotzdem auf beiden Kanälen zu erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wähle Wochen auf Twitter aus, die auch Sitzungswochen waren\n",
    "twitter_OF = twitter_dic['piv_weekOFFENSE'].loc[list(plenum_dic['piv_WocheOFFENSE'].index[30:])]\n",
    "twitter = twitter_dic['piv_week'].loc[list(plenum_dic['piv_Woche'].index[30:])]\n",
    "twitter_rel_piv = (twitter_OF/twitter)*100\n",
    "\n",
    "plenum_rel_piv = ((plenum_dic['piv_WocheOFFENSE'][30:] / plenum_dic['piv_Woche'][30:])*100).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#datensatz = plenum_rel_piv\n",
    "datensatz = twitter_rel_piv\n",
    "pyplot.style.use('seaborn-darkgrid')\n",
    "my_palette = ['blue', 'orange', 'green', 'purple', 'red', 'black']\n",
    "#my_palette = ['blue', 'green', 'black', 'purple', 'orange', 'red']\n",
    "Parteien = ['AFD', 'FDP', 'Gruene', 'Linke', 'SPD', 'Union']\n",
    "\n",
    "fig = pyplot.figure(figsize = (13,8))\n",
    "fig.suptitle('Histogramme der täglichen Summe an Tweets', fontsize = 18)\n",
    "\n",
    "num = 0\n",
    "for party, n in zip(datensatz.columns, range(1,7)):\n",
    "    ax = fig.add_subplot(2,3,n)\n",
    "    ax.hist(datensatz[party], color = my_palette[num])\n",
    "    ax.set_title(party)\n",
    "    num = num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "dataset = plenum_rel_piv\n",
    "partei = []\n",
    "result = []\n",
    "\n",
    "for party in dataset.columns:\n",
    "        partei.append(party)\n",
    "# normality test\n",
    "        stat, p = shapiro(dataset[party])\n",
    "\n",
    "        alpha = 0.05\n",
    "        if p > alpha:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + ' Gaussian')\n",
    "        else:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + ' NOT Gaussian')\n",
    "            \n",
    "Shapiro_Wilk_plen_df = pd.DataFrame({'Partei':partei, 'Ergebnis':result})\n",
    "Shapiro_Wilk_plen_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "dataset = plenum_rel_piv\n",
    "partei = []\n",
    "durch = []\n",
    "abw = []\n",
    "\n",
    "for party in dataset.columns:\n",
    "        partei.append(party)\n",
    "        durch.append('Mean: '+ str(mean(dataset[party]).round(3)))\n",
    "        abw.append('Std:'+str(std(dataset[party]).round(3)))\n",
    "        \n",
    "twitter_data_mean_std = pd.DataFrame({'Partei': partei, 'Mean': durch, 'Std': abw})\n",
    "twitter_data_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partei = []\n",
    "durch = []\n",
    "abw = []\n",
    "\n",
    "for party in plenum_rel_piv.columns:\n",
    "        partei.append(party)\n",
    "        durch.append('Mean: '+ str(mean(plenum_rel_piv[party]).round(3)))\n",
    "        abw.append('Std:'+str(std(plenum_rel_piv[party]).round(3)))\n",
    "        \n",
    "plenum_data_mean_std = pd.DataFrame({'Partei': partei, 'Mean': durch, 'Std': abw})\n",
    "plenum_data_mean_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman’s Rank Correlation\n",
    "\n",
    "\n",
    "*   Observations in each sample are independent and identically distributed (iid).\n",
    "*   Observations in each sample can be ranked.\n",
    "\n",
    "Für den Vergleich von Twitter und Plenardaten werden zwei verschiedene Zeiteinheiten verwendet und die Zeiträume wurden weitgehend aneinander angeglichen - eine genaue Übereinstimmung gibt es nicht, ist jedoch auch nicht sinnvoll weil Themen im Plenum keinen tagesaktuellen Bezug haben:\n",
    "\n",
    "* Twitter - Wochen\n",
    "* Plenum - Tage\n",
    "\n",
    "Bei den Plenardaten müssen außerdem sechs Tage entfernt werden an denen es keine OFFENSE Tweets gab - andernfalls würden nicht die gleichen Tage mit OFFENSE Tweets durch alle Tweets geteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "#-> Es können technsich beide verwendet werden - nur unten Fkt. anpassen \n",
    "# Aber Spearman ist sinnvoller weil Annahme der Normalverteilung für piv_date_str nicht haltbar ist\n",
    "\n",
    "partei = []\n",
    "partei_plen = []\n",
    "dataset = []\n",
    "result = []\n",
    "\n",
    "for party in twitter_dic['piv_month'].columns:\n",
    "    for party_plen in plenum_dic['piv_date_str'].columns:\n",
    "        partei.append(party)\n",
    "        partei_plen.append(party_plen)\n",
    "# normality test\n",
    "        stat, p = spearmanr(twitter_rel_piv[party], plenum_rel_piv[party_plen])\n",
    "\n",
    "        if p > 0.05:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + ' INDEPENDENT')\n",
    "        else:\n",
    "            result.append('Stats=%.3f, p=%.3f' % (stat, p) + 'Probably dependent')\n",
    "            \n",
    "Corr_parties_df = pd.DataFrame({'Partei':partei, 'Partei_Plenum':partei_plen, 'Ergebnis':result})\n",
    "Corr_parties_piv =  pd.pivot_table(Corr_parties_df, values='Ergebnis', index=['Partei_Plenum'], columns=['Partei'], aggfunc=max) \n",
    "Corr_parties_piv\n",
    "# -> Die Zurufe die im Parlament an den jeweiligen Sitzungstagen erfolgen sind unabbhängig von den Tweets innerhalb einer Woche "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "plenum_rel_piv.columns = ['AFD', 'Gruene', 'Union', 'Linke', 'FDP', 'SPD']\n",
    "\n",
    "partei = []\n",
    "partei_plen = []\n",
    "dataset = []\n",
    "result = []   \n",
    "\n",
    "for party in plenum_rel_piv.columns:\n",
    "    partei.append(party)\n",
    "# normality test\n",
    "    stat, p = mannwhitneyu(twitter_rel_piv[party], plenum_rel_piv[party])\n",
    "    if p > 0.05 :    # p/2 für Einseitigen t-Test\n",
    "        result.append('stat=%.3f, p=%.3f' % (stat, p) + ' Gleich offensiv oder mehr im Plenum')\n",
    "    else:\n",
    "        result.append('stat=%.3f, p=%.3f' % (stat, p) + ' WENIGER OFFENSIV im Plenum')\n",
    "                        \n",
    "H06_df = pd.DataFrame({'Partei':partei, 'Ergebnis':result})\n",
    "#H06_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pingouin as pg\n",
    "from pingouin import ttest\n",
    "#https://pingouin-stats.org/generated/pingouin.mwu.html\n",
    "#https://pingouin-stats.org/generated/pingouin.ttest.html#pingouin.ttest\n",
    "\n",
    "for party in plenum_rel_piv.columns:\n",
    "# normality test\n",
    "    print(party)\n",
    "    print(mannwhitneyu(twitter_rel_piv[party], plenum_rel_piv[party]))\n",
    "    print(pg.mwu(plenum_rel_piv[party],twitter_rel_piv[party]))\n",
    "    #print(pg.mwu(plenum_rel_piv[party],twitter_rel_piv[party], tail = 'less'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H07: Der Anteil von OFFENSE-Zurufen vor Corona ist genauso groß oder kleiner als währenddessen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gensim, numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "#df_kom = mdp_partei_emo\n",
    "PICKLE_FOLDER_PATH = '/home/lisa/Darmstadt/Master Arbeit/06_Analyse/Plenum/'\n",
    "#pickle.dump(df_kom, open(PICKLE_FOLDER_PATH + 'Plenum_Kom_all_Sent', \"wb\" ))\n",
    "\n",
    "df_kom = pickle.load(open(PICKLE_FOLDER_PATH + 'Plenum_Kom_all_Sent', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "time = ['Monat', 'Woche', 'date_str']\n",
    "cat = ['', 'OFFENSE', 'OTHER']\n",
    "plenum_dic = {}\n",
    "\n",
    "\n",
    "for c in cat: \n",
    "    if c == '':\n",
    "        sent_df = df_kom\n",
    "    else:\n",
    "        sent_df = df_kom.loc[df_kom['predict'] == c]\n",
    "    for t in time: \n",
    "        piv = pd.pivot_table(sent_df, index = [t], columns = ['Partei'], aggfunc = np.count_nonzero)['Beschreibung1'].fillna(0)\n",
    "        \n",
    "        plenum_dic['piv_'+t+c] =  piv       \n",
    "            \n",
    "for woche in ['17-w43', '18-w03', '18-w36', '19-w29', '19-w36']:\n",
    "    plenum_dic['piv_Woche'] = plenum_dic['piv_Woche'].loc[plenum_dic['piv_Woche'].index != woche]\n",
    "    plenum_dic['piv_WocheOFFENSE'] = plenum_dic['piv_WocheOFFENSE'].loc[plenum_dic['piv_WocheOFFENSE'].index != woche]\n",
    "\n",
    "# Normiere OFFENSE Tweets\n",
    "plenum_dic['piv_Woche_rel'] = (plenum_dic['piv_WocheOFFENSE'] / plenum_dic['piv_Woche'])*100\n",
    "    \n",
    "plenum_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lösche Datum für das es keine OFFENSE Zurufe gab\n",
    "plenum_dic['piv_date_str'] = plenum_dic['piv_date_str'].loc[plenum_dic['piv_date_str'].index != '2020-01-15']\n",
    "\n",
    "# Nur Lockdown - wären gerade sechs Beobachtungen\n",
    "#piv_Corona_plen_rel = (plenum_dic['piv_date_strOFFENSE'][146:152]/plenum_dic['piv_date_str'][151:157])*100\n",
    "#piv_before_Corona_plen_rel = (plenum_dic['piv_date_strOFFENSE'][141:147]/plenum_dic['piv_date_str'][146:152])*100\n",
    "\n",
    "# Ab Lockdown bis zur Sommerpause - 18 Beobachtungen\n",
    "piv_Corona_plen_rel = (plenum_dic['piv_date_strOFFENSE'][146:]/plenum_dic['piv_date_str'][151:])*100\n",
    "piv_before_Corona_plen_rel = (plenum_dic['piv_date_strOFFENSE'][128:146]/plenum_dic['piv_date_str'][133:151])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "from pingouin import ttest\n",
    "#https://pingouin-stats.org/generated/pingouin.mwu.html\n",
    "#https://pingouin-stats.org/generated/pingouin.ttest.html#pingouin.ttest\n",
    "\n",
    "for party in piv_Corona_plen_rel.columns:\n",
    "# normality test\n",
    "        print(party)\n",
    "        print(wilcoxon(piv_before_Corona_plen_rel[party], piv_Corona_plen_rel[party]))\n",
    "        print(pg.wilcoxon(piv_before_Corona_plen_rel[party], piv_Corona_plen_rel[party]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
